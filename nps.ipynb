{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/HgqkgkcLKh/sUDJAAD90",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristValen/Acciones-RNR/blob/main/nps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAtGfiwYZC3g"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Establecer la semilla del generador de números aleatorios\n",
        "np.random.seed(42)\n",
        "\n",
        "# Supongamos que 'df' es tu DataFrame y que 'clase' es tu variable objetivo\n",
        "df = pd.read_csv('tu_archivo.csv')\n",
        "\n",
        "# Crear las variables objetivo y características\n",
        "X = df.drop('clase', axis=1)\n",
        "y = df['clase']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Crear el modelo de Random Forest\n",
        "modelo = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Definir la grilla de hiperparámetros para la búsqueda aleatoria\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200, 300, 400],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth': [10, 20, 30, 40, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda aleatoria de hiperparámetros\n",
        "busqueda_aleatoria = RandomizedSearchCV(estimator=modelo,\n",
        "                                        param_distributions=param_dist,\n",
        "                                        n_iter=100,\n",
        "                                        cv=3,\n",
        "                                        verbose=2,\n",
        "                                        random_state=42,\n",
        "                                        n_jobs=-1)\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros encontrados\n",
        "busqueda_aleatoria.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de prueba\n",
        "probas = busqueda_aleatoria.predict_proba(X_test)\n",
        "\n",
        "# Definir tus umbrales personalizados aquí. Por ejemplo:\n",
        "umbrales = [0.4, 0.3, 0.3]  # Un umbral para cada clase\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "predicciones = np.array([busqueda_aleatoria.classes_[i] for i in np.argmax(probas >= umbrales, axis=1)])\n",
        "\n",
        "# Imprimir un informe de clasificación\n",
        "print(\"Informe de clasificación:\")\n",
        "print(classification_report(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la matriz de confusión con los nombres de las clases\n",
        "cm = confusion_matrix(y_test, predicciones)\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index=[i for i in busqueda_aleatoria.classes_],\n",
        "                     columns=[i for i in busqueda_aleatoria.classes_])\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_df)\n",
        "\n",
        "# Imprimir la precisión del modelo\n",
        "print(\"Precisión del modelo:\")\n",
        "print(accuracy_score(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la puntuación AUC-ROC para cada clase\n",
        "lb = LabelBinarizer()\n",
        "lb.fit(y_test)\n",
        "y_test_lb = lb.transform(y_test)\n",
        "predicciones_lb = lb.transform(predicciones)\n",
        "print(\"Puntuación AUC-ROC para cada clase:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    print(f\"{lb.classes_[i]}: {roc_auc_score(y_test_lb[:, i], predicciones_lb[:, i])}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#########nuevos\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Establecer la semilla del generador de números aleatorios\n",
        "np.random.seed(42)\n",
        "\n",
        "# Supongamos que 'df' es tu DataFrame y que 'clase' es tu variable objetivo\n",
        "df = pd.read_csv('tu_archivo.csv')\n",
        "\n",
        "# Crear las variables objetivo y características\n",
        "X = df.drop('clase', axis=1)\n",
        "y = df['clase']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Crear el modelo de Random Forest\n",
        "modelo = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Definir la grilla de hiperparámetros para la búsqueda aleatoria\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200, 300, 400],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth': [10, 20, 30, 40, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda aleatoria de hiperparámetros\n",
        "busqueda_aleatoria = RandomizedSearchCV(estimator=modelo,\n",
        "                                        param_distributions=param_dist,\n",
        "                                        n_iter=100,\n",
        "                                        cv=3,\n",
        "                                        verbose=2,\n",
        "                                        random_state=42,\n",
        "                                        n_jobs=-1)\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros encontrados\n",
        "busqueda_aleatoria.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de prueba\n",
        "probas = busqueda_aleatoria.predict_proba(X_test)\n",
        "\n",
        "# Imprimir un informe de clasificación\n",
        "print(\"Informe de clasificación:\")\n",
        "print(classification_report(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la matriz de confusión con los nombres de las clases\n",
        "cm = confusion_matrix(y_test, predicciones)\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index=[i for i in busqueda_aleatoria.classes_],\n",
        "                     columns=[i for i in busqueda_aleatoria.classes_])\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_df)\n",
        "\n",
        "# Imprimir la precisión del modelo\n",
        "print(\"Precisión del modelo:\")\n",
        "print(accuracy_score(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la puntuación AUC-ROC para cada clase\n",
        "lb = LabelBinarizer()\n",
        "lb.fit(y_test)\n",
        "y_test_lb = lb.transform(y_test)\n",
        "predicciones_lb = lb.transform(predicciones)\n",
        "print(\"Puntuación AUC-ROC para cada clase:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    print(f\"{lb.classes_[i]}: {roc_auc_score(y_test_lb[:, i], predicciones_lb[:, i])}\")\n",
        "\n",
        "# Función para calcular el valor KS\n",
        "def calc_ks(data):\n",
        "    data['bucket']=(data['probability'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().probability.to_frame(name='min_prob')\n",
        "    kstable['max_prob']=grouped.max().probability\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "# Función para calcular el coeficiente Gini\n",
        "def calc_gini(y_verdadero, y_prob):\n",
        "    fpr, tpr, _ = roc_curve(y_verdadero, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    gini = 2*roc_auc - 1\n",
        "    return gini\n",
        "\n",
        "# Calcular e imprimir el valor KS y el coeficiente Gini para cada clase\n",
        "print(\"Valor KS y coeficiente Gini para cada clase:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    data = pd.DataFrame({'label': y_test_lb[:, i], 'probability': probas[:, i]})\n",
        "    data['good']=(data['label']==0).astype(int)\n",
        "    data['bad']=(data['label']==1).astype(int)\n",
        "    ks = calc_ks(data)\n",
        "    gini = calc_gini(y_test_lb[:, i], probas[:, i])\n",
        "    print(f\"{lb.classes_[i]}: KS = {ks}, Gini = {gini}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NkZPWwKX4six"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Establecer la semilla del generador de números aleatorios\n",
        "np.random.seed(42)\n",
        "\n",
        "# Supongamos que 'df' es tu DataFrame y que 'clase' es tu variable objetivo\n",
        "df = pd.read_csv('tu_archivo.csv')\n",
        "\n",
        "# Crear las variables objetivo y características\n",
        "X = df.drop('clase', axis=1)\n",
        "y = df['clase']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Crear el modelo SVM\n",
        "modelo = SVC(probability=True, random_state=42)\n",
        "\n",
        "# Definir la grilla de hiperparámetros para la búsqueda aleatoria\n",
        "param_dist = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda aleatoria de hiperparámetros\n",
        "busqueda_aleatoria = RandomizedSearchCV(estimator=modelo,\n",
        "                                        param_distributions=param_dist,\n",
        "                                        n_iter=100,\n",
        "                                        cv=3,\n",
        "                                        verbose=2,\n",
        "                                        random_state=42,\n",
        "                                        n_jobs=-1)\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros encontrados\n",
        "busqueda_aleatoria.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de prueba\n",
        "probas = busqueda_aleatoria.predict_proba(X_test)\n",
        "\n",
        "# Definir tus umbrales personalizados aquí. Por ejemplo:\n",
        "umbrales = [0.4, 0.3, 0.3]  # Un umbral para cada clase\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "predicciones = np.array([busqueda_aleatoria.classes_[i] for i in np.argmax(probas >= umbrales, axis=1)])\n",
        "\n",
        "# Imprimir un informe de clasificación\n",
        "print(\"Informe de clasificación:\")\n",
        "print(classification_report(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la matriz de confusión con los nombres de las clases\n",
        "cm = confusion_matrix(y_test, predicciones)\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index=[i for i in busqueda_aleatoria.classes_],\n",
        "                     columns=[i for i in busqueda_aleatoria.classes_])\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_df)\n",
        "\n",
        "# Imprimir la precisión del modelo\n",
        "print(\"Precisión del modelo:\")\n",
        "print(accuracy_score(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la puntuación AUC-ROC para cada clase\n",
        "lb = LabelBinarizer()\n",
        "lb.fit(y_test)\n",
        "y_test_lb = lb.transform(y_test)\n",
        "predicciones_lb = lb.transform(predicciones)\n",
        "print(\"Puntuación AUC-ROC para cada clase:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    print(f\"{lb.classes_[i]}: {roc_auc_score(y_test_lb[:, i], predicciones_lb[:, i])}\")\n"
      ],
      "metadata": {
        "id": "r15QARqDXV94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "# Asumiendo que tu DataFrame se llama df y tus columnas son 'ope' y 'conta'\n",
        "df = df.withColumn(\"ope\", to_date(df['ope'], \"yyyy-MM-dd\"))\n",
        "df = df.withColumn(\"conta\", to_date(df['conta'], \"yyyy-MM-dd\"))\n",
        "\n",
        "# Agrupar por la columna 'ope'\n",
        "df_grouped_by_ope = df.groupBy(\"ope\").count()\n",
        "\n",
        "# Agrupar por la columna 'conta'\n",
        "df_grouped_by_conta = df.groupBy(\"conta\").count()"
      ],
      "metadata": {
        "id": "uW9K8n1zeQzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Establecer la semilla del generador de números aleatorios\n",
        "np.random.seed(42)\n",
        "\n",
        "# Supongamos que 'df' es tu DataFrame y que 'clase' es tu variable objetivo\n",
        "df = pd.read_csv('tu_archivo.csv')\n",
        "\n",
        "# Crear las variables objetivo y características\n",
        "X = df.drop('clase', axis=1)\n",
        "y = df['clase']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Crear una instancia del clasificador Gradient Boosting\n",
        "modelo = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Definir la grilla de hiperparámetros para la búsqueda en grilla\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda en grilla de hiperparámetros\n",
        "busqueda_en_grilla = GridSearchCV(estimator=modelo,\n",
        "                                  param_grid=param_grid,\n",
        "                                  cv=3,\n",
        "                                  verbose=2,\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros encontrados\n",
        "busqueda_en_grilla.fit(X_train, y_train)\n",
        "\n",
        "# Predecir las clases para el conjunto de prueba\n",
        "y_pred = busqueda_en_grilla.predict(X_test)\n",
        "\n",
        "# Definir tus umbrales personalizados aquí. Por ejemplo:\n",
        "umbrales = [0.4, 0.3, 0.3]  # Un umbral para cada clase\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "probas = busqueda_en_grilla.predict_proba(X_test)\n",
        "predicciones = np.array([busqueda_en_grilla.classes_[i] for i in np.argmax(probas >= umbrales, axis=1)])\n",
        "\n",
        "# Imprimir un informe de clasificación\n",
        "print(\"Informe de clasificación:\")\n",
        "print(classification_report(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la matriz de confusión con los nombres de las clases\n",
        "cm = confusion_matrix(y_test, predicciones)\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index=[i for i in busqueda_en_grilla.classes_],\n",
        "                     columns=[i for i in busqueda_en_grilla.classes_])\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_df)\n",
        "\n",
        "# Imprimir la precisión del modelo\n",
        "print(\"Precisión del modelo:\")\n",
        "print(accuracy_score(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la puntuación AUC-ROC para cada clase\n",
        "lb = LabelBinarizer()\n",
        "lb.fit(y_test)\n",
        "y_test_lb = lb.transform(y_test)\n",
        "predicciones_lb = lb.transform(predicciones)\n",
        "print(\"Puntuación AUC-ROC para cada clase:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    print(f\"{lb.classes_[i]}: {roc_auc_score(y_test_lb[:, i], predicciones_lb[:, i])}\")\n"
      ],
      "metadata": {
        "id": "Sq2TVcRL0Dej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
        "\n",
        "# Establecer la semilla para la reproducibilidad\n",
        "np.random.seed(0)\n",
        "\n",
        "# Supongamos que X es tu conjunto de datos e y son las etiquetas correspondientes\n",
        "\n",
        "# Crear el codificador\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Ajustar el codificador y transformar las etiquetas\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Convertir los datos a float32\n",
        "X = X.astype(np.float32)\n",
        "y = y.astype(np.float32)\n",
        "\n",
        "# División de los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Creación del modelo MLP más profundo\n",
        "model = Sequential()\n",
        "model.add(Dense(50, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))  # Tres neuronas en la capa de salida para los tres niveles de la variable dependiente\n",
        "\n",
        "# Compilación del modelo\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Asignación de pesos a las clases (supongamos que class_weights es un diccionario que contiene los pesos)\n",
        "class_weights = {0: 1., 1: 1., 2: 1.}  # Modifica esto según tus necesidades\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "model.fit(X_train, y_train, epochs=100, verbose=0, class_weight=class_weights)\n",
        "\n",
        "# Predicción en el conjunto de prueba\n",
        "y_pred_proba = model.predict(X_test)\n",
        "\n",
        "# Definir los umbrales de decisión para cada clase\n",
        "thresholds = [0.5, 0.5, 0.5]  # Modifica esto según tus necesidades\n",
        "\n",
        "y_pred = np.zeros_like(y_pred_proba)\n",
        "for i in range(3):\n",
        "    y_pred[:, i] = (y_pred_proba[:, i] > thresholds[i]).astype(int)\n",
        "\n",
        "y_pred = np.argmax(y_pred, axis=1)  # Convertir las predicciones one-hot a etiquetas\n",
        "\n",
        "# Calcular la matriz de confusión para todas las categorías\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Crear un DataFrame para visualizar la matriz de confusión con los nombres de las clases\n",
        "class_names = ['detractor', 'neutro', 'promotor']  # Modifica esto según tus necesidades\n",
        "confusion_df = pd.DataFrame(confusion, index=class_names, columns=class_names)\n",
        "\n",
        "print(\"Confusion Matrix: \\n\", confusion_df)\n",
        "\n",
        "# Calcular las métricas para cada categoría\n",
        "for i in range(3):\n",
        "    precision = precision_score(y_test == i, y_pred == i)\n",
        "    recall = recall_score(y_test == i, y_pred == i)\n",
        "    f1 = f1_score(y_test == i, y_pred == i)\n",
        "\n",
        "    # Calcular la curva ROC y el AUC-ROC\n",
        "    fpr, tpr, _ = roc_curve(y_test == i, y_pred_proba[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    print(f\"Clase {class_names[i]}:\")\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1 Score: \", f1)\n",
        "    print(\"ROC AUC Score: \", roc_auc)\n"
      ],
      "metadata": {
        "id": "l0E6y2lWoCAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular las métricas ponderadas para cada categoría\n",
        "for i in range(3):\n",
        "    precision = precision_score(y_test == i, y_pred == i, average='weighted')\n",
        "    recall = recall_score(y_test == i, y_pred == i, average='weighted')\n",
        "    f1 = f1_score(y_test == i, y_pred == i, average='weighted')\n",
        "\n",
        "    print(f\"Clase {class_names[i]} ponderada:\")\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1 Score: \", f1)\n",
        "\n",
        "### a mano\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Supongamos que 'y_test' son tus etiquetas verdaderas e 'y_pred' son tus predicciones\n",
        "f1_scores = [f1_score(y_test == i, y_pred == i) for i in range(3)]\n",
        "class_counts = [sum(y_test == i) for i in range(3)]\n",
        "total_count = len(y_test)\n",
        "\n",
        "weighted_f1_score = sum(f1 * count for f1, count in zip(f1_scores, class_counts)) / total_count"
      ],
      "metadata": {
        "id": "zqAlf41JoIka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve, auc, confusion_matrix\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Establecer la semilla del generador de números aleatorios\n",
        "np.random.seed(42)\n",
        "\n",
        "# Supongamos que 'df' es tu DataFrame y que 'clase' es tu variable objetivo\n",
        "df = pd.read_csv('tu_archivo.csv')\n",
        "\n",
        "# Crear las variables objetivo y características\n",
        "X = df.drop('clase', axis=1)\n",
        "y = df['clase']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Crear una instancia del clasificador Gradient Boosting\n",
        "modelo = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Definir la grilla de hiperparámetros para la búsqueda en grilla\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda en grilla de hiperparámetros\n",
        "busqueda_en_grilla = GridSearchCV(estimator=modelo,\n",
        "                                  param_grid=param_grid,\n",
        "                                  cv=3,\n",
        "                                  verbose=2,\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros encontrados\n",
        "busqueda_en_grilla.fit(X_train, y_train)\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de prueba\n",
        "probas = busqueda_en_grilla.predict_proba(X_test)\n",
        "\n",
        "# Definir tus umbrales personalizados aquí. Por ejemplo:\n",
        "umbrales = [0.4, 0.3, 0.3]  # Un umbral para cada clase\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "predicciones = np.array([busqueda_en_grilla.classes_[i] for i in np.argmax(probas >= umbrales, axis=1)])\n",
        "\n",
        "# Imprimir un informe de clasificación\n",
        "print(\"Informe de clasificación:\")\n",
        "print(classification_report(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la matriz de confusión con los nombres de las clases\n",
        "cm = confusion_matrix(y_test, predicciones)\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index=[i for i in busqueda_en_grilla.classes_],\n",
        "                     columns=[i for i in busqueda_en_grilla.classes_])\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_df)\n",
        "\n",
        "# Imprimir la precisión del modelo\n",
        "print(\"Precisión del modelo:\")\n",
        "print(accuracy_score(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la puntuación AUC-ROC para cada clase\n",
        "lb = LabelBinarizer()\n",
        "lb.fit(y_test)\n",
        "y_test_lb = lb.transform(y_test)\n",
        "predicciones_lb = lb.transform(predicciones)\n",
        "print(\"Puntuación AUC-ROC para cada clase:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    print(f\"{lb.classes_[i]}: {roc_auc_score(y_test_lb[:, i], predicciones_lb[:, i])}\")\n",
        "\n",
        "# Función para calcular el valor KS\n",
        "def calc_ks(data):\n",
        "    data['good']=(data['label']==0).astype(int)\n",
        "    data['bad']=(data['label']==1).astype(int)\n",
        "    data['bucket']=(data['probability'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().probability.to_frame(name='min_prob')\n",
        "    kstable['max_prob']=grouped.max().probability\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "# Función para calcular el coeficiente Gini\n",
        "def calc_gini(y_verdadero, y_prob):\n",
        "    fpr, tpr, _ = roc_curve(y_verdadero, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    gini = 2*roc_auc - 1\n",
        "    return gini\n",
        "\n",
        "# Calcular e imprimir el valor KS y el coeficiente Gini para cada clase\n",
        "print(\"Valor KS y coeficiente Gini para cada clase:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    data = pd.DataFrame({'label': y_test_lb[:, i], 'probability': probas[:, i]})\n",
        "    data['good']=(data['label']==0).astype(int)\n",
        "    data['bad']=(data['label']==1).astype(int)\n",
        "    ks = calc_ks(data)\n",
        "    gini = calc_gini(y_test_lb[:, i], probas[:, i])\n",
        "    print(f\"{lb.classes_[i]}: KS = {ks}, Gini = {gini}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "iokLgqgQBwRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
        "\n",
        "# Establecer la semilla para la reproducibilidad\n",
        "np.random.seed(0)\n",
        "\n",
        "# Supongamos que X2 es tu conjunto de datos e y2 son las etiquetas correspondientes\n",
        "X2 = X.copy()\n",
        "y2 = y.copy()\n",
        "\n",
        "# Crear el codificador\n",
        "le2 = LabelEncoder()\n",
        "\n",
        "# Ajustar el codificador y transformar las etiquetas\n",
        "y2 = le2.fit_transform(y2)\n",
        "\n",
        "# Convertir los datos a float32\n",
        "X2 = X2.astype(np.float32)\n",
        "y2 = y2.astype(np.float32)\n",
        "\n",
        "# División de los datos en conjuntos de entrenamiento y prueba\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=0)\n",
        "\n",
        "# Creación del modelo MLP más profundo\n",
        "model2 = Sequential()\n",
        "model2.add(Dense(50, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model2.add(Dense(50, activation='relu'))\n",
        "model2.add(Dense(50, activation='relu'))\n",
        "model2.add(Dense(3, activation='softmax'))  # Tres neuronas en la capa de salida para los tres niveles de la variable dependiente\n",
        "\n",
        "# Compilación del modelo\n",
        "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Asignación de pesos a las clases (supongamos que class_weights es un diccionario que contiene los pesos)\n",
        "class_weights = {0: 1., 1: 1., 2: 1.}  # Modifica esto según tus necesidades\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "model2.fit(X_train2, y_train2, epochs=100, verbose=0, class_weight=class_weights)\n",
        "\n",
        "# Predicción en el conjunto de prueba\n",
        "y_pred_proba2 = model.predict(X_test)\n",
        "\n",
        "# Definir los umbrales de decisión para cada clase\n",
        "thresholds = [0.5, 0.5, 0.5]  # Modifica esto según tus necesidades\n",
        "\n",
        "y_pred2 = np.zeros_like(y_pred_proba)\n",
        "for i in range(3):\n",
        "    y_pred[:, i] = (y_pred_proba[:, i] > thresholds[i]).astype(int)\n",
        "\n",
        "y_pred = np.argmax(y_pred, axis=1)  # Convertir las predicciones one-hot a etiquetas\n",
        "\n",
        "# Calcular la matriz de confusión para todas las categorías\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Crear un DataFrame para visualizar la matriz de confusión con los nombres de las clases\n",
        "class_names = ['detractor', 'neutro', 'promotor']  # Modifica esto según tus necesidades\n",
        "confusion_df = pd.DataFrame(confusion, index=class_names, columns=class_names)\n",
        "\n",
        "print(\"Confusion Matrix: \\n\", confusion_df)\n",
        "\n",
        "# Calcular las métricas para cada categoría\n",
        "for i in range(3):\n",
        "    precision = precision_score(y_test == i, y_pred == i)\n",
        "    recall = recall_score(y_test == i, y_pred == i)\n",
        "    f1 = f1_score(y_test == i, y_pred == i)\n",
        "\n",
        "    # Calcular la curva ROC y el AUC-ROC\n",
        "    fpr, tpr, _ = roc_curve(y_test == i, y_pred_proba[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    print(f\"Clase {class_names[i]}:\")\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1 Score: \", f1)\n",
        "    print(\"ROC AUC Score: \", roc_auc)\n",
        "\n",
        "# Función para calcular el valor KS\n",
        "def calc_ks(data):\n",
        "    data['good']=(data['label']==0).astype(int)\n",
        "    data['bad']=(data['label']==1).astype(int)\n",
        "    data['bucket']=(data['probability'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().probability.to_frame(name='min_prob')\n",
        "    kstable['max_prob']=grouped.max().probability\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "# Función para calcular el coeficiente Gini\n",
        "def calc_gini(y_verdadero, y_prob):\n",
        "    fpr, tpr, _ = roc_curve(y_verdadero, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    gini = 2*roc_auc - 1\n",
        "    return gini\n",
        "\n",
        "# Calcular e imprimir el valor KS y el coeficiente Gini para cada clase\n",
        "print(\"Valor KS y coeficiente Gini para cada clase:\")\n",
        "for i in range(len(class_names)):\n",
        "    data = pd.DataFrame({'label': (y_test == i).astype(int), 'probability': y_pred_proba[:, i]})\n",
        "    data['good']=(data['label']==0).astype(int)\n",
        "    data['bad']=(data['label']==1).astype(int)\n",
        "    ks = calc_ks(data)\n",
        "    gini = calc_gini((y_test == i).astype(int), y_pred_proba[:, i])\n",
        "    print(f\"{class_names[i]}: KS = {ks}, Gini = {gini}\")\n",
        "\n",
        "\n",
        "    from scipy.optimize import differential_evolution\n",
        "\n",
        "# Definir la función objetivo para la optimización\n",
        "def objective(thresholds):\n",
        "    # Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "    y_pred = np.zeros_like(y_pred_proba2)\n",
        "    for i in range(3):\n",
        "        y_pred[:, i] = (y_pred_proba2[:, i] > thresholds[i]).astype(int)\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    # Calcular las métricas\n",
        "    roc_auc_scores = []\n",
        "    ks_values = []\n",
        "    gini_values = []\n",
        "    for i in range(len(class_names)):\n",
        "        data = pd.DataFrame({'label': (y_test2 == i).astype(int), 'probability': y_pred_proba2[:, i]})\n",
        "        data['good']=(data['label']==0).astype(int)\n",
        "        data['bad']=(data['label']==1).astype(int)\n",
        "        ks = calc_ks(data)\n",
        "        gini = calc_gini((y_test2 == i).astype(int), y_pred_proba2[:, i])\n",
        "        fpr, tpr, _ = roc_curve((y_test2 == i).astype(int), y_pred_proba2[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        roc_auc_scores.append(roc_auc)\n",
        "        ks_values.append(ks)\n",
        "        gini_values.append(gini)\n",
        "\n",
        "    # Calcular el promedio de las métricas\n",
        "    avg_roc_auc = np.mean(roc_auc_scores)\n",
        "    avg_ks = np.mean(ks_values)\n",
        "    avg_gini = np.mean(gini_values)\n",
        "\n",
        "    # La función objetivo es el negativo del promedio de las métricas porque queremos maximizarlas\n",
        "    return -(avg_roc_auc + avg_ks + avg_gini) / 3\n",
        "\n",
        "# Definir los límites para los umbrales\n",
        "bounds = [(0, 1)] * 3\n",
        "\n",
        "# Realizar la optimización\n",
        "result = differential_evolution(objective, bounds)\n",
        "\n",
        "# Imprimir los mejores umbrales encontrados\n",
        "print(\"Mejores umbrales: \", result.x)\n",
        "\n"
      ],
      "metadata": {
        "id": "cqjoSWZzdpyo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
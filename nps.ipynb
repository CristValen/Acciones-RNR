{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVlABA8UCT1UrQWy3dGhSW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristValen/Acciones-RNR/blob/main/nps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAtGfiwYZC3g"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Establecer la semilla del generador de números aleatorios\n",
        "np.random.seed(42)\n",
        "\n",
        "# Supongamos que 'df' es tu DataFrame y que 'clase' es tu variable objetivo\n",
        "df = pd.read_csv('tu_archivo.csv')\n",
        "\n",
        "# Crear las variables objetivo y características\n",
        "X = df.drop('clase', axis=1)\n",
        "y = df['clase']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Crear el modelo de Random Forest\n",
        "modelo = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Definir la grilla de hiperparámetros para la búsqueda aleatoria\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200, 300, 400],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth': [10, 20, 30, 40, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda aleatoria de hiperparámetros\n",
        "busqueda_aleatoria = RandomizedSearchCV(estimator=modelo,\n",
        "                                        param_distributions=param_dist,\n",
        "                                        n_iter=100,\n",
        "                                        cv=3,\n",
        "                                        verbose=2,\n",
        "                                        random_state=42,\n",
        "                                        n_jobs=-1)\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros encontrados\n",
        "busqueda_aleatoria.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de prueba\n",
        "probas = busqueda_aleatoria.predict_proba(X_test)\n",
        "\n",
        "# Definir tus umbrales personalizados aquí. Por ejemplo:\n",
        "umbrales = [0.4, 0.3, 0.3]  # Un umbral para cada clase\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "predicciones = np.array([busqueda_aleatoria.classes_[i] for i in np.argmax(probas >= umbrales, axis=1)])\n",
        "\n",
        "# Imprimir un informe de clasificación\n",
        "print(\"Informe de clasificación:\")\n",
        "print(classification_report(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la matriz de confusión con los nombres de las clases\n",
        "cm = confusion_matrix(y_test, predicciones)\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index=[i for i in busqueda_aleatoria.classes_],\n",
        "                     columns=[i for i in busqueda_aleatoria.classes_])\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_df)\n",
        "\n",
        "# Imprimir la precisión del modelo\n",
        "print(\"Precisión del modelo:\")\n",
        "print(accuracy_score(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la puntuación AUC-ROC para cada clase\n",
        "lb = LabelBinarizer()\n",
        "lb.fit(y_test)\n",
        "y_test_lb = lb.transform(y_test)\n",
        "predicciones_lb = lb.transform(predicciones)\n",
        "print(\"Puntuación AUC-ROC para cada clase:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    print(f\"{lb.classes_[i]}: {roc_auc_score(y_test_lb[:, i], predicciones_lb[:, i])}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2aGcM0-GR836"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0QNwMjnjd_db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########nuevos\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Establecer la semilla del generador de números aleatorios\n",
        "np.random.seed(42)\n",
        "\n",
        "# Supongamos que 'df' es tu DataFrame y que 'clase' es tu variable objetivo\n",
        "df = pd.read_csv('tu_archivo.csv')\n",
        "\n",
        "# Crear las variables objetivo y características\n",
        "X = df.drop('clase', axis=1)\n",
        "y = df['clase']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Crear el modelo de Random Forest\n",
        "modelo = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Definir la grilla de hiperparámetros para la búsqueda aleatoria\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200, 300, 400],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth': [10, 20, 30, 40, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda aleatoria de hiperparámetros\n",
        "busqueda_aleatoria = RandomizedSearchCV(estimator=modelo,\n",
        "                                        param_distributions=param_dist,\n",
        "                                        n_iter=100,\n",
        "                                        cv=3,\n",
        "                                        verbose=2,\n",
        "                                        random_state=42,\n",
        "                                        n_jobs=-1)\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros encontrados\n",
        "busqueda_aleatoria.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de prueba\n",
        "probas = busqueda_aleatoria.predict_proba(X_test)\n",
        "\n",
        "# Imprimir un informe de clasificación\n",
        "print(\"Informe de clasificación:\")\n",
        "print(classification_report(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la matriz de confusión con los nombres de las clases\n",
        "cm = confusion_matrix(y_test, predicciones)\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index=[i for i in busqueda_aleatoria.classes_],\n",
        "                     columns=[i for i in busqueda_aleatoria.classes_])\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_df)\n",
        "\n",
        "# Imprimir la precisión del modelo\n",
        "print(\"Precisión del modelo:\")\n",
        "print(accuracy_score(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la puntuación AUC-ROC para cada clase\n",
        "lb = LabelBinarizer()\n",
        "lb.fit(y_test)\n",
        "y_test_lb = lb.transform(y_test)\n",
        "predicciones_lb = lb.transform(predicciones)\n",
        "print(\"Puntuación AUC-ROC para cada clase:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    print(f\"{lb.classes_[i]}: {roc_auc_score(y_test_lb[:, i], predicciones_lb[:, i])}\")\n",
        "\n",
        "# Función para calcular el valor KS\n",
        "def calc_ks(data):\n",
        "    data['bucket']=(data['probability'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().probability.to_frame(name='min_prob')\n",
        "    kstable['max_prob']=grouped.max().probability\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "# Función para calcular el coeficiente Gini\n",
        "def calc_gini(y_verdadero, y_prob):\n",
        "    fpr, tpr, _ = roc_curve(y_verdadero, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    gini = 2*roc_auc - 1\n",
        "    return gini\n",
        "\n",
        "# Calcular e imprimir el valor KS y el coeficiente Gini para cada clase\n",
        "print(\"Valor KS y coeficiente Gini para cada clase:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    data = pd.DataFrame({'label': y_test_lb[:, i], 'probability': probas[:, i]})\n",
        "    data['good']=(data['label']==0).astype(int)\n",
        "    data['bad']=(data['label']==1).astype(int)\n",
        "    ks = calc_ks(data)\n",
        "    gini = calc_gini(y_test_lb[:, i], probas[:, i])\n",
        "    print(f\"{lb.classes_[i]}: KS = {ks}, Gini = {gini}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NkZPWwKX4six"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###definitivo\n",
        "\n",
        "# Establecer la semilla del generador de números aleatorios\n",
        "np.random.seed(42)\n",
        "\n",
        "# Supongamos que 'df' es tu DataFrame y que 'clase' es tu variable objetivo\n",
        "df = pd.read_csv('tu_archivo.csv')\n",
        "\n",
        "# Crear las variables objetivo y características\n",
        "X = df.drop('clase', axis=1)\n",
        "y = df['clase']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento, validación y prueba\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Crear el modelo de Random Forest\n",
        "modelo = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Definir la grilla de hiperparámetros para la búsqueda aleatoria\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200, 300, 400],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth': [10, 20, 30, 40, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda aleatoria de hiperparámetros\n",
        "busqueda_aleatoria = RandomizedSearchCV(estimator=modelo,\n",
        "                                        param_distributions=param_dist,\n",
        "                                        n_iter=100,\n",
        "                                        cv=3,\n",
        "                                        verbose=2,\n",
        "                                        random_state=42,\n",
        "                                        n_jobs=-1)\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros encontrados\n",
        "busqueda_aleatoria.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de validación y prueba\n",
        "probas_val = busqueda_aleatoria.predict_proba(X_val)\n",
        "probas_test = busqueda_aleatoria.predict_proba(X_test)\n",
        "\n",
        "# Imprimir un informe de clasificación para validación y prueba\n",
        "print(\"Informe de clasificación para validación:\")\n",
        "print(classification_report(y_val, probas_val))\n",
        "print(\"Informe de clasificación para prueba:\")\n",
        "print(classification_report(y_test, probas_test))\n",
        "\n",
        "# Calcular e imprimir la matriz de confusión con los nombres de las clases para validación y prueba\n",
        "cm_val = confusion_matrix(y_val, probas_val)\n",
        "cm_test = confusion_matrix(y_test, probas_test)\n",
        "cm_df_val = pd.DataFrame(cm_val,\n",
        "                     index=[i for i in busqueda_aleatoria.classes_],\n",
        "                     columns=[i for i in busqueda_aleatoria.classes_])\n",
        "cm_df_test = pd.DataFrame(cm_test,\n",
        "                     index=[i for i in busqueda_aleatoria.classes_],\n",
        "                     columns=[i for i in busqueda_aleatoria.classes_])\n",
        "print(\"Matriz de confusión para validación:\")\n",
        "print(cm_df_val)\n",
        "print(\"Matriz de confusión para prueba:\")\n",
        "print(cm_df_test)\n",
        "\n",
        "# Imprimir la precisión del modelo para validación y prueba\n",
        "print(\"Precisión del modelo para validación:\")\n",
        "print(accuracy_score(y_val, probas_val))\n",
        "print(\"Precisión del modelo para prueba:\")\n",
        "print(accuracy_score(y_test, probas_test))\n",
        "\n",
        "# Calcular e imprimir la puntuación AUC-ROC para cada clase para validación y prueba\n",
        "lb = LabelBinarizer()\n",
        "lb.fit(y_val)\n",
        "y_val_lb = lb.transform(y_val)\n",
        "probas_val_lb = lb.transform(probas_val)\n",
        "y_test_lb = lb.transform(y_test)\n",
        "probas_test_lb = lb.transform(probas_test)\n",
        "print(\"Puntuación AUC-ROC para cada clase para validación:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    print(f\"{lb.classes_[i]}: {roc_auc_score(y_val_lb[:, i], probas_val_lb[:, i])}\")\n",
        "print(\"Puntuación AUC-ROC para cada clase para prueba:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    print(f\"{lb.classes_[i]}: {roc_auc_score(y_test_lb[:, i], probas_test_lb[:, i])}\")\n",
        "\n",
        "# Función para calcular el valor KS\n",
        "def calc_ks(data):\n",
        "    data['bucket']=(data['probability'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().probability.to_frame(name='min_prob')\n",
        "    kstable['max_prob']=grouped.max().probability\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "# Función para calcular el coeficiente Gini\n",
        "def calc_gini(y_verdadero, y_prob):\n",
        "    fpr, tpr, _ = roc_curve(y_verdadero, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    gini = 2*roc_auc - 1\n",
        "    return gini\n",
        "\n",
        "# Calcular e imprimir el valor KS y el coeficiente Gini para cada clase para validación y prueba\n",
        "print(\"Valor KS y coeficiente Gini para cada clase para validación:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    data = pd.DataFrame({'label': y_val_lb[:, i], 'probability': probas_val[:, i]})\n",
        "    data['good']=(data['label']==0).astype(int)\n",
        "    data['bad']=(data['label']==1).astype(int)\n",
        "    ks = calc_ks(data)\n",
        "    gini = calc_gini(y_val_lb[:, i], probas_val[:, i])\n",
        "    print(f\"{lb.classes_[i]}: KS = {ks}, Gini = {gini}\")\n",
        "print(\"Valor KS y coeficiente Gini para cada clase para prueba:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    data = pd.DataFrame({'label': y_test_lb[:, i], 'probability': probas_test[:, i]})\n",
        "    data['good']=(data['label']==0).astype(int)\n",
        "    data['bad']=(data['label']==1).astype(int)\n",
        "    ks = calc_ks(data)\n",
        "    gini = calc_gini(y_test_lb[:, i], probas_test[:, i])\n",
        "    print(f\"{lb.classes_[i]}: KS = {ks}, Gini = {gini}\")\n"
      ],
      "metadata": {
        "id": "r15QARqDXV94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "# Asumiendo que tu DataFrame se llama df y tus columnas son 'ope' y 'conta'\n",
        "df = df.withColumn(\"ope\", to_date(df['ope'], \"yyyy-MM-dd\"))\n",
        "df = df.withColumn(\"conta\", to_date(df['conta'], \"yyyy-MM-dd\"))\n",
        "\n",
        "# Agrupar por la columna 'ope'\n",
        "df_grouped_by_ope = df.groupBy(\"ope\").count()\n",
        "\n",
        "# Agrupar por la columna 'conta'\n",
        "df_grouped_by_conta = df.groupBy(\"conta\").count()"
      ],
      "metadata": {
        "id": "uW9K8n1zeQzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Establecer la semilla del generador de números aleatorios\n",
        "np.random.seed(42)\n",
        "\n",
        "# Supongamos que 'df' es tu DataFrame y que 'clase' es tu variable objetivo\n",
        "df = pd.read_csv('tu_archivo.csv')\n",
        "\n",
        "# Crear las variables objetivo y características\n",
        "X = df.drop('clase', axis=1)\n",
        "y = df['clase']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Crear una instancia del clasificador Gradient Boosting\n",
        "modelo = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Definir la grilla de hiperparámetros para la búsqueda en grilla\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda en grilla de hiperparámetros\n",
        "busqueda_en_grilla = GridSearchCV(estimator=modelo,\n",
        "                                  param_grid=param_grid,\n",
        "                                  cv=3,\n",
        "                                  verbose=2,\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros encontrados\n",
        "busqueda_en_grilla.fit(X_train, y_train)\n",
        "\n",
        "# Predecir las clases para el conjunto de prueba\n",
        "y_pred = busqueda_en_grilla.predict(X_test)\n",
        "\n",
        "# Definir tus umbrales personalizados aquí. Por ejemplo:\n",
        "umbrales = [0.4, 0.3, 0.3]  # Un umbral para cada clase\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "probas = busqueda_en_grilla.predict_proba(X_test)\n",
        "predicciones = np.array([busqueda_en_grilla.classes_[i] for i in np.argmax(probas >= umbrales, axis=1)])\n",
        "\n",
        "# Imprimir un informe de clasificación\n",
        "print(\"Informe de clasificación:\")\n",
        "print(classification_report(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la matriz de confusión con los nombres de las clases\n",
        "cm = confusion_matrix(y_test, predicciones)\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index=[i for i in busqueda_en_grilla.classes_],\n",
        "                     columns=[i for i in busqueda_en_grilla.classes_])\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_df)\n",
        "\n",
        "# Imprimir la precisión del modelo\n",
        "print(\"Precisión del modelo:\")\n",
        "print(accuracy_score(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la puntuación AUC-ROC para cada clase\n",
        "lb = LabelBinarizer()\n",
        "lb.fit(y_test)\n",
        "y_test_lb = lb.transform(y_test)\n",
        "predicciones_lb = lb.transform(predicciones)\n",
        "print(\"Puntuación AUC-ROC para cada clase:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    print(f\"{lb.classes_[i]}: {roc_auc_score(y_test_lb[:, i], predicciones_lb[:, i])}\")\n"
      ],
      "metadata": {
        "id": "Sq2TVcRL0Dej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
        "\n",
        "# Establecer la semilla para la reproducibilidad\n",
        "np.random.seed(0)\n",
        "\n",
        "# Supongamos que X es tu conjunto de datos e y son las etiquetas correspondientes\n",
        "\n",
        "# Crear el codificador\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Ajustar el codificador y transformar las etiquetas\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Convertir los datos a float32\n",
        "X = X.astype(np.float32)\n",
        "y = y.astype(np.float32)\n",
        "\n",
        "# División de los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Creación del modelo MLP más profundo\n",
        "model = Sequential()\n",
        "model.add(Dense(50, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))  # Tres neuronas en la capa de salida para los tres niveles de la variable dependiente\n",
        "\n",
        "# Compilación del modelo\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Asignación de pesos a las clases (supongamos que class_weights es un diccionario que contiene los pesos)\n",
        "class_weights = {0: 1., 1: 1., 2: 1.}  # Modifica esto según tus necesidades\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "model.fit(X_train, y_train, epochs=100, verbose=0, class_weight=class_weights)\n",
        "\n",
        "# Predicción en el conjunto de prueba\n",
        "y_pred_proba = model.predict(X_test)\n",
        "\n",
        "# Definir los umbrales de decisión para cada clase\n",
        "thresholds = [0.5, 0.5, 0.5]  # Modifica esto según tus necesidades\n",
        "\n",
        "y_pred = np.zeros_like(y_pred_proba)\n",
        "for i in range(3):\n",
        "    y_pred[:, i] = (y_pred_proba[:, i] > thresholds[i]).astype(int)\n",
        "\n",
        "y_pred = np.argmax(y_pred, axis=1)  # Convertir las predicciones one-hot a etiquetas\n",
        "\n",
        "# Calcular la matriz de confusión para todas las categorías\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Crear un DataFrame para visualizar la matriz de confusión con los nombres de las clases\n",
        "class_names = ['detractor', 'neutro', 'promotor']  # Modifica esto según tus necesidades\n",
        "confusion_df = pd.DataFrame(confusion, index=class_names, columns=class_names)\n",
        "\n",
        "print(\"Confusion Matrix: \\n\", confusion_df)\n",
        "\n",
        "# Calcular las métricas para cada categoría\n",
        "for i in range(3):\n",
        "    precision = precision_score(y_test == i, y_pred == i)\n",
        "    recall = recall_score(y_test == i, y_pred == i)\n",
        "    f1 = f1_score(y_test == i, y_pred == i)\n",
        "\n",
        "    # Calcular la curva ROC y el AUC-ROC\n",
        "    fpr, tpr, _ = roc_curve(y_test == i, y_pred_proba[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    print(f\"Clase {class_names[i]}:\")\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1 Score: \", f1)\n",
        "    print(\"ROC AUC Score: \", roc_auc)\n"
      ],
      "metadata": {
        "id": "l0E6y2lWoCAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular las métricas ponderadas para cada categoría\n",
        "for i in range(3):\n",
        "    precision = precision_score(y_test == i, y_pred == i, average='weighted')\n",
        "    recall = recall_score(y_test == i, y_pred == i, average='weighted')\n",
        "    f1 = f1_score(y_test == i, y_pred == i, average='weighted')\n",
        "\n",
        "    print(f\"Clase {class_names[i]} ponderada:\")\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1 Score: \", f1)\n",
        "\n",
        "### a mano\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Supongamos que 'y_test' son tus etiquetas verdaderas e 'y_pred' son tus predicciones\n",
        "f1_scores = [f1_score(y_test == i, y_pred == i) for i in range(3)]\n",
        "class_counts = [sum(y_test == i) for i in range(3)]\n",
        "total_count = len(y_test)\n",
        "\n",
        "weighted_f1_score = sum(f1 * count for f1, count in zip(f1_scores, class_counts)) / total_count"
      ],
      "metadata": {
        "id": "zqAlf41JoIka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve, auc, confusion_matrix\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Establecer la semilla del generador de números aleatorios\n",
        "np.random.seed(42)\n",
        "\n",
        "# Supongamos que 'df' es tu DataFrame y que 'clase' es tu variable objetivo\n",
        "df = pd.read_csv('tu_archivo.csv')\n",
        "\n",
        "# Crear las variables objetivo y características\n",
        "X = df.drop('clase', axis=1)\n",
        "y = df['clase']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Crear una instancia del clasificador Gradient Boosting\n",
        "modelo = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Definir la grilla de hiperparámetros para la búsqueda en grilla\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda en grilla de hiperparámetros\n",
        "busqueda_en_grilla = GridSearchCV(estimator=modelo,\n",
        "                                  param_grid=param_grid,\n",
        "                                  cv=3,\n",
        "                                  verbose=2,\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros encontrados\n",
        "busqueda_en_grilla.fit(X_train, y_train)\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de prueba\n",
        "probas = busqueda_en_grilla.predict_proba(X_test)\n",
        "\n",
        "# Definir tus umbrales personalizados aquí. Por ejemplo:\n",
        "umbrales = [0.4, 0.3, 0.3]  # Un umbral para cada clase\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "predicciones = np.array([busqueda_en_grilla.classes_[i] for i in np.argmax(probas >= umbrales, axis=1)])\n",
        "\n",
        "# Imprimir un informe de clasificación\n",
        "print(\"Informe de clasificación:\")\n",
        "print(classification_report(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la matriz de confusión con los nombres de las clases\n",
        "cm = confusion_matrix(y_test, predicciones)\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index=[i for i in busqueda_en_grilla.classes_],\n",
        "                     columns=[i for i in busqueda_en_grilla.classes_])\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_df)\n",
        "\n",
        "# Imprimir la precisión del modelo\n",
        "print(\"Precisión del modelo:\")\n",
        "print(accuracy_score(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la puntuación AUC-ROC para cada clase\n",
        "lb = LabelBinarizer()\n",
        "lb.fit(y_test)\n",
        "y_test_lb = lb.transform(y_test)\n",
        "predicciones_lb = lb.transform(predicciones)\n",
        "print(\"Puntuación AUC-ROC para cada clase:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    print(f\"{lb.classes_[i]}: {roc_auc_score(y_test_lb[:, i], predicciones_lb[:, i])}\")\n",
        "\n",
        "# Función para calcular el valor KS\n",
        "def calc_ks(data):\n",
        "    data['good']=(data['label']==0).astype(int)\n",
        "    data['bad']=(data['label']==1).astype(int)\n",
        "    data['bucket']=(data['probability'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().probability.to_frame(name='min_prob')\n",
        "    kstable['max_prob']=grouped.max().probability\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "# Función para calcular el coeficiente Gini\n",
        "def calc_gini(y_verdadero, y_prob):\n",
        "    fpr, tpr, _ = roc_curve(y_verdadero, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    gini = 2*roc_auc - 1\n",
        "    return gini\n",
        "\n",
        "# Calcular e imprimir el valor KS y el coeficiente Gini para cada clase\n",
        "print(\"Valor KS y coeficiente Gini para cada clase:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    data = pd.DataFrame({'label': y_test_lb[:, i], 'probability': probas[:, i]})\n",
        "    data['good']=(data['label']==0).astype(int)\n",
        "    data['bad']=(data['label']==1).astype(int)\n",
        "    ks = calc_ks(data)\n",
        "    gini = calc_gini(y_test_lb[:, i], probas[:, i])\n",
        "    print(f\"{lb.classes_[i]}: KS = {ks}, Gini = {gini}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "iokLgqgQBwRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
        "\n",
        "# Establecer la semilla para la reproducibilidad\n",
        "np.random.seed(0)\n",
        "\n",
        "# Supongamos que X2 es tu conjunto de datos e y2 son las etiquetas correspondientes\n",
        "X2 = X.copy()\n",
        "y2 = y.copy()\n",
        "\n",
        "# Crear el codificador\n",
        "le2 = LabelEncoder()\n",
        "\n",
        "# Ajustar el codificador y transformar las etiquetas\n",
        "y2 = le2.fit_transform(y2)\n",
        "\n",
        "# Convertir los datos a float32\n",
        "X2 = X2.astype(np.float32)\n",
        "y2 = y2.astype(np.float32)\n",
        "\n",
        "# División de los datos en conjuntos de entrenamiento y prueba\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=0)\n",
        "\n",
        "# Creación del modelo MLP más profundo\n",
        "model2 = Sequential()\n",
        "model2.add(Dense(50, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model2.add(Dense(50, activation='relu'))\n",
        "model2.add(Dense(50, activation='relu'))\n",
        "model2.add(Dense(3, activation='softmax'))  # Tres neuronas en la capa de salida para los tres niveles de la variable dependiente\n",
        "\n",
        "# Compilación del modelo\n",
        "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Asignación de pesos a las clases (supongamos que class_weights es un diccionario que contiene los pesos)\n",
        "class_weights = {0: 1., 1: 1., 2: 1.}  # Modifica esto según tus necesidades\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "model2.fit(X_train2, y_train2, epochs=100, verbose=0, class_weight=class_weights)\n",
        "\n",
        "# Predicción en el conjunto de prueba\n",
        "y_pred_proba2 = model.predict(X_test)\n",
        "\n",
        "# Definir los umbrales de decisión para cada clase\n",
        "thresholds = [0.5, 0.5, 0.5]  # Modifica esto según tus necesidades\n",
        "\n",
        "y_pred2 = np.zeros_like(y_pred_proba)\n",
        "for i in range(3):\n",
        "    y_pred[:, i] = (y_pred_proba[:, i] > thresholds[i]).astype(int)\n",
        "\n",
        "y_pred = np.argmax(y_pred, axis=1)  # Convertir las predicciones one-hot a etiquetas\n",
        "\n",
        "# Calcular la matriz de confusión para todas las categorías\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Crear un DataFrame para visualizar la matriz de confusión con los nombres de las clases\n",
        "class_names = ['detractor', 'neutro', 'promotor']  # Modifica esto según tus necesidades\n",
        "confusion_df = pd.DataFrame(confusion, index=class_names, columns=class_names)\n",
        "\n",
        "print(\"Confusion Matrix: \\n\", confusion_df)\n",
        "\n",
        "# Calcular las métricas para cada categoría\n",
        "for i in range(3):\n",
        "    precision = precision_score(y_test == i, y_pred == i)\n",
        "    recall = recall_score(y_test == i, y_pred == i)\n",
        "    f1 = f1_score(y_test == i, y_pred == i)\n",
        "\n",
        "    # Calcular la curva ROC y el AUC-ROC\n",
        "    fpr, tpr, _ = roc_curve(y_test == i, y_pred_proba[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    print(f\"Clase {class_names[i]}:\")\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1 Score: \", f1)\n",
        "    print(\"ROC AUC Score: \", roc_auc)\n",
        "\n",
        "# Función para calcular el valor KS\n",
        "def calc_ks(data):\n",
        "    data['good']=(data['label']==0).astype(int)\n",
        "    data['bad']=(data['label']==1).astype(int)\n",
        "    data['bucket']=(data['probability'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().probability.to_frame(name='min_prob')\n",
        "    kstable['max_prob']=grouped.max().probability\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "# Función para calcular el coeficiente Gini\n",
        "def calc_gini(y_verdadero, y_prob):\n",
        "    fpr, tpr, _ = roc_curve(y_verdadero, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    gini = 2*roc_auc - 1\n",
        "    return gini\n",
        "\n",
        "# Calcular e imprimir el valor KS y el coeficiente Gini para cada clase\n",
        "print(\"Valor KS y coeficiente Gini para cada clase:\")\n",
        "for i in range(len(class_names)):\n",
        "    data = pd.DataFrame({'label': (y_test == i).astype(int), 'probability': y_pred_proba[:, i]})\n",
        "    data['good']=(data['label']==0).astype(int)\n",
        "    data['bad']=(data['label']==1).astype(int)\n",
        "    ks = calc_ks(data)\n",
        "    gini = calc_gini((y_test == i).astype(int), y_pred_proba[:, i])\n",
        "    print(f\"{class_names[i]}: KS = {ks}, Gini = {gini}\")\n",
        "\n",
        "\n",
        "    from scipy.optimize import differential_evolution\n",
        "\n",
        "# Definir la función objetivo para la optimización\n",
        "def objective(thresholds):\n",
        "    # Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "    y_pred = np.zeros_like(y_pred_proba2)\n",
        "    for i in range(3):\n",
        "        y_pred[:, i] = (y_pred_proba2[:, i] > thresholds[i]).astype(int)\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    # Calcular las métricas\n",
        "    roc_auc_scores = []\n",
        "    ks_values = []\n",
        "    gini_values = []\n",
        "    for i in range(len(class_names)):\n",
        "        data = pd.DataFrame({'label': (y_test2 == i).astype(int), 'probability': y_pred_proba2[:, i]})\n",
        "        data['good']=(data['label']==0).astype(int)\n",
        "        data['bad']=(data['label']==1).astype(int)\n",
        "        ks = calc_ks(data)\n",
        "        gini = calc_gini((y_test2 == i).astype(int), y_pred_proba2[:, i])\n",
        "        fpr, tpr, _ = roc_curve((y_test2 == i).astype(int), y_pred_proba2[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        roc_auc_scores.append(roc_auc)\n",
        "        ks_values.append(ks)\n",
        "        gini_values.append(gini)\n",
        "\n",
        "    # Calcular el promedio de las métricas\n",
        "    avg_roc_auc = np.mean(roc_auc_scores)\n",
        "    avg_ks = np.mean(ks_values)\n",
        "    avg_gini = np.mean(gini_values)\n",
        "\n",
        "    # La función objetivo es el negativo del promedio de las métricas porque queremos maximizarlas\n",
        "    return -(avg_roc_auc + avg_ks + avg_gini) / 3\n",
        "\n",
        "# Definir los límites para los umbrales\n",
        "bounds = [(0, 1)] * 3\n",
        "\n",
        "# Realizar la optimización\n",
        "result = differential_evolution(objective, bounds)\n",
        "\n",
        "# Imprimir los mejores umbrales encontrados\n",
        "print(\"Mejores umbrales: \", result.x)\n",
        "\n",
        "\n",
        "\n",
        "from scipy.optimize import differential_evolution\n",
        "\n",
        "# Definir la función objetivo para la optimización\n",
        "def objective(weights):\n",
        "    # Asignar los pesos a las clases\n",
        "    class_weights = {i: weights[i] for i in range(len(weights))}\n",
        "\n",
        "    # Entrenar el modelo con los pesos de las clases\n",
        "    model2.fit(X_train2, y_train2, epochs=100, verbose=0, class_weight=class_weights)\n",
        "\n",
        "    # Predicción en el conjunto de prueba\n",
        "    y_pred_proba2 = model.predict(X_test)\n",
        "\n",
        "    # Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "    y_pred = np.zeros_like(y_pred_proba)\n",
        "    for i in range(3):\n",
        "        y_pred[:, i] = (y_pred_proba[:, i] > thresholds[i]).astype(int)\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    # Calcular las métricas\n",
        "    roc_auc_scores = []\n",
        "    ks_values = []\n",
        "    gini_values = []\n",
        "    for i in range(len(class_names)):\n",
        "        data = pd.DataFrame({'label': (y_test2 == i).astype(int), 'probability': y_pred_proba2[:, i]})\n",
        "        data['good']=(data['label']==0).astype(int)\n",
        "        data['bad']=(data['label']==1).astype(int)\n",
        "        ks = calc_ks(data)\n",
        "        gini = calc_gini((y_test2 == i).astype(int), y_pred_proba2[:, i])\n",
        "        fpr, tpr, _ = roc_curve((y_test2 == i).astype(int), y_pred_proba2[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        roc_auc_scores.append(roc_auc)\n",
        "        ks_values.append(ks)\n",
        "        gini_values.append(gini)\n",
        "\n",
        "    # Calcular el promedio de las métricas\n",
        "    avg_roc_auc = np.mean(roc_auc_scores)\n",
        "    avg_ks = np.mean(ks_values)\n",
        "    avg_gini = np.mean(gini_values)\n",
        "\n",
        "    # La función objetivo es el negativo del promedio de las métricas porque queremos maximizarlas\n",
        "    return -(avg_roc_auc + avg_ks + avg_gini) / 3\n",
        "\n",
        "# Definir los límites para los pesos (supongamos que los pesos están entre 0 y 10)\n",
        "bounds = [(0, 10)] * 3\n",
        "\n",
        "# Realizar la optimización\n",
        "result = differential_evolution(objective, bounds)\n",
        "\n",
        "# Imprimir los mejores pesos encontrados\n",
        "print(\"Mejores pesos: \", result.x)\n"
      ],
      "metadata": {
        "id": "cqjoSWZzdpyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Establecer la semilla del generador de números aleatorios\n",
        "np.random.seed(42)\n",
        "\n",
        "# Supongamos que 'df' es tu DataFrame y que 'clase' es tu variable objetivo\n",
        "df = pd.read_csv('tu_archivo.csv')\n",
        "\n",
        "# Crear las variables objetivo y características\n",
        "X = df.drop('clase', axis=1)\n",
        "y = df['clase']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Crear el modelo SVM\n",
        "modelo = SVC(probability=True, random_state=42)\n",
        "\n",
        "# Definir la grilla de hiperparámetros para la búsqueda aleatoria\n",
        "param_dist = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': [1, 0.1, 0.01],\n",
        "    'kernel': ['rbf', 'poly']\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda aleatoria de hiperparámetros\n",
        "busqueda_aleatoria = RandomizedSearchCV(estimator=modelo,\n",
        "                                        param_distributions=param_dist,\n",
        "                                        n_iter=10,\n",
        "                                        cv=3,\n",
        "                                        verbose=2,\n",
        "                                        random_state=42,\n",
        "                                        n_jobs=-1)\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros encontrados\n",
        "busqueda_aleatoria.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de prueba\n",
        "probas = busqueda_aleatoria.predict_proba(X_test)\n",
        "\n",
        "# Definir tus umbrales personalizados aquí. Por ejemplo:\n",
        "umbrales = [0.4, 0.3, 0.3]  # Un umbral para cada clase\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "predicciones = np.array([busqueda_aleatoria.classes_[i] for i in np.argmax(probas >= umbrales, axis=1)])\n",
        "\n",
        "# Imprimir un informe de clasificación\n",
        "print(\"Informe de clasificación:\")\n",
        "print(classification_report(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la matriz de confusión con los nombres de las clases\n",
        "cm = confusion_matrix(y_test, predicciones)\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index=[i for i in busqueda_aleatoria.classes_],\n",
        "                     columns=[i for i in busqueda_aleatoria.classes_])\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_df)\n",
        "\n",
        "# Imprimir la precisión del modelo\n",
        "print(\"Precisión del modelo:\")\n",
        "print(accuracy_score(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la puntuación AUC-ROC para cada clase\n",
        "lb = LabelBinarizer()\n",
        "lb.fit(y_test)\n",
        "y_test_lb = lb.transform(y_test)\n",
        "predicciones_lb = lb.transform(predicciones)\n",
        "print(\"Puntuación AUC-ROC para cada clase:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    print(f\"{lb.classes_[i]}: {roc_auc_score(y_test_lb[:, i], predicciones_lb[:, i])}\")\n",
        "\n",
        "# Función para calcular el valor KS\n",
        "def calc_ks(data):\n",
        "    data['good']=(data['label']==0).astype(int)\n",
        "    data['bad']=(data['label']==1).astype(int)\n",
        "    data['bucket']=(data['probability'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().probability.to_frame(name='min_prob')\n",
        "    kstable['max_prob']=grouped.max().probability\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "# Función para calcular el coeficiente Gini\n",
        "def calc_gini(y_verdadero, y_prob):\n",
        "    fpr, tpr, _ = roc_curve(y_verdadero, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    gini = 2*roc_auc - 1\n",
        "    return gini\n",
        "\n",
        "# Calcular e imprimir el valor KS y el coeficiente Gini para cada clase\n",
        "print(\"Valor KS y coeficiente Gini para cada clase:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    data = pd.DataFrame({'label': (y_test == i).astype(int), 'probability': probas[:, i]})\n",
        "    data['good']=(data['label']==0).astype(int)\n",
        "    data['bad']=(data['label']==1).astype(int)\n",
        "    ks = calc_ks(data)\n",
        "    gini = calc_gini((y_test == i).astype(int), probas[:, i])\n",
        "    print(f\"{lb.classes_[i]}: KS = {ks}, Gini = {gini}\")"
      ],
      "metadata": {
        "id": "5w5wzbXPxnwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RelacionarTablas\").getOrCreate()\n",
        "\n",
        "# Carga las tablas\n",
        "Pedidos = spark.read.table(\"Pedidos\")\n",
        "DetallePedidos = spark.read.table(\"DetallePedidos\")\n",
        "Productos = spark.read.table(\"Productos\")\n",
        "CodSubCategoria = spark.read.table(\"CodSubCategoria\")\n",
        "CodCategoria = spark.read.table(\"CodCategoria\")\n",
        "Clientes = spark.read.table(\"Clientes\")\n",
        "Ciudades = spark.read.table(\"Ciudades\")\n",
        "Paises = spark.read.table(\"Paises\")\n",
        "ModoEnvio = spark.read.table(\"ModoEnvio\")\n",
        "Segmento = spark.read.table(\"Segmento\")\n",
        "\n",
        "# Relaciona las tablas\n",
        "df1 = Pedidos.join(DetallePedidos, Pedidos.NroPedido == DetallePedidos.NroPedido)\n",
        "df2 = df1.join(Productos, df1.IDProducto == Productos.NroProducto)\n",
        "df3 = df2.join(CodSubCategoria, df2.CodSubCategoria == CodSubCategoria.CODSubCategoria)\n",
        "df4 = df3.join(CodCategoria, df3.CODCategoria == CodCategoria.CODCategoria)\n",
        "df5 = df4.join(Clientes, df4.IDCliente == Clientes.NroCliente)\n",
        "df6 = df5.join(Ciudades, df5.CiudadDespacho == Ciudades.CODCiudad)\n",
        "df7 = df6.join(Paises, Ciudades.Pais == Paises.CodigoPais)\n",
        "df8 = df7.join(ModoEnvio, df7.IDModoEnvio == ModoEnvio.CODModoEnvio)\n",
        "final_df = df8.join(Segmento, Clientes.IDSegmento == Segmento.CODSegmento)\n",
        "\n",
        "# Muestra el resultado\n",
        "final_df.show()"
      ],
      "metadata": {
        "id": "3VrY0NU127CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "# Asegúrate de que 'Marfen' y 'Precio' sean de tipo numérico\n",
        "final_df = final_df.withColumn(\"Marfen\", final_df[\"Marfen\"].cast(\"double\"))\n",
        "final_df = final_df.withColumn(\"Precio\", final_df[\"Precio\"].cast(\"double\"))\n",
        "\n",
        "# Crea un VectorAssembler que combina las columnas de características en una sola columna de vector\n",
        "vectorAssembler = VectorAssembler(inputCols=[\"Marfen\"], outputCol=\"features\")\n",
        "\n",
        "# Transforma los datos\n",
        "df = vectorAssembler.transform(final_df)\n",
        "\n",
        "# Crea el modelo de regresión lineal\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Precio\")\n",
        "\n",
        "# Ajusta el modelo a los datos\n",
        "lr_model = lr.fit(df)\n",
        "\n",
        "# Imprime los coeficientes y la intersección del modelo\n",
        "print(\"Coeficientes: \" + str(lr_model.coefficients))\n",
        "print(\"Intersección: \" + str(lr_model.intercept))"
      ],
      "metadata": {
        "id": "_E-OdABM6SMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Supongamos que X son tus datos\n",
        "# X = ...\n",
        "\n",
        "# Calcula las sumas de los cuadrados dentro del cluster para diferentes valores de k\n",
        "sse = []\n",
        "silhouette = []\n",
        "list_k = list(range(2, 10))\n",
        "\n",
        "for k in list_k:\n",
        "    km = KMeans(n_clusters=k)\n",
        "    km.fit(X)\n",
        "\n",
        "    # suma de los cuadrados dentro del cluster\n",
        "    sse.append(km.inertia_)\n",
        "\n",
        "    # coeficiente de silueta\n",
        "    silhouette.append(silhouette_score(X, km.labels_))\n",
        "\n",
        "# Grafica sse en función de k\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(list_k, sse, '-o')\n",
        "plt.xlabel(r'Número de clusters *k*')\n",
        "plt.ylabel('Suma de los cuadrados dentro del cluster')\n",
        "plt.show()\n",
        "\n",
        "# Grafica el coeficiente de silueta en función de k\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(list_k, silhouette, '-o')\n",
        "plt.xlabel(r'Número de clusters *k*')\n",
        "plt.ylabel('Coeficiente de Silueta')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tnd60riDN5iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Supongamos que df es tu DataFrame y las columnas son 'ModoEnvio', 'FechaPedido' y 'FechaDespacho'\n",
        "# df = ...\n",
        "\n",
        "# Calcula la diferencia en días entre la fecha de pedido y la fecha de despacho\n",
        "df['Dias'] = (df['FechaDespacho'] - df['FechaPedido']).dt.days\n",
        "\n",
        "# Obtiene los modos de envío únicos\n",
        "modos_envio = df['ModoEnvio'].unique()\n",
        "\n",
        "# Realiza un test t para cada par de modos de envío\n",
        "for i in range(len(modos_envio)):\n",
        "    for j in range(i+1, len(modos_envio)):\n",
        "        modo_envio1 = df[df['ModoEnvio'] == modos_envio[i]]['Dias']\n",
        "        modo_envio2 = df[df['ModoEnvio'] == modos_envio[j]]['Dias']\n",
        "\n",
        "        t_stat, p_val = stats.ttest_ind(modo_envio1, modo_envio2)\n",
        "\n",
        "        print(f\"Modo de envío: {modos_envio[i]} vs {modos_envio[j]}\")\n",
        "        print(f\"Estadística t: {t_stat}\")\n",
        "        print(f\"Valor p: {p_val}\\n\")"
      ],
      "metadata": {
        "id": "LkJJOZqaT6IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Supongamos que df es tu DataFrame\n",
        "# df = ...\n",
        "\n",
        "# Obtiene la lista de columnas categóricas\n",
        "categoricas = [col.name for col in df.schema.fields if col.dataType == StringType()]\n",
        "\n",
        "print(\"Variables categóricas: \", categoricas)"
      ],
      "metadata": {
        "id": "oIw8GVlyjBZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RelacionarTablas\").getOrCreate()\n",
        "\n",
        "# Carga las tablas\n",
        "Pedidos = spark.read.table(\"Pedidos\").withColumnRenamed(\"NroPedido\", \"NumeroPedido\")\n",
        "DetallePedidos = spark.read.table(\"DetallePedidos\").withColumnRenamed(\"NroPedido\", \"NumeroPedido\")\n",
        "Productos = spark.read.table(\"Productos\").withColumnRenamed(\"CodSubCategoria\", \"CodSubCategoriaProductos\")\n",
        "CodSubCategoria = spark.read.table(\"CodSubCategoria\").withColumnRenamed(\"CODCategoria\", \"CODCategoriaSub\")\n",
        "CodCategoria = spark.read.table(\"CodCategoria\")\n",
        "Clientes = spark.read.table(\"Clientes\")\n",
        "Ciudades = spark.read.table(\"Ciudades\")\n",
        "Paises = spark.read.table(\"Paises\")\n",
        "ModoEnvio = spark.read.table(\"ModoEnvio\")\n",
        "Segmento = spark.read.table(\"Segmento\")\n",
        "\n",
        "# Relaciona las tablas\n",
        "df1 = Pedidos.join(DetallePedidos, Pedidos.NumeroPedido == DetallePedidos.NumeroPedido)\n",
        "df2 = df1.join(Productos, df1.IDProducto == Productos.NroProducto)\n",
        "df3 = df2.join(CodSubCategoria, df2.CodSubCategoriaProductos == CodSubCategoria.CODSubCategoria)\n",
        "df4 = df3.join(CodCategoria, df3.CODCategoriaSub == CodCategoria.CODCategoria)\n",
        "df5 = df4.join(Clientes, df4.IDCliente == Clientes.NroCliente)\n",
        "df6 = df5.join(Ciudades, df5.CiudadDespacho == Ciudades.CODCiudad)\n",
        "df7 = df6.join(Paises, Ciudades.Pais == Paises.CodigoPais)\n",
        "df8 = df7.join(ModoEnvio, df7.IDModoEnvio == ModoEnvio.CODModoEnvio)\n",
        "final_df = df8.join(Segmento, Clientes.IDSegmento == Segmento.CODSegmento)\n",
        "\n",
        "# Muestra el resultado\n",
        "final_df.show()"
      ],
      "metadata": {
        "id": "MNNczW40mRTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Supongamos que df es tu DataFrame y categoricas es una lista de las columnas categóricas\n",
        "# df = ...\n",
        "# categoricas = ...\n",
        "\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df) for column in categoricas]\n",
        "encoder = OneHotEncoder(inputCols=[indexer.getOutputCol() for indexer in indexers], outputCols=[\"{0}_encoded\".format(indexer.getOutputCol()) for indexer in indexers])\n",
        "\n",
        "pipeline = Pipeline(stages=indexers + [encoder])\n",
        "df = pipeline.fit(df).transform(df)"
      ],
      "metadata": {
        "id": "0KKRSKCjeC5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# Lista de columnas categóricas\n",
        "cat_cols = [item[0] for item in final_df.dtypes if item[1].startswith('string')]\n",
        "\n",
        "indexers = []\n",
        "\n",
        "for column in cat_cols:\n",
        "    if column + \"_index\" not in final_df.columns:\n",
        "        indexer = StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(final_df)\n",
        "        indexers.append(indexer)\n",
        "\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "final_df = pipeline.fit(final_df).transform(final_df)\n",
        "\n",
        "# Lista de columnas para el VectorAssembler\n",
        "input_cols = [column+\"_index\" for column in cat_cols]\n",
        "\n",
        "if \"features\" not in final_df.columns:\n",
        "    vec_assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
        "    final_df = vec_assembler.transform(final_df)\n",
        "\n",
        "# Aplicar PCA\n",
        "pca = PCA(k=min(final_df.select(\"features\").count(), 27), inputCol=\"features\", outputCol=\"pca_features\")\n",
        "model = pca.fit(final_df)\n",
        "result = model.transform(final_df)\n",
        "\n",
        "# Encontrar el número de componentes necesarios para explicar el 95% de la varianza\n",
        "variances = model.explainedVariance\n",
        "cum_sum = variances.cumsum()\n",
        "n_components = len(cum_sum[cum_sum < 0.95]) + 1\n",
        "\n",
        "print(f\"Se necesitan {n_components} componentes para explicar el 95% de la varianza.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UBnmu3iESAYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Lista de columnas categóricas\n",
        "cat_cols = [item[0] for item in final_df.dtypes if item[1].startswith('string')]\n",
        "\n",
        "indexers = []\n",
        "\n",
        "for column in cat_cols:\n",
        "    if column + \"_index\" not in final_df.columns:\n",
        "        indexer = StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(final_df)\n",
        "        indexers.append(indexer)\n",
        "\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "final_df = pipeline.fit(final_df).transform(final_df)\n",
        "\n",
        "# Lista de columnas para el VectorAssembler\n",
        "input_cols = [column+\"_index\" for column in cat_cols]\n",
        "\n",
        "if \"features\" not in final_df.columns:\n",
        "    vec_assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
        "    final_df = vec_assembler.transform(final_df)\n",
        "\n",
        "# Aplicar PCA\n",
        "if \"pca_features\" not in final_df.columns:\n",
        "    pca = PCA(k=min(final_df.select(\"features\").count(), 27), inputCol=\"features\", outputCol=\"pca_features\")\n",
        "    model = pca.fit(final_df)\n",
        "    final_df = model.transform(final_df)\n",
        "\n",
        "# Encontrar el número de componentes necesarios para explicar el 95% de la varianza\n",
        "variances = model.explainedVariance\n",
        "cum_sum = variances.cumsum()\n",
        "n_components = len(cum_sum[cum_sum < 0.95]) + 1\n",
        "\n",
        "print(f\"Se necesitan {n_components} componentes para explicar el 95% de la varianza.\")\n",
        "\n",
        "# Aplicar KMeans a los datos reducidos\n",
        "if \"prediction\" not in final_df.columns:\n",
        "    kmeans = KMeans(featuresCol='pca_features', predictionCol=\"prediction\", k=3)  # Ajusta 'k' según tus necesidades\n",
        "    kmeans_model = kmeans.fit(final_df)\n",
        "    final_df = kmeans_model.transform(final_df)\n",
        "\n",
        "# Convertir la columna 'prediction' a float\n",
        "to_float = udf(lambda x: float(x), FloatType())\n",
        "final_df = final_df.withColumn(\"cluster\", to_float(final_df.prediction))\n",
        "\n",
        "# Seleccionar las primeras dos componentes principales para visualizar\n",
        "pandas_df = final_df.select(\"pca_features\").rdd.map(lambda x: (float(x[0][0]), float(x[0][1]))).toDF([\"PCA1\", \"PCA2\"]).toPandas()\n",
        "pandas_df['cluster'] = final_df.select(\"cluster\").toPandas()\n",
        "\n",
        "plt.scatter(pandas_df['PCA1'], pandas_df['PCA2'], c=pandas_df['cluster'])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L9BgXEg1XlMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las funciones necesarias de PySpark\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "# Definir la UDF para calcular el valor total de un pedido\n",
        "def calcular_total(precio, descuento, margen):\n",
        "    precio_descuento = precio - (precio * descuento / 100)\n",
        "    total = precio_descuento + (precio_descuento * margen / 100)\n",
        "    return total\n",
        "\n",
        "calcular_total_udf = udf(calcular_total, FloatType())\n",
        "\n",
        "# Aplicar la UDF al DataFrame\n",
        "final_df = final_df.withColumn(\"total_pedido\", calcular_total_udf(final_df['precio'], final_df['descuento'], final_df['margen']))\n",
        "\n",
        "# Definir la UDF para encontrar el pedido con el mayor total\n",
        "def maximo_pedido(total):\n",
        "    return max(total)\n",
        "\n",
        "maximo_pedido_udf = udf(maximo_pedido, FloatType())\n",
        "\n",
        "# Aplicar la UDF al DataFrame\n",
        "final_df = final_df.withColumn(\"maximo_pedido\", maximo_pedido_udf(final_df['total_pedido']))\n",
        "\n"
      ],
      "metadata": {
        "id": "3lQByOVReLR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las funciones necesarias de PySpark\n",
        "from pyspark.sql.functions import udf, max\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "# Definir la UDF para calcular el valor total de un pedido\n",
        "def calcular_total(precio, descuento, margen):\n",
        "    precio_descuento = precio - (precio * descuento / 100)\n",
        "    total = precio_descuento + (precio_descuento * margen / 100)\n",
        "    return total\n",
        "\n",
        "calcular_total_udf = udf(calcular_total, FloatType())\n",
        "\n",
        "# Aplicar la UDF al DataFrame\n",
        "final_df = final_df.withColumn(\"total_pedido\", calcular_total_udf(final_df['precio'], final_df['descuento'], final_df['margen']))\n",
        "\n",
        "# Encontrar el pedido con el mayor total usando funciones incorporadas de PySpark\n",
        "maximo_pedido = final_df.orderBy(final_df['total_pedido'].desc()).first()\n",
        "\n",
        "print(\"El pedido con el mayor total es: \", maximo_pedido)\n"
      ],
      "metadata": {
        "id": "ZcYD6Lr1iI05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Calcula el primer y tercer cuartil\n",
        "cuartiles = final_df.approxQuantile('nombre_de_tu_columna', [0.25, 0.75], 0)\n",
        "\n",
        "# Calcula el rango intercuartil\n",
        "IQR = cuartiles[1] - cuartiles[0]\n",
        "\n",
        "# Define los límites para los outliers\n",
        "limite_inferior = cuartiles[0] - 1.5 * IQR\n",
        "limite_superior = cuartiles[1] + 1.5 * IQR\n",
        "\n",
        "# Filtra los outliers\n",
        "outliers = final_df.filter((F.col('nombre_de_tu_columna') < limite_inferior) | (F.col('nombre_de_tu_columna') > limite_superior))\n",
        "\n",
        "# Cuenta los outliers\n",
        "numero_de_outliers = outliers.count()\n",
        "\n",
        "print(\"Número de outliers: \", numero_de_outliers)\n"
      ],
      "metadata": {
        "id": "N5GMEW9KoHGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Calcula el promedio y la desviación estándar\n",
        "stats = final_df.select(\n",
        "    F.mean(F.col('nombre_de_tu_columna')).alias('media'),\n",
        "    F.stddev(F.col('nombre_de_tu_columna')).alias('desviacion_estandar')\n",
        ").collect()[0]\n",
        "\n",
        "# Define los límites para los outliers\n",
        "limite_inferior = stats.media - 3 * stats.desviacion_estandar\n",
        "limite_superior = stats.media + 3 * stats.desviacion_estandar\n",
        "\n",
        "# Filtra los outliers\n",
        "outliers = final_df.filter((F.col('nombre_de_tu_columna') < limite_inferior) | (F.col('nombre_de_tu_columna') > limite_superior))\n",
        "\n",
        "print(\"Outliers:\")\n",
        "outliers.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "TvYyy1Ugo3u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Filtra el DataFrame para incluir solo las filas con ID = 123\n",
        "df_filtrado = final_df.filter(F.col('ID') == 123)\n",
        "\n",
        "# Agrupa por la columna 'fecha' y calcula la media, mediana y desviación estándar\n",
        "resultados = df_filtrado.groupBy('fecha').agg(\n",
        "    F.mean(F.col('precio_unitario')).alias('media'),\n",
        "    F.expr('percentile_approx(precio_unitario, 0.5)').alias('mediana'),\n",
        "    F.stddev(F.col('precio_unitario')).alias('desviacion_estandar')\n",
        ")\n",
        "\n",
        "resultados.show()\n",
        "\n",
        "\n",
        "Si la media y la mediana de los precios de tu producto son iguales, y la desviación estándar es 0, esto indica que todos los precios de tu producto son iguales a 28.35 a lo largo de todos los períodos. En otras palabras, no hay variabilidad en los precios de tu producto; son constantes en el tiempo. Esto podría ser el caso si el precio de tu producto no cambia o si todos los datos disponibles son del mismo período de tiempo en el que el precio fue 28.35\n"
      ],
      "metadata": {
        "id": "ObSK_E8qsYdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Cambia el tipo de datos de la columna 'total_pedido' a entero\n",
        "final_df = final_df.withColumn('total_pedido', final_df['total_pedido'].cast(IntegerType()))\n",
        "\n",
        "# Muestra el DataFrame para verificar el cambio\n",
        "final_df.show()\n"
      ],
      "metadata": {
        "id": "7baqrFviwS_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Configura Spark\n",
        "conf = SparkConf().setAppName(\"load text file\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Carga el archivo de texto\n",
        "text_file = sc.textFile(\"harry.txt\")\n",
        "\n",
        "# Muestra las primeras líneas del archivo\n",
        "for line in text_file.take(10):\n",
        "    print(line)\n"
      ],
      "metadata": {
        "id": "IrZZk7TXSQpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Obtén la ruta del directorio actual\n",
        "dir_path = os.getcwd()\n",
        "\n",
        "# Añade el nombre de tu archivo a la ruta del directorio\n",
        "file_path = os.path.join(dir_path, 'harry.txt')\n",
        "\n",
        "print(file_path)\n"
      ],
      "metadata": {
        "id": "0iPGWsRNV1UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####### vla test boosting\n",
        "\n",
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Establecer la semilla del generador de números aleatorios\n",
        "np.random.seed(42)\n",
        "\n",
        "# Supongamos que 'df' es tu DataFrame y que 'clase' es tu variable objetivo\n",
        "df = pd.read_csv('tu_archivo.csv')\n",
        "\n",
        "# Crear las variables objetivo y características\n",
        "X = df.drop('clase', axis=1)\n",
        "y = df['clase']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento (60%) y temporal (40%)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "# Luego, dividir el conjunto temporal en conjuntos de prueba y validación (ambos del 20% del conjunto original)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# Crear una instancia del clasificador Gradient Boosting\n",
        "modelo = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Definir la grilla de hiperparámetros para la búsqueda en grilla\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda en grilla de hiperparámetros\n",
        "busqueda_en_grilla = GridSearchCV(estimator=modelo,\n",
        "                                  param_grid=param_grid,\n",
        "                                  cv=3,\n",
        "                                  verbose=2,\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros encontrados\n",
        "busqueda_en_grilla.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Definir tus umbrales personalizados aquí. Por ejemplo:\n",
        "umbrales = [0.4, 0.3, 0.3]  # Un umbral para cada clase\n",
        "\n",
        "# Función para calcular el valor KS\n",
        "def calc_ks(data):\n",
        "    data['good']=(data['label']==0).astype(int)\n",
        "    data['bad']=(data['label']==1).astype(int)\n",
        "    data['bucket']=(data['probability'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().probability.to_frame(name='min_prob')\n",
        "    kstable['max_prob']=grouped.max().probability\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "# Función para calcular el coeficiente Gini\n",
        "def calc_gini(y_verdadero, y_prob):\n",
        "    fpr, tpr, _ = roc_curve(y_verdadero, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    gini = 2*roc_auc - 1\n",
        "    return gini\n",
        "\n",
        "# Función para calcular las métricas de rendimiento\n",
        "def calcular_metricas(y_verdadero, y_pred, probas, clases):\n",
        "    print(\"Informe de clasificación:\")\n",
        "    print(classification_report(y_verdadero, y_pred))\n",
        "\n",
        "    cm = confusion_matrix(y_verdadero, y_pred)\n",
        "    cm_df = pd.DataFrame(cm,\n",
        "                         index=[i for i in clases],\n",
        "                         columns=[i for i in clases])\n",
        "    print(\"Matriz de confusión:\")\n",
        "    print(cm_df)\n",
        "\n",
        "    print(\"Precisión del modelo:\")\n",
        "    print(accuracy_score(y_verdadero, y_pred))\n",
        "\n",
        "    lb = LabelBinarizer()\n",
        "    lb.fit(y_verdadero)\n",
        "    y_verdadero_lb = lb.transform(y_verdadero)\n",
        "    y_pred_lb = lb.transform(y_pred)\n",
        "    print(\"Puntuación AUC-ROC para cada clase:\")\n",
        "    for i in range(len(lb.classes_)):\n",
        "        print(f\"{lb.classes_[i]}: {roc_auc_score(y_verdadero_lb[:, i], y_pred_lb[:, i])}\")\n",
        "\n",
        "    print(\"Valor KS y coeficiente Gini para cada clase:\")\n",
        "    for i in range(len(lb.classes_)):\n",
        "        data = pd.DataFrame({'label': y_verdadero_lb[:, i], 'probability': probas[:, i]})\n",
        "        ks = calc_ks(data)\n",
        "        gini = calc_gini(y_verdadero_lb[:, i], probas[:, i])\n",
        "        print(f\"{lb.classes_[i]}: KS = {ks}, Gini = {gini}\")\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de prueba\n",
        "probas_test = busqueda_en_grilla.predict_proba(X_test)\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "y_pred_test = np.zeros_like(probas_test)\n",
        "for i in range(3):\n",
        "    y_pred_test[:, i] = (probas_test[:, i] > thresholds[i]).astype(int)\n",
        "\n",
        "y_pred_test = np.argmax(y_pred_test, axis=1)  # Convertir las predicciones one-hot a etiquetas\n",
        "\n",
        "# Calcular las métricas de rendimiento en el conjunto de prueba\n",
        "print(\"Métricas de rendimiento en el conjunto de prueba:\")\n",
        "calcular_metricas(y_test, y_pred_test, probas_test, le.classes_)\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de validación\n",
        "probas_val = busqueda_en_grilla.predict_proba(X_val)\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "y_pred_val = np.zeros_like(probas_val)\n",
        "for i in range(3):\n",
        "    y_pred_val[:, i] = (probas_val[:, i] > thresholds[i]).astype(int)\n",
        "\n",
        "y_pred_val = np.argmax(y_pred_val, axis=1)  # Convertir las predicciones one-hot a etiquetas\n",
        "\n",
        "# Calcular las métricas de rendimiento en el conjunto de validación\n",
        "print(\"Métricas de rendimiento en el conjunto de validación:\")\n",
        "calcular_metricas(y_val, y_pred_val, probas_val, le.classes_)\n",
        "\n"
      ],
      "metadata": {
        "id": "8te2Rt5ZWeCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### correc\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Asumiendo que 'X' es tu conjunto de características y 'y' es tu variable objetivo\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "# Luego, dividir el conjunto temporal en conjuntos de prueba y validación (ambos del 20% del conjunto original)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# Crear una instancia del clasificador Gradient Boosting\n",
        "modelo = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Definir la grilla de hiperparámetros para la búsqueda en grilla\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda en grilla de hiperparámetros\n",
        "busqueda_en_grilla = GridSearchCV(estimator=modelo,\n",
        "                                  param_grid=param_grid,\n",
        "                                  cv=3,\n",
        "                                  verbose=2,\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros encontrados\n",
        "busqueda_en_grilla.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Definir tus umbrales personalizados aquí. Por ejemplo:\n",
        "umbrales = [0.4, 0.3, 0.3]  # Un umbral para cada clase\n",
        "\n",
        "# Función para calcular el valor KS\n",
        "def calc_ks(data):\n",
        "    data['good']=(data['label']==0).astype(int)\n",
        "    data['bad']=(data['label']==1).astype(int)\n",
        "    data['bucket']=(data['probability'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().probability.to_frame(name='min_prob')\n",
        "    kstable['max_prob']=grouped.max().probability\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "# Función para calcular el coeficiente Gini\n",
        "def calc_gini(y_verdadero, y_prob):\n",
        "    fpr, tpr, _ = roc_curve(y_verdadero, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    gini = 2*roc_auc - 1\n",
        "    return gini\n",
        "\n",
        "# Función para calcular las métricas de rendimiento\n",
        "def calcular_metricas(y_verdadero, y_pred, probas, clases):\n",
        "    print(\"Informe de clasificación:\")\n",
        "    print(classification_report(y_verdadero, y_pred))\n",
        "\n",
        "    cm = confusion_matrix(y_verdadero, y_pred)\n",
        "    cm_df = pd.DataFrame(cm,\n",
        "                         index=[i for i in clases],\n",
        "                         columns=[i for i in clases])\n",
        "    print(\"Matriz de confusión:\")\n",
        "    print(cm_df)\n",
        "\n",
        "    print(\"Precisión del modelo:\")\n",
        "    print(accuracy_score(y_verdadero, y_pred))\n",
        "\n",
        "    lb = LabelBinarizer()\n",
        "    lb.fit(y_verdadero)\n",
        "    y_verdadero_lb = lb.transform(y_verdadero)\n",
        "    y_pred_lb = lb.transform(y_pred)\n",
        "    print(\"Puntuación AUC-ROC para cada clase:\")\n",
        "    for i in range(len(lb.classes_)):\n",
        "        print(f\"{lb.classes_[i]}: {roc_auc_score(y_verdadero_lb[:, i], y_pred_lb[:, i])}\")\n",
        "\n",
        "    print(\"Valor KS y coeficiente Gini para cada clase:\")\n",
        "    for i in range(len(lb.classes_)):\n",
        "        data = pd.DataFrame({'label': y_verdadero_lb[:, i], 'probability': probas[:, i]})\n",
        "        ks = calc_ks(data)\n",
        "        gini = calc_gini(y_verdadero_lb[:, i], probas[:, i])\n",
        "        print(f\"{lb.classes_[i]}: KS = {ks}, Gini = {gini}\")\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de prueba\n",
        "probas_test = busqueda_en_grilla.predict_proba(X_test)\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "y_pred_test = np.zeros_like(probas_test)\n",
        "for i in range(3):\n",
        "    y_pred_test[:, i] = (probas_test[:, i] > umbrales[i]).astype(int)\n",
        "\n",
        "y_pred_test = np.argmax(y_pred_test, axis=1)  # Convertir las predicciones one-hot a etiquetas\n",
        "\n",
        "# Calcular las métricas de rendimiento en el conjunto de prueba\n",
        "print(\"Métricas de rendimiento en el conjunto de prueba:\")\n",
        "calcular_metricas(y_test, y_pred_test, probas_test, le.classes_)\n"
      ],
      "metadata": {
        "id": "D0UJLBzNNFb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener las probabilidades de las predicciones en el conjunto de validación\n",
        "probas_val = busqueda_en_grilla.predict_proba(X_val)\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "y_pred_val = np.zeros_like(probas_val)\n",
        "for i in range(3):\n",
        "    y_pred_val[:, i] = (probas_val[:, i] > umbrales[i]).astype(int)\n",
        "\n",
        "y_pred_val = np.argmax(y_pred_val, axis=1)  # Convertir las predicciones one-hot a etiquetas\n",
        "\n",
        "# Calcular las métricas de rendimiento en el conjunto de validación\n",
        "print(\"Métricas de rendimiento en el conjunto de validación:\")\n",
        "calcular_metricas(y_val, y_pred_val, probas_val, le.classes_)\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de prueba\n",
        "probas_test = busqueda_en_grilla.predict_proba(X_test)\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "y_pred_test = np.zeros_like(probas_test)\n",
        "for i in range(3):\n",
        "    y_pred_test[:, i] = (probas_test[:, i] > umbrales[i]).astype(int)\n",
        "\n",
        "y_pred_test = np.argmax(y_pred_test, axis=1)  # Convertir las predicciones one-hot a etiquetas\n",
        "\n",
        "# Calcular las métricas de rendimiento en el conjunto de prueba\n",
        "print(\"Métricas de rendimiento en el conjunto de prueba:\")\n",
        "calcular_metricas(y_test, y_pred_test, probas_test, le.classes_)\n"
      ],
      "metadata": {
        "id": "yHpQIBQQNYPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Asumiendo que 'X' es tu conjunto de características y 'y' es tu variable objetivo\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "# Luego, dividir el conjunto temporal en conjuntos de prueba y validación (ambos del 20% del conjunto original)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# Crear una instancia del clasificador Random Forest\n",
        "modelo = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Definir la grilla de hiperparámetros para la búsqueda en grilla\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda en grilla de hiperparámetros\n",
        "busqueda_en_grilla = GridSearchCV(estimator=modelo,\n",
        "                                  param_grid=param_grid,\n",
        "                                  cv=3,\n",
        "                                  verbose=2,\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros encontrados\n",
        "busqueda_en_grilla.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Definir tus umbrales personalizados aquí. Por ejemplo:\n",
        "umbrales = [0.4, 0.3, 0.3]  # Un umbral para cada clase\n",
        "\n",
        "# Función para calcular el valor KS\n",
        "def calc_ks(data):\n",
        "    data['good']=(data['label']==0).astype(int)\n",
        "    data['bad']=(data['label']==1).astype(int)\n",
        "    data['bucket']=(data['probability'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().probability.to_frame(name='min_prob')\n",
        "    kstable['max_prob']=grouped.max().probability\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "# Función para calcular el coeficiente Gini\n",
        "def calc_gini(y_verdadero, y_prob):\n",
        "    fpr, tpr, _ = roc_curve(y_verdadero, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    gini = 2*roc_auc - 1\n",
        "    return gini\n",
        "\n",
        "# Función para calcular las métricas de rendimiento\n",
        "def calcular_metricas(y_verdadero, y_pred, probas, clases):\n",
        "    print(\"Informe de clasificación:\")\n",
        "    print(classification_report(y_verdadero, y_pred))\n",
        "\n",
        "    cm = confusion_matrix(y_verdadero, y_pred)\n",
        "    cm_df = pd.DataFrame(cm,\n",
        "                         index=[i for i in clases],\n",
        "                         columns=[i for i in clases])\n",
        "    print(\"Matriz de confusión:\")\n",
        "    print(cm_df)\n",
        "\n",
        "    print(\"Precisión del modelo:\")\n",
        "    print(accuracy_score(y_verdadero, y_pred))\n",
        "\n",
        "    lb = LabelBinarizer()\n",
        "    lb.fit(y_verdadero)\n",
        "    y_verdadero_lb = lb.transform(y_verdadero)\n",
        "    y_pred_lb = lb.transform(y_pred)\n",
        "    print(\"Puntuación AUC-ROC para cada clase:\")\n",
        "    for i in range(len(lb.classes_)):\n",
        "        print(f\"{lb.classes_[i]}: {roc_auc_score(y_verdadero_lb[:, i], y_pred_lb[:, i])}\")\n",
        "\n",
        "    print(\"Valor KS y coeficiente Gini para cada clase:\")\n",
        "    for i in range(len(lb.classes_)):\n",
        "        data = pd.DataFrame({'label': y_verdadero_lb[:, i], 'probability': probas[:, i]})\n",
        "        ks = calc_ks(data)\n",
        "        gini = calc_gini(y_verdadero_lb[:, i], probas[:, i])\n",
        "        print(f\"{lb.classes_[i]}: KS = {ks}, Gini = {gini}\")\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de validación\n",
        "probas_val = busqueda_en_grilla.predict_proba(X_val)\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "y_pred_val = np.zeros_like(probas_val)\n",
        "for i in range(3):\n",
        "    y_pred_val[:, i] = (probas_val[:, i] > umbrales[i]).astype(int)\n",
        "\n",
        "y_pred_val = np.argmax(y_pred_val, axis=1)  # Convertir las predicciones one-hot a etiquetas\n",
        "\n",
        "# Calcular las métricas de rendimiento en el conjunto de validación\n",
        "print(\"Métricas de rendimiento en el conjunto de validación:\")\n",
        "calcular_metricas(y_val, y_pred_val, probas_val, le.classes_)\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de prueba\n",
        "probas_test = busqueda_en_grilla.predict_proba(X_test)\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "y_pred_test = np.zeros_like(probas_test)\n",
        "for i in range(3):\n",
        "    y_pred_test[:, i] = (probas_test[:, i] > umbrales[i]).astype(int)\n",
        "\n",
        "y_pred_test = np.argmax(y_pred_test, axis=1)  # Convertir las predicciones one-hot a etiquetas\n",
        "\n",
        "# Calcular las métricas de rendimiento en el conjunto de prueba\n",
        "print(\"Métricas de rendimiento en el conjunto de prueba:\")\n",
        "calcular_metricas(y_test, y_pred_test, probas_test, le.classes_)\n"
      ],
      "metadata": {
        "id": "oPtNkgX3rDbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Asumiendo que 'X' es tu conjunto de características y 'y' es tu variable objetivo\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "# Luego, dividir el conjunto temporal en conjuntos de prueba y validación (ambos del 20% del conjunto original)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# Crear una instancia del clasificador LightGBM\n",
        "modelo = LGBMClassifier(random_state=42)\n",
        "\n",
        "# Definir la grilla de hiperparámetros para la búsqueda en grilla\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda en grilla de hiperparámetros\n",
        "busqueda_en_grilla = GridSearchCV(estimator=modelo,\n",
        "                                  param_grid=param_grid,\n",
        "                                  cv=3,\n",
        "                                  verbose=2,\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros encontrados\n",
        "busqueda_en_grilla.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Definir tus umbrales personalizados aquí. Por ejemplo:\n",
        "umbrales = [0.4, 0.3, 0.3]  # Un umbral para cada clase\n",
        "\n",
        "# Función para calcular el valor KS\n",
        "def calc_ks(data):\n",
        "    data['good']=(data['label']==0).astype(int)\n",
        "    data['bad']=(data['label']==1).astype(int)\n",
        "    data['bucket']=(data['probability'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().probability.to_frame(name='min_prob')\n",
        "    kstable['max_prob']=grouped.max().probability\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "# Función para calcular el coeficiente Gini\n",
        "def calc_gini(y_verdadero, y_prob):\n",
        "    fpr, tpr, _ = roc_curve(y_verdadero, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    gini = 2*roc_auc - 1\n",
        "    return gini\n",
        "\n",
        "# Función para calcular las métricas de rendimiento\n",
        "def calcular_metricas(y_verdadero, y_pred, probas, clases):\n",
        "    print(\"Informe de clasificación:\")\n",
        "    print(classification_report(y_verdadero, y_pred))\n",
        "\n",
        "    cm = confusion_matrix(y_verdadero, y_pred)\n",
        "    cm_df = pd.DataFrame(cm,\n",
        "                         index=[i for i in clases],\n",
        "                         columns=[i for i in clases])\n",
        "    print(\"Matriz de confusión:\")\n",
        "    print(cm_df)\n",
        "\n",
        "    print(\"Precisión del modelo:\")\n",
        "    print(accuracy_score(y_verdadero, y_pred))\n",
        "\n",
        "    lb = LabelBinarizer()\n",
        "    lb.fit(y_verdadero)\n",
        "    y_verdadero_lb = lb.transform(y_verdadero)\n",
        "    y_pred_lb = lb.transform(y_pred)\n",
        "    print(\"Puntuación AUC-ROC para cada clase:\")\n",
        "    for i in range(len(lb.classes_)):\n",
        "        print(f\"{lb.classes_[i]}: {roc_auc_score(y_verdadero_lb[:, i], y_pred_lb[:, i])}\")\n",
        "\n",
        "    print(\"Valor KS y coeficiente Gini para cada clase:\")\n",
        "    for i in range(len(lb.classes_)):\n",
        "        data = pd.DataFrame({'label': y_verdadero_lb[:, i], 'probability': probas[:, i]})\n",
        "        ks = calc_ks(data)\n",
        "        gini = calc_gini(y_verdadero_lb[:, i], probas[:, i])\n",
        "        print(f\"{lb.classes_[i]}: KS = {ks}, Gini = {gini}\")\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de validación\n",
        "probas_val = busqueda_en_grilla.predict_proba(X_val)\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "y_pred_val = np.zeros_like(probas_val)\n",
        "for i in range(3):\n",
        "    y_pred_val[:, i] = (probas_val[:, i] > umbrales[i]).astype(int)\n",
        "\n",
        "y_pred_val = np.argmax(y_pred_val, axis=1)  # Convertir las predicciones one-hot a etiquetas\n",
        "\n",
        "# Calcular las métricas de rendimiento en el conjunto de validación\n",
        "print(\"Métricas de rendimiento en el conjunto de validación:\")\n",
        "calcular_metricas(y_val, y_pred_val, probas_val, le.classes_)\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de prueba\n",
        "probas_test = busqueda_en_grilla.predict_proba(X_test)\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "y_pred_test = np.zeros_like(probas_test)\n",
        "for i in range(3):\n",
        "    y_pred_test[:, i] = (probas_test[:, i] > umbrales[i]).astype(int)\n",
        "\n",
        "y_pred_test = np.argmax(y_pred_test, axis=1)  # Convertir las predicciones one-hot a etiquetas\n",
        "\n",
        "# Calcular las métricas de rendimiento en el conjunto de prueba\n",
        "print(\"Métricas de rendimiento en el conjunto de prueba:\")\n",
        "calcular_metricas(y_test, y_pred_test, probas_test, le.classes_)\n"
      ],
      "metadata": {
        "id": "8ROtE8i20Ln4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Asumiendo que 'X' es tu conjunto de características y 'y' es tu variable objetivo\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "# Luego, dividir el conjunto temporal en conjuntos de prueba y validación (ambos del 20% del conjunto original)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# Crear una instancia del clasificador Naive Bayes Multinomial\n",
        "modelo = MultinomialNB()\n",
        "\n",
        "# Entrenar el modelo\n",
        "modelo.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Definir tus umbrales personalizados aquí. Por ejemplo:\n",
        "umbrales = [0.4, 0.3, 0.3]  # Un umbral para cada clase\n",
        "\n",
        "# Función para calcular el valor KS\n",
        "def calc_ks(data):\n",
        "    data['good']=(data['label']==0).astype(int)\n",
        "    data['bad']=(data['label']==1).astype(int)\n",
        "    data['bucket']=(data['probability'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().probability.to_frame(name='min_prob')\n",
        "    kstable['max_prob']=grouped.max().probability\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "# Función para calcular el coeficiente Gini\n",
        "def calc_gini(y_verdadero, y_prob):\n",
        "    fpr, tpr, _ = roc_curve(y_verdadero, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    gini = 2*roc_auc - 1\n",
        "    return gini\n",
        "\n",
        "# Función para calcular las métricas de rendimiento\n",
        "def calcular_metricas(y_verdadero, y_pred, probas, clases):\n",
        "    print(\"Informe de clasificación:\")\n",
        "    print(classification_report(y_verdadero, y_pred))\n",
        "\n",
        "    cm = confusion_matrix(y_verdadero, y_pred)\n",
        "    cm_df = pd.DataFrame(cm,\n",
        "                         index=[i for i in clases],\n",
        "                         columns=[i for i in clases])\n",
        "    print(\"Matriz de confusión:\")\n",
        "    print(cm_df)\n",
        "\n",
        "    print(\"Precisión del modelo:\")\n",
        "    print(accuracy_score(y_verdadero, y_pred))\n",
        "\n",
        "    lb = LabelBinarizer()\n",
        "    lb.fit(y_verdadero)\n",
        "    y_verdadero_lb = lb.transform(y_verdadero)\n",
        "    y_pred_lb = lb.transform(y_pred)\n",
        "    print(\"Puntuación AUC-ROC para cada clase:\")\n",
        "    for i in range(len(lb.classes_)):\n",
        "        print(f\"{lb.classes_[i]}: {roc_auc_score(y_verdadero_lb[:, i], y_pred_lb[:, i])}\")\n",
        "\n",
        "    print(\"Valor KS y coeficiente Gini para cada clase:\")\n",
        "    for i in range(len(lb.classes_)):\n",
        "        data = pd.DataFrame({'label': y_verdadero_lb[:, i], 'probability': probas[:, i]})\n",
        "        ks = calc_ks(data)\n",
        "        gini = calc_gini(y_verdadero_lb[:, i], probas[:, i])\n",
        "        print(f\"{lb.classes_[i]}: KS = {ks}, Gini = {gini}\")\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de validación\n",
        "probas_val = modelo.predict_proba(X_val)\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "y_pred_val = np.zeros_like(probas_val)\n",
        "for i in range(3):\n",
        "    y_pred_val[:, i] = (probas_val[:, i] > umbrales[i]).astype(int)\n",
        "\n",
        "y_pred_val = np.argmax(y_pred_val, axis=1)  # Convertir las predicciones one-hot a etiquetas\n",
        "\n",
        "# Calcular las métricas de rendimiento en el conjunto de validación\n",
        "print(\"Métricas de rendimiento en el conjunto de validación:\")\n",
        "calcular_metricas(y_val, y_pred_val, probas_val, le.classes_)\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de prueba\n",
        "probas_test = modelo.predict_proba(X_test)\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "y_pred_test = np.zeros_like(probas_test)\n",
        "for i in range(3):\n",
        "    y_pred_test[:, i] = (probas_test[:, i] > umbrales[i]).astype(int)\n",
        "\n",
        "y_pred_test = np.argmax(y_pred_test, axis=1)  # Convertir las predicciones one-hot a etiquetas\n",
        "\n",
        "# Calcular las métricas de rendimiento en el conjunto de prueba\n",
        "print(\"Métricas de rendimiento en el conjunto de prueba:\")\n",
        "calcular_metricas(y_test, y_pred_test, probas_test, le.classes_)\n"
      ],
      "metadata": {
        "id": "QETcoXZp_4S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, classification_report\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from scipy import interp\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Supongamos que X es tu matriz de características e y son tus etiquetas\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento, validación y prueba\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
        "\n",
        "# Normalizar los datos\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Crear el clasificador MLP\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=1000)\n",
        "\n",
        "# Entrenar el modelo\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# Hacer predicciones en el conjunto de validación y prueba\n",
        "y_pred_val = mlp.predict(X_val)\n",
        "y_pred_test = mlp.predict(X_test)\n",
        "\n",
        "# Calcular la precisión del modelo\n",
        "print(f'Informe de clasificación para el conjunto de validación:\\n{classification_report(y_val, y_pred_val)}')\n",
        "print(f'Informe de clasificación para el conjunto de prueba:\\n{classification_report(y_test, y_pred_test)}')\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "print(f'Matriz de confusión para el conjunto de validación:\\n{confusion_matrix(y_val, y_pred_val)}')\n",
        "print(f'Matriz de confusión para el conjunto de prueba:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
        "\n",
        "# Calcular el valor de AUC-ROC para cada clase\n",
        "lb = LabelBinarizer()\n",
        "lb.fit(y)\n",
        "y_test_lb = lb.transform(y_test)\n",
        "y_pred_test_lb = lb.transform(y_pred_test)\n",
        "roc_auc = roc_auc_score(y_test_lb, y_pred_test_lb, average=None)\n",
        "print(f'Puntuación AUC-ROC para cada clase en el conjunto de prueba: {roc_auc}')\n",
        "\n",
        "# Calcular el valor de Gini para cada clase\n",
        "gini = 2*roc_auc - 1\n",
        "print(f'Coeficiente de Gini para cada clase en el conjunto de prueba: {gini}')\n",
        "\n",
        "# Calcular el valor de KS para cada clase\n",
        "for i in range(len(lb.classes_)):\n",
        "    data = pd.DataFrame({'label': y_test_lb[:, i], 'probability': y_pred_test_lb[:, i]})\n",
        "    data['good'] = (data['label'] == 0).astype(int)\n",
        "    data['bad'] = (data['label'] == 1).astype(int)\n",
        "    data['bucket'] = (data['probability'].rank(pct=True)*10).astype(int)\n",
        "    grouped = data.groupby('bucket', as_index=True)\n",
        "    kstable = grouped.min().probability.to_frame(name='min_prob')\n",
        "    kstable['max_prob'] = grouped.max().probability\n",
        "    kstable['bads'] = grouped.sum().bad\n",
        "    kstable['goods'] = grouped.sum().good\n",
        "    kstable = kstable.reset_index()\n",
        "    kstable['bad_rate'] = kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks'] = (kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value = kstable.ks.abs().max()\n",
        "    print(f'Valor KS para la clase {lb.classes_[i]} en el conjunto de prueba: {ks_value}')\n"
      ],
      "metadata": {
        "id": "k4wNxeEmCqN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, classification_report\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Supongamos que X es tu matriz de características e y son tus etiquetas\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento, validación y prueba\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
        "\n",
        "# Normalizar los datos\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Crear el selector de características\n",
        "selector = SelectKBest(f_classif, k=10)  # Ajusta 'k' al número de características que deseas mantener\n",
        "\n",
        "# Ajustar y transformar los datos de entrenamiento\n",
        "X_train_new = selector.fit_transform(X_train, y_train)\n",
        "\n",
        "# Transformar los datos de validación y prueba\n",
        "X_val_new = selector.transform(X_val)\n",
        "X_test_new = selector.transform(X_test)\n",
        "\n",
        "# Crear el clasificador MLP con regularización L2\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=1000, alpha=0.01)\n",
        "\n",
        "# Entrenar el modelo con los nuevos datos\n",
        "mlp.fit(X_train_new, y_train)\n",
        "\n",
        "# Hacer predicciones en el conjunto de validación y prueba\n",
        "y_pred_val = mlp.predict(X_val_new)\n",
        "y_pred_test = mlp.predict(X_test_new)\n",
        "\n",
        "# Calcular la precisión del modelo\n",
        "print(f'Informe de clasificación para el conjunto de validación:\\n{classification_report(y_val, y_pred_val)}')\n",
        "print(f'Informe de clasificación para el conjunto de prueba:\\n{classification_report(y_test, y_pred_test)}')\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "print(f'Matriz de confusión para el conjunto de validación:\\n{confusion_matrix(y_val, y_pred_val)}')\n",
        "print(f'Matriz de confusión para el conjunto de prueba:\\n{confusion_matrix(y_test, y_pred_test)}')\n",
        "\n",
        "# Calcular el valor de AUC-ROC para cada clase\n",
        "lb = LabelBinarizer()\n",
        "lb.fit(y)\n",
        "y_test_lb = lb.transform(y_test)\n",
        "y_pred_test_lb = lb.transform(y_pred_test)\n",
        "roc_auc = roc_auc_score(y_test_lb, y_pred_test_lb, average=None)\n",
        "print(f'Puntuación AUC-ROC para cada clase en el conjunto de prueba: {roc_auc}')\n",
        "\n",
        "# Calcular el valor de Gini para cada clase\n",
        "gini = 2*roc_auc - 1\n",
        "print(f'Coeficiente de Gini para cada clase en el conjunto de prueba: {gini}')\n",
        "\n",
        "# Calcular el valor de KS para cada clase\n",
        "for i in range(len(lb.classes_)):\n",
        "    data = pd.DataFrame({'label': y_test_lb[:, i], 'probability': y_pred_test_lb[:, i]})\n",
        "    data['good'] = (data['label'] == 0).astype(int)\n",
        "    data['bad'] = (data['label'] == 1).astype(int)\n",
        "    data['bucket'] = (data['probability'].rank(pct=True)*10).astype(int)\n",
        "    grouped = data.groupby('bucket', as_index=True)\n",
        "    kstable = grouped.min().probability.to_frame(name='min_prob')\n",
        "    kstable['max_prob'] = grouped.max().probability\n",
        "    kstable['bads'] = grouped.sum().bad\n",
        "    kstable['goods'] = grouped.sum().good\n",
        "    kstable = kstable.reset_index()\n",
        "    kstable['bad_rate'] = kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks'] = (kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value = kstable.ks.abs().max()\n",
        "    print(f'Valor KS para la clase {lb.classes_[i]} en el conjunto de prueba: {ks_value}')\n"
      ],
      "metadata": {
        "id": "r3CPNZ7wuVqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, auc\n",
        "from scipy.stats import ks_2samp\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Carga tus datos\n",
        "# df = pd.read_csv('ruta/a/tu/dataset.csv')\n",
        "\n",
        "# Supongamos que 'X' son tus características y 'y' es tu variable objetivo\n",
        "# X = df.drop('columna_objetivo', axis=1)\n",
        "# y = df['columna_objetivo']\n",
        "\n",
        "# Estandariza los datos de entrada\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Codifica las etiquetas de clase como vectores one-hot\n",
        "label_binarizer = LabelBinarizer()\n",
        "y = label_binarizer.fit_transform(y)\n",
        "\n",
        "# Divide los datos en conjuntos de entrenamiento, prueba y validación\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
        "\n",
        "# Define la arquitectura de la DNN\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))  # 3 clases\n",
        "\n",
        "# Compila el modelo\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Entrena el modelo\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
        "\n",
        "# Predice las probabilidades de las clases para los conjuntos de prueba y validación\n",
        "y_test_probs = model.predict(X_test)\n",
        "y_val_probs = model.predict(X_val)\n",
        "\n",
        "# Función para calcular el coeficiente Gini\n",
        "def calc_gini(y_verdadero, y_prob):\n",
        "    fpr, tpr, _ = roc_curve(y_verdadero, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    gini = 2*roc_auc - 1\n",
        "    return gini\n",
        "\n",
        "# Función para calcular el valor KS\n",
        "def calc_ks(data):\n",
        "    data['bucket'] = (data['probability'].rank(pct=True)*10).astype(int)\n",
        "    grouped = data.groupby('bucket', as_index=True)\n",
        "    kstable = grouped.min().probability.to_frame(name='min_prob')\n",
        "    kstable['max_prob'] = grouped.max().probability\n",
        "    kstable['bads'] = grouped.sum().bad\n",
        "    kstable['goods'] = grouped.sum().good\n",
        "    kstable = kstable.reset_index()\n",
        "    kstable['bad_rate'] = kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks'] = (kstable.bads/kstable.bads.sum()).cumsum() - (kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value = kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "# Calcular e imprimir el valor KS, el coeficiente Gini y la métrica ROC AUC para cada clase\n",
        "for i in range(y.shape[1]):\n",
        "    print(f\"\\nClase {i+1}:\")\n",
        "\n",
        "    data_test = pd.DataFrame({'probability': y_test_probs[:, i], 'label': y_test[:, i]})\n",
        "    data_test['good'] = (data_test['label'] == 0).astype(int)\n",
        "    data_test['bad'] = (data_test['label'] == 1).astype(int)\n",
        "\n",
        "    data_val = pd.DataFrame({'probability': y_val_probs[:, i], 'label': y_val[:, i]})\n",
        "    data_val['good'] = (data_val['label'] == 0).astype(int)\n",
        "    data_val['bad'] = (data_val['label'] == 1).astype(int)\n",
        "\n",
        "    ks_test = calc_ks(data_test)\n",
        "    ks_val = calc_ks(data_val)\n",
        "\n",
        "    gini_test = calc_gini(y_test[:, i], y_test_probs[:, i])\n",
        "    gini_val = calc_gini(y_val[:, i], y_val_probs[:, i])\n",
        "\n",
        "    roc_auc_test_ovr = roc_auc_score(y_test[:, i], y_test_probs[:, i])\n",
        "    roc_auc_val_ovr = roc_auc_score(y_val[:, i], y_val_probs[:, i])\n",
        "\n",
        "    print(f'KS Test: {ks_test}')\n",
        "    print(f'KS Validation: {ks_val}')\n",
        "    print(f'Gini Test: {gini_test}')\n",
        "    print(f'Gini Validation: {gini_val}')\n",
        "    print(f'ROC AUC Test (OvR): {roc_auc_test_ovr}')\n",
        "    print(f'ROC AUC Validation (OvR): {roc_auc_val_ovr}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5OOa6nDS0Rjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Codifica las etiquetas de clase como números enteros\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y)\n",
        "encoded_Y = encoder.transform(y)\n",
        "\n",
        "# Convierte las etiquetas de clase enteras a one-hot\n",
        "y = np_utils.to_categorical(encoded_Y)\n",
        "\n",
        "# Define los pesos de las clases manualmente\n",
        "class_weights = {0: 1.0,  # Peso para la clase 0\n",
        "                 1: 1.0,  # Peso para la clase 1\n",
        "                 2: 1.0}  # Peso para la clase 2\n",
        "\n",
        "# Define el umbral de decisión\n",
        "threshold = 0.5\n",
        "\n",
        "# Define la arquitectura de la DNN\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))  # 3 clases\n",
        "\n",
        "# Compila el modelo\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Entrena el modelo con los pesos de las clases\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, class_weight=class_weights)\n",
        "\n",
        "# Predice las probabilidades de las clases para los conjuntos de prueba y validación\n",
        "y_test_probs = model.predict(X_test)\n",
        "y_val_probs = model.predict(X_val)\n",
        "\n",
        "# Convierte las probabilidades a predicciones de clase usando el umbral de decisión\n",
        "y_test_preds = (y_test_probs >= threshold).astype(int)\n",
        "y_val_preds = (y_val_probs >= threshold).astype(int)\n",
        "\n",
        "# Aquí puedes continuar con el cálculo de tus métricas...\n",
        "\n"
      ],
      "metadata": {
        "id": "zpD7MJjHACBE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
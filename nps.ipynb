{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpEWikePZZONw+zq1NgsuT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristValen/Acciones-RNR/blob/main/nps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAtGfiwYZC3g"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Establecer la semilla del generador de números aleatorios\n",
        "np.random.seed(42)\n",
        "\n",
        "# Supongamos que 'df' es tu DataFrame y que 'clase' es tu variable objetivo\n",
        "df = pd.read_csv('tu_archivo.csv')\n",
        "\n",
        "# Crear las variables objetivo y características\n",
        "X = df.drop('clase', axis=1)\n",
        "y = df['clase']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Crear el modelo de Random Forest\n",
        "modelo = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Definir la grilla de hiperparámetros para la búsqueda aleatoria\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200, 300, 400],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth': [10, 20, 30, 40, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda aleatoria de hiperparámetros\n",
        "busqueda_aleatoria = RandomizedSearchCV(estimator=modelo,\n",
        "                                        param_distributions=param_dist,\n",
        "                                        n_iter=100,\n",
        "                                        cv=3,\n",
        "                                        verbose=2,\n",
        "                                        random_state=42,\n",
        "                                        n_jobs=-1)\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros encontrados\n",
        "busqueda_aleatoria.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de prueba\n",
        "probas = busqueda_aleatoria.predict_proba(X_test)\n",
        "\n",
        "# Definir tus umbrales personalizados aquí. Por ejemplo:\n",
        "umbrales = [0.4, 0.3, 0.3]  # Un umbral para cada clase\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "predicciones = np.array([busqueda_aleatoria.classes_[i] for i in np.argmax(probas >= umbrales, axis=1)])\n",
        "\n",
        "# Imprimir un informe de clasificación\n",
        "print(\"Informe de clasificación:\")\n",
        "print(classification_report(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la matriz de confusión con los nombres de las clases\n",
        "cm = confusion_matrix(y_test, predicciones)\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index=[i for i in busqueda_aleatoria.classes_],\n",
        "                     columns=[i for i in busqueda_aleatoria.classes_])\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_df)\n",
        "\n",
        "# Imprimir la precisión del modelo\n",
        "print(\"Precisión del modelo:\")\n",
        "print(accuracy_score(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la puntuación AUC-ROC para cada clase\n",
        "lb = LabelBinarizer()\n",
        "lb.fit(y_test)\n",
        "y_test_lb = lb.transform(y_test)\n",
        "predicciones_lb = lb.transform(predicciones)\n",
        "print(\"Puntuación AUC-ROC para cada clase:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    print(f\"{lb.classes_[i]}: {roc_auc_score(y_test_lb[:, i], predicciones_lb[:, i])}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Establecer la semilla del generador de números aleatorios\n",
        "np.random.seed(42)\n",
        "\n",
        "# Supongamos que 'df' es tu DataFrame y que 'clase' es tu variable objetivo\n",
        "df = pd.read_csv('tu_archivo.csv')\n",
        "\n",
        "# Crear las variables objetivo y características\n",
        "X = df.drop('clase', axis=1)\n",
        "y = df['clase']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Crear el modelo SVM\n",
        "modelo = SVC(probability=True, random_state=42)\n",
        "\n",
        "# Definir la grilla de hiperparámetros para la búsqueda aleatoria\n",
        "param_dist = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda aleatoria de hiperparámetros\n",
        "busqueda_aleatoria = RandomizedSearchCV(estimator=modelo,\n",
        "                                        param_distributions=param_dist,\n",
        "                                        n_iter=100,\n",
        "                                        cv=3,\n",
        "                                        verbose=2,\n",
        "                                        random_state=42,\n",
        "                                        n_jobs=-1)\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros encontrados\n",
        "busqueda_aleatoria.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Obtener las probabilidades de las predicciones en el conjunto de prueba\n",
        "probas = busqueda_aleatoria.predict_proba(X_test)\n",
        "\n",
        "# Definir tus umbrales personalizados aquí. Por ejemplo:\n",
        "umbrales = [0.4, 0.3, 0.3]  # Un umbral para cada clase\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "predicciones = np.array([busqueda_aleatoria.classes_[i] for i in np.argmax(probas >= umbrales, axis=1)])\n",
        "\n",
        "# Imprimir un informe de clasificación\n",
        "print(\"Informe de clasificación:\")\n",
        "print(classification_report(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la matriz de confusión con los nombres de las clases\n",
        "cm = confusion_matrix(y_test, predicciones)\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index=[i for i in busqueda_aleatoria.classes_],\n",
        "                     columns=[i for i in busqueda_aleatoria.classes_])\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_df)\n",
        "\n",
        "# Imprimir la precisión del modelo\n",
        "print(\"Precisión del modelo:\")\n",
        "print(accuracy_score(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la puntuación AUC-ROC para cada clase\n",
        "lb = LabelBinarizer()\n",
        "lb.fit(y_test)\n",
        "y_test_lb = lb.transform(y_test)\n",
        "predicciones_lb = lb.transform(predicciones)\n",
        "print(\"Puntuación AUC-ROC para cada clase:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    print(f\"{lb.classes_[i]}: {roc_auc_score(y_test_lb[:, i], predicciones_lb[:, i])}\")\n"
      ],
      "metadata": {
        "id": "r15QARqDXV94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "# Asumiendo que tu DataFrame se llama df y tus columnas son 'ope' y 'conta'\n",
        "df = df.withColumn(\"ope\", to_date(df['ope'], \"yyyy-MM-dd\"))\n",
        "df = df.withColumn(\"conta\", to_date(df['conta'], \"yyyy-MM-dd\"))\n",
        "\n",
        "# Agrupar por la columna 'ope'\n",
        "df_grouped_by_ope = df.groupBy(\"ope\").count()\n",
        "\n",
        "# Agrupar por la columna 'conta'\n",
        "df_grouped_by_conta = df.groupBy(\"conta\").count()"
      ],
      "metadata": {
        "id": "uW9K8n1zeQzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Establecer la semilla del generador de números aleatorios\n",
        "np.random.seed(42)\n",
        "\n",
        "# Supongamos que 'df' es tu DataFrame y que 'clase' es tu variable objetivo\n",
        "df = pd.read_csv('tu_archivo.csv')\n",
        "\n",
        "# Crear las variables objetivo y características\n",
        "X = df.drop('clase', axis=1)\n",
        "y = df['clase']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Crear una instancia del clasificador Gradient Boosting\n",
        "modelo = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Definir la grilla de hiperparámetros para la búsqueda en grilla\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda en grilla de hiperparámetros\n",
        "busqueda_en_grilla = GridSearchCV(estimator=modelo,\n",
        "                                  param_grid=param_grid,\n",
        "                                  cv=3,\n",
        "                                  verbose=2,\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros encontrados\n",
        "busqueda_en_grilla.fit(X_train, y_train)\n",
        "\n",
        "# Predecir las clases para el conjunto de prueba\n",
        "y_pred = busqueda_en_grilla.predict(X_test)\n",
        "\n",
        "# Definir tus umbrales personalizados aquí. Por ejemplo:\n",
        "umbrales = [0.4, 0.3, 0.3]  # Un umbral para cada clase\n",
        "\n",
        "# Aplicar los umbrales a las probabilidades para obtener las predicciones finales\n",
        "probas = busqueda_en_grilla.predict_proba(X_test)\n",
        "predicciones = np.array([busqueda_en_grilla.classes_[i] for i in np.argmax(probas >= umbrales, axis=1)])\n",
        "\n",
        "# Imprimir un informe de clasificación\n",
        "print(\"Informe de clasificación:\")\n",
        "print(classification_report(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la matriz de confusión con los nombres de las clases\n",
        "cm = confusion_matrix(y_test, predicciones)\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index=[i for i in busqueda_en_grilla.classes_],\n",
        "                     columns=[i for i in busqueda_en_grilla.classes_])\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_df)\n",
        "\n",
        "# Imprimir la precisión del modelo\n",
        "print(\"Precisión del modelo:\")\n",
        "print(accuracy_score(y_test, predicciones))\n",
        "\n",
        "# Calcular e imprimir la puntuación AUC-ROC para cada clase\n",
        "lb = LabelBinarizer()\n",
        "lb.fit(y_test)\n",
        "y_test_lb = lb.transform(y_test)\n",
        "predicciones_lb = lb.transform(predicciones)\n",
        "print(\"Puntuación AUC-ROC para cada clase:\")\n",
        "for i in range(len(lb.classes_)):\n",
        "    print(f\"{lb.classes_[i]}: {roc_auc_score(y_test_lb[:, i], predicciones_lb[:, i])}\")\n"
      ],
      "metadata": {
        "id": "Sq2TVcRL0Dej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Establecer la semilla para la reproducibilidad\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# Supongamos que X es tu conjunto de datos e y son las etiquetas correspondientes\n",
        "\n",
        "# División de los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Codificación one-hot de las etiquetas\n",
        "y_train_one_hot = to_categorical(y_train)\n",
        "y_test_one_hot = to_categorical(y_test)\n",
        "\n",
        "# Creación del modelo RNN\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(50, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compilación del modelo\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Asignación de pesos a las clases (supongamos que class_weights es un diccionario que contiene los pesos)\n",
        "class_weights = {0: 1., 1: 1., 2: 1.}  # Modifica esto según tus necesidades\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "model.fit(X_train, y_train_one_hot, epochs=100, verbose=0, class_weight=class_weights)\n",
        "\n",
        "# Predicción en el conjunto de prueba\n",
        "y_pred_proba = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_proba, axis=1)  # Convertir las predicciones one-hot a etiquetas\n",
        "\n",
        "# Modificación del umbral de probabilidad\n",
        "threshold = 0.5  # Modifica esto según tus necesidades\n",
        "y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "# Cálculo de las métricas\n",
        "precision = precision_score(y_test, y_pred_threshold, average=None)\n",
        "recall = recall_score(y_test, y_pred_threshold, average=None)\n",
        "f1 = f1_score(y_test, y_pred_threshold, average=None)\n",
        "confusion = confusion_matrix(y_test, y_pred_threshold)\n",
        "roc_auc = roc_auc_score(y_test_one_hot, to_categorical(y_pred_threshold), multi_class='ovr')\n",
        "\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1 Score: \", f1)\n",
        "print(\"Confusion Matrix: \\n\", confusion)\n",
        "print(\"ROC AUC Score: \", roc_auc)\n"
      ],
      "metadata": {
        "id": "l0E6y2lWoCAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zqAlf41JoIka"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJi5+sEj4NhF6SOaBypzVv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristValen/Acciones-RNR/blob/main/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnDIFep0ZSFo"
      },
      "outputs": [],
      "source": [
        "#correccion\n",
        "\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Convert the training data to a Pandas DataFrame\n",
        "train_pd = train.toPandas()\n",
        "\n",
        "# Define features and label\n",
        "features = df8.columns\n",
        "features.remove('label')\n",
        "\n",
        "# Separate the features and label\n",
        "X = train_pd[features]\n",
        "y = train_pd['label']\n",
        "\n",
        "# Perform NearMiss undersampling\n",
        "nm = NearMiss()\n",
        "X_resampled, y_resampled = nm.fit_resample(X, y)\n",
        "\n",
        "# Convert the resampled data back to a PySpark DataFrame\n",
        "train_undersampled_pd = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "train_undersampled = spark.createDataFrame(train_undersampled_pd)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Create the Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=seed)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Define the parameter grid for cross-validation\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
        "\n",
        "# Define the evaluator for cross-validation\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create the cross-validator object\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
        "\n",
        "# Fit the model on the undersampled training data\n",
        "model = cv.fit(train_undersampled)\n",
        "\n",
        "# Convert the test data to a Pandas DataFrame\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Separate the features and label in the test data\n",
        "X_test = test_pd[features]\n",
        "y_test = test_pd['label']\n",
        "\n",
        "# Perform NearMiss undersampling on the test data\n",
        "X_test_resampled, y_test_resampled = nm.fit_resample(X_test, y_test)\n",
        "\n",
        "# Convert the resampled test data back to a PySpark DataFrame\n",
        "test_undersampled_pd = pd.concat([X_test_resampled, y_test_resampled], axis=1)\n",
        "test_undersampled = spark.createDataFrame(test_undersampled_pd)\n",
        "\n",
        "# Make predictions on the undersampled test data\n",
        "predictions = model.transform(test_undersampled)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc = evaluator.evaluate(predictions)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "metrics = MulticlassMetrics(predictionAndLabels)\n",
        "confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "print(f\"Confusion matrix:\\n{confusion_matrix}\")\n",
        "\n",
        "# Manually calculate recall and F1 score\n",
        "TP = confusion_matrix[1, 1]\n",
        "FP = confusion_matrix[0, 1]\n",
        "FN = confusion_matrix[1, 0]\n",
        "precision_manual = TP / (TP + FP)\n",
        "recall_manual = TP / (TP + FN)\n",
        "f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
        "print(f\"Recall (manually calculated): {recall_manual:.3f}\")\n",
        "print(f\"F1 (manually calculated): {f1_manual:.3f}\")\n",
        "\n",
        "# Get the most important features\n",
        "importances = model.bestModel.stages[-1].featureImportances\n",
        "important_features = sorted(zip(importances, features), reverse=True)\n",
        "print(\"Most important features:\")\n",
        "for importance, feature in important_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n",
        "\n",
        "def calc_ks(data):\n",
        "    data_pd=data.toPandas()\n",
        "    data_pd['good']=(data_pd['label']==0).astype(int)\n",
        "    data_pd['bad']=(data_pd['label']==1).astype(int)\n",
        "    data_pd['bucket']=(data_pd['score'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data_pd.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().score.to_frame(name='min_score')\n",
        "    kstable['max_score']=grouped.max().score\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "score_udf=udf(lambda v:float(v[0]),DoubleType())\n",
        "predictions=predictions.withColumn('score',score_udf('probability'))\n",
        "ks_value=calc_ks(predictions)\n",
        "print(f\"KS statistic: {ks_value:.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import TomekLinks\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Convert the training data to a Pandas DataFrame\n",
        "train_pd = train.toPandas()\n",
        "\n",
        "# Define features and label\n",
        "features = df8.columns\n",
        "features.remove('label')\n",
        "\n",
        "# Separate the features and label\n",
        "X = train_pd[features]\n",
        "y = train_pd['label']\n",
        "\n",
        "# Perform Tomek Links undersampling\n",
        "tl = TomekLinks()\n",
        "X_resampled, y_resampled = tl.fit_resample(X, y)\n",
        "\n",
        "# Convert the resampled data back to a PySpark DataFrame\n",
        "train_undersampled_pd = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "train_undersampled = spark.createDataFrame(train_undersampled_pd)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Create the Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=seed)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Define the parameter grid for cross-validation\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
        "\n",
        "# Define the evaluator for cross-validation\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create the cross-validator object\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
        "\n",
        "# Fit the model on the undersampled training data\n",
        "model = cv.fit(train_undersampled)\n",
        "\n",
        "# Convert the test data to a Pandas DataFrame\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Separate the features and label in the test data\n",
        "X_test = test_pd[features]\n",
        "y_test = test_pd['label']\n",
        "\n",
        "# Perform Tomek Links undersampling on the test data\n",
        "X_test_resampled, y_test_resampled = tl.fit_resample(X_test, y_test)\n",
        "\n",
        "# Convert the resampled test data back to a PySpark DataFrame\n",
        "test_undersampled_pd = pd.concat([X_test_resampled, y_test_resampled], axis=1)\n",
        "test_undersampled = spark.createDataFrame(test_undersampled_pd)\n",
        "\n",
        "# Make predictions on the undersampled test data\n",
        "predictions = model.transform(test_undersampled)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc = evaluator.evaluate(predictions)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "metrics = MulticlassMetrics(predictionAndLabels)\n",
        "confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "print(f\"Confusion matrix:\\n{confusion_matrix}\")\n",
        "\n",
        "# Manually calculate recall and F1 score\n",
        "TP = confusion_matrix[1, 1]\n",
        "FP = confusion_matrix[0, 1]\n",
        "FN = confusion_matrix[1, 0]\n",
        "precision_manual = TP / (TP + FP)\n",
        "recall_manual = TP / (TP + FN)\n",
        "f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
        "print(f\"Recall (manually calculated): {recall_manual:.3f}\")\n",
        "print(f\"F1 (manually calculated): {f1_manual:.3f}\")\n",
        "\n",
        "# Get the most important features\n",
        "importances = model.bestModel.stages[-1].featureImportances\n",
        "important_features = sorted(zip(importances, features), reverse=True)\n",
        "print(\"Most important features:\")\n",
        "for importance, feature in important_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n",
        "\n",
        "def calc_ks(data):\n",
        "    data_pd=data.toPandas()\n",
        "    data_pd['good']=(data_pd['label']==0).astype(int)\n",
        "    data_pd['bad']=(data_pd['label']==1).astype(int)\n",
        "    data_pd['bucket']=(data_pd['score'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data_pd.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().score.to_frame(name='min_score')\n",
        "    kstable['max_score']=grouped.max().score\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "score_udf=udf(lambda v:float(v[0]),DoubleType())\n",
        "predictions=predictions.withColumn('score',score_udf('probability'))\n",
        "ks_value=calc_ks(predictions)\n",
        "print(f\"KS statistic: {ks_value:.3f}\")\n"
      ],
      "metadata": {
        "id": "AlOyzdcXekOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Convert the training data to a Pandas DataFrame\n",
        "train_pd = train.toPandas()\n",
        "\n",
        "# Define features and label\n",
        "features = df8.columns\n",
        "features.remove('label')\n",
        "\n",
        "# Separate the features and label\n",
        "X = train_pd[features]\n",
        "y = train_pd['label']\n",
        "\n",
        "# Perform SMOTE oversampling\n",
        "sm = SMOTE(random_state=seed)\n",
        "X_resampled, y_resampled = sm.fit_resample(X, y)\n",
        "\n",
        "# Convert the resampled data back to a PySpark DataFrame\n",
        "train_oversampled_pd = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "train_oversampled = spark.createDataFrame(train_oversampled_pd)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Create the Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=seed)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Define the parameter grid for cross-validation\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
        "\n",
        "# Define the evaluator for cross-validation\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create the cross-validator object\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
        "\n",
        "# Fit the model on the oversampled training data\n",
        "model = cv.fit(train_oversampled)\n",
        "\n",
        "# Convert the test data to a Pandas DataFrame\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Separate the features and label in the test data\n",
        "X_test = test_pd[features]\n",
        "y_test = test_pd['label']\n",
        "\n",
        "# Perform SMOTE oversampling on the test data\n",
        "X_test_resampled, y_test_resampled = sm.fit_resample(X_test, y_test)\n",
        "\n",
        "# Convert the resampled test data back to a PySpark DataFrame\n",
        "test_oversampled_pd = pd.concat([X_test_resampled, y_test_resampled], axis=1)\n",
        "test_oversampled = spark.createDataFrame(test_oversampled_pd)\n",
        "\n",
        "# Make predictions on the oversampled test data\n",
        "predictions = model.transform(test_oversampled)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc = evaluator.evaluate(predictions)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "metrics = MulticlassMetrics(predictionAndLabels)\n",
        "confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "print(f\"Matriz de confusión:\\n{confusion_matrix}\")\n",
        "\n",
        "# Manually calculate recall and F1 score\n",
        "TP = confusion_matrix[1, 1]\n",
        "FP = confusion_matrix[0, 1]\n",
        "FN = confusion_matrix[1, 0]\n",
        "precision_manual = TP / (TP + FP)\n",
        "recall_manual = TP / (TP + FN)\n",
        "f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
        "print(f\"Recall (calculado manualmente): {recall_manual:.3f}\")\n",
        "print(f\"F1 (calculado manualmente): {f1_manual:.3f}\")\n",
        "\n",
        "# Get the most important features\n",
        "importances = model.bestModel.stages[-1].featureImportances\n",
        "important_features = sorted(zip(importances, features), reverse=True)\n",
        "print(\"Variables más importantes:\")\n",
        "for importance, feature in important_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n",
        "\n",
        "def calc_ks(data):\n",
        "    data_pd=data.toPandas()\n",
        "    data_pd['good']=(data_pd['label']==0).astype(int)\n",
        "    data_pd['bad']=(data_pd['label']==1).astype(int)\n",
        "    data_pd['bucket']=(data_pd['score'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data_pd.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().score.to_frame(name='min_score')\n",
        "    kstable['max_score']=grouped.max().score\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "score_udf=udf(lambda v:float(v[0]),DoubleType())\n",
        "predictions=predictions.withColumn('score',score_udf('probability'))\n",
        "ks_value=calc_ks(predictions)\n",
        "print(f\"Estadística KS: {ks_value:.3f}\")\n"
      ],
      "metadata": {
        "id": "Y4I9nXUwlWPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Convert the training data to a Pandas DataFrame\n",
        "train_pd = train.toPandas()\n",
        "\n",
        "# Define features and label\n",
        "features = df8.columns\n",
        "features.remove('label')\n",
        "\n",
        "# Separate the features and label\n",
        "X = train_pd[features]\n",
        "y = train_pd['label']\n",
        "\n",
        "# Perform ADASYN oversampling\n",
        "adasyn = ADASYN(random_state=seed)\n",
        "X_resampled, y_resampled = adasyn.fit_resample(X, y)\n",
        "\n",
        "# Convert the resampled data back to a PySpark DataFrame\n",
        "train_oversampled_pd = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "train_oversampled = spark.createDataFrame(train_oversampled_pd)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Create the Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=seed)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Define the parameter grid for cross-validation\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
        "\n",
        "# Define the evaluator for cross-validation\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create the cross-validator object\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
        "\n",
        "# Fit the model on the oversampled training data\n",
        "model = cv.fit(train_oversampled)\n",
        "\n",
        "# Convert the test data to a Pandas DataFrame\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Separate the features and label in the test data\n",
        "X_test = test_pd[features]\n",
        "y_test = test_pd['label']\n",
        "\n",
        "# Perform ADASYN oversampling on the test data\n",
        "X_test_resampled, y_test_resampled = adasyn.fit_resample(X_test, y_test)\n",
        "\n",
        "# Convert the resampled test data back to a PySpark DataFrame\n",
        "test_oversampled_pd = pd.concat([X_test_resampled, y_test_resampled], axis=1)\n",
        "test_oversampled = spark.createDataFrame(test_oversampled_pd)\n",
        "\n",
        "# Make predictions on the oversampled test data\n",
        "predictions = model.transform(test_oversampled)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc = evaluator.evaluate(predictions)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "metrics = MulticlassMetrics(predictionAndLabels)\n",
        "confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "print(f\"Matriz de confusión:\\n{confusion_matrix}\")\n",
        "\n",
        "# Manually calculate recall and F1 score\n",
        "TP = confusion_matrix[1, 1]\n",
        "FP = confusion_matrix[0, 1]\n",
        "FN = confusion_matrix[1, 0]\n",
        "precision_manual = TP / (TP + FP)\n",
        "recall_manual = TP / (TP + FN)\n",
        "f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
        "print(f\"Recall (calculado manualmente): {recall_manual:.3f}\")\n",
        "print(f\"F1 (calculado manualmente): {f1_manual:.3f}\")\n",
        "\n",
        "# Get the most important features\n",
        "importances = model.bestModel.stages[-1].featureImportances\n",
        "important_features = sorted(zip(importances, features), reverse=True)\n",
        "print(\"Variables más importantes:\")\n",
        "for importance, feature in important_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n",
        "\n",
        "def calc_ks(data):\n",
        "    data_pd=data.toPandas()\n",
        "    data_pd['good']=(data_pd['label']==0).astype(int)\n",
        "    data_pd['bad']=(data_pd['label']==1).astype(int)\n",
        "    data_pd['bucket']=(data_pd['score'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data_pd.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().score.to_frame(name='min_score')\n",
        "    kstable['max_score']=grouped.max().score\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "score_udf=udf(lambda v:float(v[0]),DoubleType())\n",
        "predictions=predictions.withColumn('score',score_udf('rawPrediction'))\n",
        "ks_value=calc_ks(predictions)\n",
        "print(f\"Estadística KS: {ks_value:.3f}\")\n"
      ],
      "metadata": {
        "id": "uDEvhrY8pAdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### svm\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 12345\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Separate the features and the label\n",
        "X = pandas_df.drop('Malo_Dias_tot', axis=1)\n",
        "y = pandas_df['Malo_Dias_tot']\n",
        "\n",
        "# Apply SMOTE to the data\n",
        "smote = SMOTE(random_state=random_state)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Convert the resampled data to a PySpark DataFrame\n",
        "resampled_data = spark.createDataFrame(pd.concat([X_resampled, y_resampled], axis=1))\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train, test = resampled_data.randomSplit([0.7, 0.3], seed=random_state)\n",
        "\n",
        "# Define the feature columns\n",
        "feature_cols = [col for col in train.columns if col != 'Malo_Dias_tot']\n",
        "\n",
        "# Create a VectorAssembler to combine the feature columns into a single vector column\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "\n",
        "# Create a StandardScaler to standardize the features\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=True)\n",
        "\n",
        "# Train the Support Vector Machine model without cross-validation\n",
        "svm = LinearSVC(featuresCol='scaledFeatures', labelCol='Malo_Dias_tot', maxIter=10, regParam=0.1)\n",
        "\n",
        "# Create a pipeline to chain the assembler, scaler and SVM together\n",
        "pipeline = Pipeline(stages=[assembler, scaler, svm])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model = pipeline.fit(train)\n",
        "\n",
        "# Convert the test data to a Pandas DataFrame\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Separate the features and label in the test data\n",
        "X_test = test_pd[feature_cols]\n",
        "y_test = test_pd['Malo_Dias_tot']\n",
        "\n",
        "# Perform SMOTE oversampling on the test data\n",
        "X_test_resampled, y_test_resampled = smote.fit_resample(X_test, y_test)\n",
        "\n",
        "# Convert the resampled test data back to a PySpark DataFrame\n",
        "test_oversampled_pd = pd.concat([X_test_resampled, y_test_resampled], axis=1)\n",
        "test_oversampled = spark.createDataFrame(test_oversampled_pd)\n",
        "\n",
        "# Make predictions on the oversampled test data\n",
        "predictions = model.transform(test_oversampled)\n",
        "\n",
        "# Calculate metrics\n",
        "tp = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 1)].count()\n",
        "tn = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 0)].count()\n",
        "fp = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 1)].count()\n",
        "fn = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 0)].count()\n",
        "\n",
        "print(f'Confusion Matrix:\\n[[{tn} {fp}]\\n [{fn} {tp}]]')\n",
        "\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1_score}')\n",
        "\n",
        "# Calculate AUC using BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='Malo_Dias_tot', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f'AUC: {auc}')\n",
        "\n",
        "def calc_ks(data):\n",
        "    data_pd=data.toPandas()\n",
        "    data_pd['good']=(data_pd['Malo_Dias_tot']==0).astype(int)\n",
        "    data_pd['bad']=(data_pd['Malo_Dias_tot']==1).astype(int)\n",
        "    data_pd['bucket']=(data_pd['score'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data_pd.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().score.to_frame(name='min_score')\n",
        "    kstable['max_score']=grouped.max().score\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "score_udf=udf(lambda v:float(v[0]),DoubleType())\n",
        "predictions=predictions.withColumn('score',score_udf('rawPrediction'))\n",
        "ks_value=calc_ks(predictions)\n",
        "print(f\"Estadístico KS: {ks_value:.3f}\")\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "importances = model.stages[-1].coefficients.toArray()\n",
        "importance_df = pd.DataFrame(list(zip(feature_cols, importances)), columns=['Feature', 'Importance']).sort_values('Importance', ascending=False)\n",
        "print(importance_df.head(10))\n"
      ],
      "metadata": {
        "id": "wdkfpn4p3Lvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 0\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Separate the features and the label\n",
        "X = pandas_df.drop('Malo_Dias_tot', axis=1)\n",
        "y = pandas_df['Malo_Dias_tot']\n",
        "\n",
        "# Apply ADASYN to the data with the corrected sampling_strategy parameter\n",
        "adasyn = ADASYN(random_state=random_state, sampling_strategy='minority')\n",
        "X_resampled, y_resampled = adasyn.fit_resample(X, y)\n",
        "\n",
        "# Convert the resampled data to a PySpark DataFrame\n",
        "resampled_data = spark.createDataFrame(pd.concat([X_resampled, y_resampled], axis=1))\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train, test = resampled_data.randomSplit([0.7, 0.3], seed=random_state)\n",
        "\n",
        "# Define the feature columns\n",
        "feature_cols = [col for col in train.columns if col != 'Malo_Dias_tot']\n",
        "\n",
        "# Create a VectorAssembler to combine the feature columns into a single vector column\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "\n",
        "# Create a StandardScaler to standardize the features\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=True)\n",
        "\n",
        "# Train the Support Vector Machine model without cross-validation\n",
        "svm = LinearSVC(featuresCol='scaledFeatures', labelCol='Malo_Dias_tot', maxIter=10, regParam=0.1)\n",
        "\n",
        "# Create a pipeline to chain the assembler, scaler and SVM together\n",
        "pipeline = Pipeline(stages=[assembler, scaler, svm])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model = pipeline.fit(train)\n",
        "\n",
        "# Convert the test data to a Pandas DataFrame\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Separate the features and label in the test data\n",
        "X_test = test_pd[feature_cols]\n",
        "y_test = test_pd['Malo_Dias_tot']\n",
        "\n",
        "# Perform ADASYN oversampling on the test data with the corrected sampling_strategy parameter\n",
        "X_test_resampled, y_test_resampled = adasyn.fit_resample(X_test, y_test)\n",
        "\n",
        "# Convert the resampled test data back to a PySpark DataFrame\n",
        "test_oversampled_pd = pd.concat([X_test_resampled, y_test_resampled], axis=1)\n",
        "test_oversampled = spark.createDataFrame(test_oversampled_pd)\n",
        "\n",
        "# Make predictions on the oversampled test data\n",
        "predictions = model.transform(test_oversampled)\n",
        "\n",
        "# Calculate metrics\n",
        "tp = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 1)].count()\n",
        "tn = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 0)].count()\n",
        "fp = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 1)].count()\n",
        "fn = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 0)].count()\n",
        "\n",
        "print(f'Confusion Matrix:\\n[[{tn} {fp}]\\n [{fn} {tp}]]')\n",
        "\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1_score}')\n",
        "\n",
        "# Calculate AUC using BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='Malo_Dias_tot', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f'AUC: {auc}')\n",
        "\n",
        "def calc_ks(data):\n",
        "    data_pd=data.toPandas()\n",
        "    data_pd['good']=(data_pd['Malo_Dias_tot']==0).astype(int)\n",
        "    data_pd['bad']=(data_pd['Malo_Dias_tot']==1).astype(int)\n",
        "    data_pd['bucket']=(data_pd['score'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data_pd.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().score.to_frame(name='min_score')\n",
        "    kstable['max_score']=grouped.max().score\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "score_udf=udf(lambda v:float(v[0]),DoubleType())\n",
        "predictions=predictions.withColumn('score',score_udf('rawPrediction'))\n",
        "ks_value=calc_ks(predictions)\n",
        "print(f\"Estadístico KS: {ks_value:.3f}\")\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "importances = model.stages[-1].coefficients.toArray()\n",
        "importance_df = pd.DataFrame(list(zip(feature_cols, importances)), columns=['Feature', 'Importance']).sort_values('Importance', ascending=False)\n",
        "print(importance_df.head(10))\n"
      ],
      "metadata": {
        "id": "CDMvukWU7VtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EOEkDPR__ZLS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
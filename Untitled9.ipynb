{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuq3QBKr5FT07tiQbkj98V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristValen/Acciones-RNR/blob/main/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnDIFep0ZSFo"
      },
      "outputs": [],
      "source": [
        "#correccion\n",
        "\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Convert the training data to a Pandas DataFrame\n",
        "train_pd = train.toPandas()\n",
        "\n",
        "# Define features and label\n",
        "features = df8.columns\n",
        "features.remove('label')\n",
        "\n",
        "# Separate the features and label\n",
        "X = train_pd[features]\n",
        "y = train_pd['label']\n",
        "\n",
        "# Perform NearMiss undersampling\n",
        "nm = NearMiss()\n",
        "X_resampled, y_resampled = nm.fit_resample(X, y)\n",
        "\n",
        "# Convert the resampled data back to a PySpark DataFrame\n",
        "train_undersampled_pd = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "train_undersampled = spark.createDataFrame(train_undersampled_pd)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Create the Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=seed)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Define the parameter grid for cross-validation\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
        "\n",
        "# Define the evaluator for cross-validation\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create the cross-validator object\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
        "\n",
        "# Fit the model on the undersampled training data\n",
        "model = cv.fit(train_undersampled)\n",
        "\n",
        "# Convert the test data to a Pandas DataFrame\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Separate the features and label in the test data\n",
        "X_test = test_pd[features]\n",
        "y_test = test_pd['label']\n",
        "\n",
        "# Perform NearMiss undersampling on the test data\n",
        "X_test_resampled, y_test_resampled = nm.fit_resample(X_test, y_test)\n",
        "\n",
        "# Convert the resampled test data back to a PySpark DataFrame\n",
        "test_undersampled_pd = pd.concat([X_test_resampled, y_test_resampled], axis=1)\n",
        "test_undersampled = spark.createDataFrame(test_undersampled_pd)\n",
        "\n",
        "# Make predictions on the undersampled test data\n",
        "predictions = model.transform(test_undersampled)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc = evaluator.evaluate(predictions)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "metrics = MulticlassMetrics(predictionAndLabels)\n",
        "confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "print(f\"Confusion matrix:\\n{confusion_matrix}\")\n",
        "\n",
        "# Manually calculate recall and F1 score\n",
        "TP = confusion_matrix[1, 1]\n",
        "FP = confusion_matrix[0, 1]\n",
        "FN = confusion_matrix[1, 0]\n",
        "precision_manual = TP / (TP + FP)\n",
        "recall_manual = TP / (TP + FN)\n",
        "f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
        "print(f\"Recall (manually calculated): {recall_manual:.3f}\")\n",
        "print(f\"F1 (manually calculated): {f1_manual:.3f}\")\n",
        "\n",
        "# Get the most important features\n",
        "importances = model.bestModel.stages[-1].featureImportances\n",
        "important_features = sorted(zip(importances, features), reverse=True)\n",
        "print(\"Most important features:\")\n",
        "for importance, feature in important_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n",
        "\n",
        "def calc_ks(data):\n",
        "    data_pd=data.toPandas()\n",
        "    data_pd['good']=(data_pd['label']==0).astype(int)\n",
        "    data_pd['bad']=(data_pd['label']==1).astype(int)\n",
        "    data_pd['bucket']=(data_pd['score'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data_pd.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().score.to_frame(name='min_score')\n",
        "    kstable['max_score']=grouped.max().score\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "score_udf=udf(lambda v:float(v[0]),DoubleType())\n",
        "predictions=predictions.withColumn('score',score_udf('probability'))\n",
        "ks_value=calc_ks(predictions)\n",
        "print(f\"KS statistic: {ks_value:.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import TomekLinks\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Convert the training data to a Pandas DataFrame\n",
        "train_pd = train.toPandas()\n",
        "\n",
        "# Define features and label\n",
        "features = df8.columns\n",
        "features.remove('label')\n",
        "\n",
        "# Separate the features and label\n",
        "X = train_pd[features]\n",
        "y = train_pd['label']\n",
        "\n",
        "# Perform Tomek Links undersampling\n",
        "tl = TomekLinks()\n",
        "X_resampled, y_resampled = tl.fit_resample(X, y)\n",
        "\n",
        "# Convert the resampled data back to a PySpark DataFrame\n",
        "train_undersampled_pd = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "train_undersampled = spark.createDataFrame(train_undersampled_pd)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Create the Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=seed)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Define the parameter grid for cross-validation\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
        "\n",
        "# Define the evaluator for cross-validation\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create the cross-validator object\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
        "\n",
        "# Fit the model on the undersampled training data\n",
        "model = cv.fit(train_undersampled)\n",
        "\n",
        "# Convert the test data to a Pandas DataFrame\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Separate the features and label in the test data\n",
        "X_test = test_pd[features]\n",
        "y_test = test_pd['label']\n",
        "\n",
        "# Perform Tomek Links undersampling on the test data\n",
        "X_test_resampled, y_test_resampled = tl.fit_resample(X_test, y_test)\n",
        "\n",
        "# Convert the resampled test data back to a PySpark DataFrame\n",
        "test_undersampled_pd = pd.concat([X_test_resampled, y_test_resampled], axis=1)\n",
        "test_undersampled = spark.createDataFrame(test_undersampled_pd)\n",
        "\n",
        "# Make predictions on the undersampled test data\n",
        "predictions = model.transform(test_undersampled)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc = evaluator.evaluate(predictions)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "metrics = MulticlassMetrics(predictionAndLabels)\n",
        "confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "print(f\"Confusion matrix:\\n{confusion_matrix}\")\n",
        "\n",
        "# Manually calculate recall and F1 score\n",
        "TP = confusion_matrix[1, 1]\n",
        "FP = confusion_matrix[0, 1]\n",
        "FN = confusion_matrix[1, 0]\n",
        "precision_manual = TP / (TP + FP)\n",
        "recall_manual = TP / (TP + FN)\n",
        "f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
        "print(f\"Recall (manually calculated): {recall_manual:.3f}\")\n",
        "print(f\"F1 (manually calculated): {f1_manual:.3f}\")\n",
        "\n",
        "# Get the most important features\n",
        "importances = model.bestModel.stages[-1].featureImportances\n",
        "important_features = sorted(zip(importances, features), reverse=True)\n",
        "print(\"Most important features:\")\n",
        "for importance, feature in important_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n",
        "\n",
        "def calc_ks(data):\n",
        "    data_pd=data.toPandas()\n",
        "    data_pd['good']=(data_pd['label']==0).astype(int)\n",
        "    data_pd['bad']=(data_pd['label']==1).astype(int)\n",
        "    data_pd['bucket']=(data_pd['score'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data_pd.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().score.to_frame(name='min_score')\n",
        "    kstable['max_score']=grouped.max().score\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "score_udf=udf(lambda v:float(v[0]),DoubleType())\n",
        "predictions=predictions.withColumn('score',score_udf('probability'))\n",
        "ks_value=calc_ks(predictions)\n",
        "print(f\"KS statistic: {ks_value:.3f}\")\n"
      ],
      "metadata": {
        "id": "AlOyzdcXekOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Convert the training data to a Pandas DataFrame\n",
        "train_pd = train.toPandas()\n",
        "\n",
        "# Define features and label\n",
        "features = df8.columns\n",
        "features.remove('label')\n",
        "\n",
        "# Separate the features and label\n",
        "X = train_pd[features]\n",
        "y = train_pd['label']\n",
        "\n",
        "# Perform SMOTE oversampling\n",
        "sm = SMOTE(random_state=seed)\n",
        "X_resampled, y_resampled = sm.fit_resample(X, y)\n",
        "\n",
        "# Convert the resampled data back to a PySpark DataFrame\n",
        "train_oversampled_pd = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "train_oversampled = spark.createDataFrame(train_oversampled_pd)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Create the Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=seed)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Define the parameter grid for cross-validation\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
        "\n",
        "# Define the evaluator for cross-validation\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create the cross-validator object\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
        "\n",
        "# Fit the model on the oversampled training data\n",
        "model = cv.fit(train_oversampled)\n",
        "\n",
        "# Convert the test data to a Pandas DataFrame\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Separate the features and label in the test data\n",
        "X_test = test_pd[features]\n",
        "y_test = test_pd['label']\n",
        "\n",
        "# Perform SMOTE oversampling on the test data\n",
        "X_test_resampled, y_test_resampled = sm.fit_resample(X_test, y_test)\n",
        "\n",
        "# Convert the resampled test data back to a PySpark DataFrame\n",
        "test_oversampled_pd = pd.concat([X_test_resampled, y_test_resampled], axis=1)\n",
        "test_oversampled = spark.createDataFrame(test_oversampled_pd)\n",
        "\n",
        "# Make predictions on the oversampled test data\n",
        "predictions = model.transform(test_oversampled)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc = evaluator.evaluate(predictions)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "metrics = MulticlassMetrics(predictionAndLabels)\n",
        "confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "print(f\"Matriz de confusión:\\n{confusion_matrix}\")\n",
        "\n",
        "# Manually calculate recall and F1 score\n",
        "TP = confusion_matrix[1, 1]\n",
        "FP = confusion_matrix[0, 1]\n",
        "FN = confusion_matrix[1, 0]\n",
        "precision_manual = TP / (TP + FP)\n",
        "recall_manual = TP / (TP + FN)\n",
        "f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
        "print(f\"Recall (calculado manualmente): {recall_manual:.3f}\")\n",
        "print(f\"F1 (calculado manualmente): {f1_manual:.3f}\")\n",
        "\n",
        "# Get the most important features\n",
        "importances = model.bestModel.stages[-1].featureImportances\n",
        "important_features = sorted(zip(importances, features), reverse=True)\n",
        "print(\"Variables más importantes:\")\n",
        "for importance, feature in important_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n",
        "\n",
        "def calc_ks(data):\n",
        "    data_pd=data.toPandas()\n",
        "    data_pd['good']=(data_pd['label']==0).astype(int)\n",
        "    data_pd['bad']=(data_pd['label']==1).astype(int)\n",
        "    data_pd['bucket']=(data_pd['score'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data_pd.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().score.to_frame(name='min_score')\n",
        "    kstable['max_score']=grouped.max().score\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "score_udf=udf(lambda v:float(v[0]),DoubleType())\n",
        "predictions=predictions.withColumn('score',score_udf('probability'))\n",
        "ks_value=calc_ks(predictions)\n",
        "print(f\"Estadística KS: {ks_value:.3f}\")\n"
      ],
      "metadata": {
        "id": "Y4I9nXUwlWPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Convert the training data to a Pandas DataFrame\n",
        "train_pd = train.toPandas()\n",
        "\n",
        "# Define features and label\n",
        "features = df8.columns\n",
        "features.remove('label')\n",
        "\n",
        "# Separate the features and label\n",
        "X = train_pd[features]\n",
        "y = train_pd['label']\n",
        "\n",
        "# Perform ADASYN oversampling\n",
        "adasyn = ADASYN(random_state=seed)\n",
        "X_resampled, y_resampled = adasyn.fit_resample(X, y)\n",
        "\n",
        "# Convert the resampled data back to a PySpark DataFrame\n",
        "train_oversampled_pd = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "train_oversampled = spark.createDataFrame(train_oversampled_pd)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Create the Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=seed)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Define the parameter grid for cross-validation\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
        "\n",
        "# Define the evaluator for cross-validation\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create the cross-validator object\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
        "\n",
        "# Fit the model on the oversampled training data\n",
        "model = cv.fit(train_oversampled)\n",
        "\n",
        "# Convert the test data to a Pandas DataFrame\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Separate the features and label in the test data\n",
        "X_test = test_pd[features]\n",
        "y_test = test_pd['label']\n",
        "\n",
        "# Perform ADASYN oversampling on the test data\n",
        "X_test_resampled, y_test_resampled = adasyn.fit_resample(X_test, y_test)\n",
        "\n",
        "# Convert the resampled test data back to a PySpark DataFrame\n",
        "test_oversampled_pd = pd.concat([X_test_resampled, y_test_resampled], axis=1)\n",
        "test_oversampled = spark.createDataFrame(test_oversampled_pd)\n",
        "\n",
        "# Make predictions on the oversampled test data\n",
        "predictions = model.transform(test_oversampled)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc = evaluator.evaluate(predictions)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "metrics = MulticlassMetrics(predictionAndLabels)\n",
        "confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "print(f\"Matriz de confusión:\\n{confusion_matrix}\")\n",
        "\n",
        "# Manually calculate recall and F1 score\n",
        "TP = confusion_matrix[1, 1]\n",
        "FP = confusion_matrix[0, 1]\n",
        "FN = confusion_matrix[1, 0]\n",
        "precision_manual = TP / (TP + FP)\n",
        "recall_manual = TP / (TP + FN)\n",
        "f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
        "print(f\"Recall (calculado manualmente): {recall_manual:.3f}\")\n",
        "print(f\"F1 (calculado manualmente): {f1_manual:.3f}\")\n",
        "\n",
        "# Get the most important features\n",
        "importances = model.bestModel.stages[-1].featureImportances\n",
        "important_features = sorted(zip(importances, features), reverse=True)\n",
        "print(\"Variables más importantes:\")\n",
        "for importance, feature in important_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n",
        "\n",
        "def calc_ks(data):\n",
        "    data_pd=data.toPandas()\n",
        "    data_pd['good']=(data_pd['label']==0).astype(int)\n",
        "    data_pd['bad']=(data_pd['label']==1).astype(int)\n",
        "    data_pd['bucket']=(data_pd['score'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data_pd.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().score.to_frame(name='min_score')\n",
        "    kstable['max_score']=grouped.max().score\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "score_udf=udf(lambda v:float(v[0]),DoubleType())\n",
        "predictions=predictions.withColumn('score',score_udf('rawPrediction'))\n",
        "ks_value=calc_ks(predictions)\n",
        "print(f\"Estadística KS: {ks_value:.3f}\")\n"
      ],
      "metadata": {
        "id": "uDEvhrY8pAdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### svm\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 12345\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Separate the features and the label\n",
        "X = pandas_df.drop('Malo_Dias_tot', axis=1)\n",
        "y = pandas_df['Malo_Dias_tot']\n",
        "\n",
        "# Apply SMOTE to the data\n",
        "smote = SMOTE(random_state=random_state)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Convert the resampled data to a PySpark DataFrame\n",
        "resampled_data = spark.createDataFrame(pd.concat([X_resampled, y_resampled], axis=1))\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train, test = resampled_data.randomSplit([0.7, 0.3], seed=random_state)\n",
        "\n",
        "# Define the feature columns\n",
        "feature_cols = [col for col in train.columns if col != 'Malo_Dias_tot']\n",
        "\n",
        "# Create a VectorAssembler to combine the feature columns into a single vector column\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "\n",
        "# Create a StandardScaler to standardize the features\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=True)\n",
        "\n",
        "# Train the Support Vector Machine model without cross-validation\n",
        "svm = LinearSVC(featuresCol='scaledFeatures', labelCol='Malo_Dias_tot', maxIter=10, regParam=0.1)\n",
        "\n",
        "# Create a pipeline to chain the assembler, scaler and SVM together\n",
        "pipeline = Pipeline(stages=[assembler, scaler, svm])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model = pipeline.fit(train)\n",
        "\n",
        "# Convert the test data to a Pandas DataFrame\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Separate the features and label in the test data\n",
        "X_test = test_pd[feature_cols]\n",
        "y_test = test_pd['Malo_Dias_tot']\n",
        "\n",
        "# Perform SMOTE oversampling on the test data\n",
        "X_test_resampled, y_test_resampled = smote.fit_resample(X_test, y_test)\n",
        "\n",
        "# Convert the resampled test data back to a PySpark DataFrame\n",
        "test_oversampled_pd = pd.concat([X_test_resampled, y_test_resampled], axis=1)\n",
        "test_oversampled = spark.createDataFrame(test_oversampled_pd)\n",
        "\n",
        "# Make predictions on the oversampled test data\n",
        "predictions = model.transform(test_oversampled)\n",
        "\n",
        "# Calculate metrics\n",
        "tp = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 1)].count()\n",
        "tn = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 0)].count()\n",
        "fp = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 1)].count()\n",
        "fn = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 0)].count()\n",
        "\n",
        "print(f'Confusion Matrix:\\n[[{tn} {fp}]\\n [{fn} {tp}]]')\n",
        "\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1_score}')\n",
        "\n",
        "# Calculate AUC using BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='Malo_Dias_tot', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f'AUC: {auc}')\n",
        "\n",
        "def calc_ks(data):\n",
        "    data_pd=data.toPandas()\n",
        "    data_pd['good']=(data_pd['Malo_Dias_tot']==0).astype(int)\n",
        "    data_pd['bad']=(data_pd['Malo_Dias_tot']==1).astype(int)\n",
        "    data_pd['bucket']=(data_pd['score'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data_pd.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().score.to_frame(name='min_score')\n",
        "    kstable['max_score']=grouped.max().score\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "score_udf=udf(lambda v:float(v[0]),DoubleType())\n",
        "predictions=predictions.withColumn('score',score_udf('rawPrediction'))\n",
        "ks_value=calc_ks(predictions)\n",
        "print(f\"Estadístico KS: {ks_value:.3f}\")\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "importances = model.stages[-1].coefficients.toArray()\n",
        "importance_df = pd.DataFrame(list(zip(feature_cols, importances)), columns=['Feature', 'Importance']).sort_values('Importance', ascending=False)\n",
        "print(importance_df.head(10))\n"
      ],
      "metadata": {
        "id": "wdkfpn4p3Lvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "\n",
        "# Establecer la semilla para el generador de números aleatorios\n",
        "random_state = 0\n",
        "\n",
        "# Convertir el DataFrame de PySpark a Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Separar las características y la etiqueta\n",
        "X = pandas_df.drop('Malo_Dias_tot', axis=1)\n",
        "y = pandas_df['Malo_Dias_tot']\n",
        "\n",
        "# Aplicar ADASYN a los datos con el parámetro sampling_strategy corregido\n",
        "adasyn = ADASYN(random_state=random_state, sampling_strategy='minority')\n",
        "X_resampled, y_resampled = adasyn.fit_resample(X, y)\n",
        "\n",
        "# Convertir los datos remuestreados a un DataFrame de PySpark\n",
        "resampled_data = spark.createDataFrame(pd.concat([X_resampled, y_resampled], axis=1))\n",
        "\n",
        "# Dividir el conjunto de datos en conjuntos de entrenamiento y prueba\n",
        "train, test = resampled_data.randomSplit([0.7, 0.3], seed=random_state)\n",
        "\n",
        "# Definir las columnas de características\n",
        "feature_cols = [col for col in train.columns if col != 'Malo_Dias_tot']\n",
        "\n",
        "# Crear un VectorAssembler para combinar las columnas de características en una sola columna vectorial\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "\n",
        "# Crear un StandardScaler para estandarizar las características\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=True)\n",
        "\n",
        "# Entrenar el modelo de Máquina de Vectores de Soporte sin validación cruzada\n",
        "svm = LinearSVC(featuresCol='scaledFeatures', labelCol='Malo_Dias_tot', maxIter=10, regParam=0.1)\n",
        "\n",
        "# Crear un pipeline para encadenar el ensamblador, el escalador y el SVM juntos\n",
        "pipeline = Pipeline(stages=[assembler, scaler, svm])\n",
        "\n",
        "# Ajustar el pipeline a los datos de entrenamiento\n",
        "model = pipeline.fit(train)\n",
        "\n",
        "# Convertir los datos de prueba a un DataFrame de Pandas\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Separar las características y la etiqueta en los datos de prueba\n",
        "X_test = test_pd[feature_cols]\n",
        "y_test = test_pd['Malo_Dias_tot']\n",
        "\n",
        "# Realizar sobremuestreo ADASYN en los datos de prueba con el parámetro sampling_strategy corregido\n",
        "X_test_resampled, y_test_resampled = adasyn.fit_resample(X_test, y_test)\n",
        "\n",
        "# Convertir los datos de prueba remuestreados nuevamente a un DataFrame de PySpark\n",
        "test_oversampled_pd = pd.concat([X_test_resampled, y_test_resampled], axis=1)\n",
        "test_oversampled = spark.createDataFrame(test_oversampled_pd)\n",
        "\n",
        "# Hacer predicciones en los datos de prueba sobremuestreados\n",
        "predictions = model.transform(test_oversampled)\n",
        "\n",
        "# Calcular métricas\n",
        "tp = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 1)].count()\n",
        "tn = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 0)].count()\n",
        "fp = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 1)].count()\n",
        "fn = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 0)].count()\n",
        "\n",
        "print(f'Matriz de confusión:\\n[[{tn} {fp}]\\n [{fn} {tp}]]')\n",
        "\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(f'Precisión: {accuracy}')\n",
        "print(f'Precisión: {precision}')\n",
        "print(f'Sensibilidad: {recall}')\n",
        "print(f'Puntuación F1: {f1_score}')\n",
        "\n",
        "# Calcular AUC usando BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='Malo_Dias_tot', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f'AUC: {auc}')\n",
        "\n",
        "def calc_ks(data):\n",
        "    data_pd=data.toPandas()\n",
        "    data_pd['good']=(data_pd['Malo_Dias_tot']==0).astype(int)\n",
        "    data_pd['bad']=(data_pd['Malo_Dias_tot']==1).astype(int)\n",
        "    data_pd['bucket']=(data_pd['score'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data_pd.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().score.to_frame(name='min_score')\n",
        "    kstable['max_score']=grouped.max().score\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "score_udf=udf(lambda v:float(v[0]),DoubleType())\n",
        "predictions=predictions.withColumn('score',score_udf('rawPrediction'))\n",
        "ks_value=calc_ks(predictions)\n",
        "print(f\"Estadístico KS: {ks_value:.3f}\")\n",
        "\n",
        "# Obtener las 10 variables más importantes del modelo\n",
        "importances = model.stages[-1].coefficients.toArray()\n",
        "importance_df = pd.DataFrame(list(zip(feature_cols, importances)), columns=['Feature', 'Importance']).sort_values('Importance', ascending=False)\n",
        "print(importance_df.head(10))\n",
        "\n"
      ],
      "metadata": {
        "id": "CDMvukWU7VtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.sql.functions import col, udf, rand\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "\n",
        "# Establecer la semilla para el generador de números aleatorios\n",
        "random_state = 0\n",
        "\n",
        "# Calcular el número de ejemplos en cada clase\n",
        "class_counts = df_2.groupBy('Malo_Dias_tot').count().collect()\n",
        "num_positives = class_counts[1][1]\n",
        "num_negatives = class_counts[0][1]\n",
        "\n",
        "# Calcular el número de ejemplos negativos a mantener\n",
        "num_to_keep = int(num_positives / num_negatives * num_negatives)\n",
        "\n",
        "# Seleccionar un subconjunto aleatorio de la clase mayoritaria\n",
        "majority_subset = df_2.filter(col('Malo_Dias_tot') == 0).orderBy(rand(seed=random_state)).limit(num_to_keep)\n",
        "\n",
        "# Combinar el subconjunto mayoritario con la clase minoritaria para crear los datos submuestreados\n",
        "undersampled_data = majority_subset.union(df_2.filter(col('Malo_Dias_tot') == 1))\n",
        "\n",
        "# Dividir el conjunto de datos en conjuntos de entrenamiento y prueba\n",
        "train, test = undersampled_data.randomSplit([0.7, 0.3], seed=random_state)\n",
        "\n",
        "# Definir las columnas de características\n",
        "feature_cols = [col for col in train.columns if col != 'Malo_Dias_tot']\n",
        "\n",
        "# Crear un VectorAssembler para combinar las columnas de características en una sola columna vectorial\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "\n",
        "# Crear un StandardScaler para estandarizar las características\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=True)\n",
        "\n",
        "# Entrenar el modelo de Máquina de Vectores de Soporte sin validación cruzada\n",
        "svm = LinearSVC(featuresCol='scaledFeatures', labelCol='Malo_Dias_tot', maxIter=10, regParam=0.1)\n",
        "\n",
        "# Crear un pipeline para encadenar el ensamblador, el escalador y el SVM juntos\n",
        "pipeline = Pipeline(stages=[assembler, scaler, svm])\n",
        "\n",
        "# Ajustar el pipeline a los datos de entrenamiento\n",
        "model = pipeline.fit(train)\n",
        "\n",
        "# Submuestrear los datos de prueba utilizando la misma técnica que se aplicó a los datos de entrenamiento\n",
        "test_positives = test.filter(col('Malo_Dias_tot') == 1)\n",
        "test_negatives = test.filter(col('Malo_Dias_tot') == 0).orderBy(rand(seed=random_state)).limit(num_to_keep)\n",
        "test_undersampled = test_positives.union(test_negatives)\n",
        "\n",
        "# Hacer predicciones en los datos de prueba submuestreados\n",
        "predictions = model.transform(test_undersampled)\n",
        "\n",
        "# Calcular métricas\n",
        "tp = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 1)].count()\n",
        "tn = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 0)].count()\n",
        "fp = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 1)].count()\n",
        "fn = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 0)].count()\n",
        "\n",
        "print(f'Matriz de confusión:\\n[[{tn} {fp}]\\n [{fn} {tp}]]')\n",
        "\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(f'Precisión: {accuracy}')\n",
        "print(f'Precisión: {precision}')\n",
        "print(f'Sensibilidad: {recall}')\n",
        "print(f'Puntuación F1: {f1_score}')\n",
        "\n",
        "# Calcular AUC usando BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='Malo_Dias_tot', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f'AUC: {auc}')\n",
        "\n",
        "def calc_ks(data):\n",
        "    data_pd=data.toPandas()\n",
        "    data_pd['good']=(data_pd['Malo_Dias_tot']==0).astype(int)\n",
        "    data_pd['bad']=(data_pd['Malo_Dias_tot']==1).astype(int)\n",
        "    data_pd['bucket']=(data_pd['score'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data_pd.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().score.to_frame(name='min_score')\n",
        "    kstable['max_score']=grouped.max().score\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "score_udf=udf(lambda v:float(v[0]),DoubleType())\n",
        "predictions=predictions.withColumn('score',score_udf('rawPrediction'))\n",
        "ks_value=calc_ks(predictions)\n",
        "print(f\"Estadístico KS: {ks_value:.3f}\")\n",
        "\n",
        "# Obtener las 10 variables más importantes del modelo\n",
        "importances = model.stages[-1].coefficients.toArray()\n",
        "importance_df = pd.DataFrame(list(zip(feature_cols, importances)), columns=['Feature', 'Importance']).sort_values('Importance', ascending=False)\n",
        "print(importance_df.head(10))\n"
      ],
      "metadata": {
        "id": "EOEkDPR__ZLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "\n",
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 0\n",
        "\n",
        "# Convert the Spark DataFrame to a Pandas DataFrame\n",
        "data_pd = df_2.toPandas()\n",
        "\n",
        "# Define the feature columns\n",
        "feature_cols = [col for col in data_pd.columns if col != 'Malo_Dias_tot']\n",
        "\n",
        "# Extract the feature matrix and label vector\n",
        "X = data_pd[feature_cols].values\n",
        "y = data_pd['Malo_Dias_tot'].values\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Create a SMOTE object\n",
        "smote = SMOTE(random_state=random_state)\n",
        "\n",
        "# Train the Decision Tree model without cross-validation\n",
        "dt = DecisionTreeClassifier(random_state=random_state)\n",
        "\n",
        "# Create a pipeline to chain SMOTE and Decision Tree together\n",
        "pipeline = Pipeline([('smote', smote), ('dt', dt)])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model = pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Perform SMOTE oversampling on the test data\n",
        "X_test_resampled, y_test_resampled = smote.fit_resample(X_test, y_test)\n",
        "\n",
        "# Predict values for the oversampled test set\n",
        "y_pred = model.predict(X_test_resampled)\n",
        "\n",
        "# Calculate metrics\n",
        "tn, fp, fn, tp = confusion_matrix(y_test_resampled, y_pred).ravel()\n",
        "\n",
        "print(f'Confusion Matrix:\\n[[{tn} {fp}]\\n [{fn} {tp}]]')\n",
        "\n",
        "accuracy = accuracy_score(y_test_resampled, y_pred)\n",
        "precision = precision_score(y_test_resampled, y_pred)\n",
        "recall = recall_score(y_test_resampled, y_pred)\n",
        "f1_score = f1_score(y_test_resampled, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1_score}')\n",
        "\n",
        "# Calculate AUC using roc_auc_score function\n",
        "auc = roc_auc_score(y_test_resampled, y_pred)\n",
        "print(f'AUC: {auc}')\n",
        "\n",
        "# Calculate the KS statistic\n",
        "data_pd = pd.DataFrame({'Malo_Dias_tot': y_test_resampled, 'score': model.predict_proba(X_test_resampled)[:, 1]})\n",
        "data_pd['good'] = (data_pd['Malo_Dias_tot'] == 0).astype(int)\n",
        "data_pd['bad'] = (data_pd['Malo_Dias_tot'] == 1).astype(int)\n",
        "data_pd['bucket'] = (data_pd['score'].rank(pct=True) * 10).astype(int)\n",
        "grouped = data_pd.groupby('bucket', as_index=True)\n",
        "kstable = grouped.min().score.to_frame(name='min_score')\n",
        "kstable['max_score'] = grouped.max().score\n",
        "kstable['bads'] = grouped.sum().bad\n",
        "kstable['goods'] = grouped.sum().good\n",
        "kstable = kstable.reset_index()\n",
        "kstable['bad_rate'] = kstable.bads / (kstable.bads + kstable.goods)\n",
        "kstable['ks'] = (kstable.bads / kstable.bads.sum()).cumsum() - (kstable.goods / kstable.goods.sum()).cumsum()\n",
        "ks_value = kstable.ks.abs().max()\n",
        "\n",
        "print(f'KS Statistic: {ks_value:.3f}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "importances = model.named_steps['dt'].feature_importances_\n",
        "importance_df = pd.DataFrame(list(zip(feature_cols, importances)), columns=['Feature', 'Importance']).sort_values('Importance', ascending=False)\n",
        "print(importance_df.head(10))\n"
      ],
      "metadata": {
        "id": "RNQSKZRxjYZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 0\n",
        "\n",
        "# Convert the Spark DataFrame to a Pandas DataFrame\n",
        "data_pd = df_2.toPandas()\n",
        "\n",
        "# Define the feature columns\n",
        "feature_cols = [col for col in data_pd.columns if col != 'Malo_Dias_tot']\n",
        "\n",
        "# Extract the feature matrix and label vector\n",
        "X = data_pd[feature_cols].values\n",
        "y = data_pd['Malo_Dias_tot'].values\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Create an ADASYN object\n",
        "adasyn = ADASYN(random_state=random_state)\n",
        "\n",
        "# Train the Decision Tree model without cross-validation\n",
        "dt = DecisionTreeClassifier(random_state=random_state)\n",
        "\n",
        "# Create a pipeline to chain ADASYN and Decision Tree together\n",
        "pipeline = Pipeline([('adasyn', adasyn), ('dt', dt)])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model = pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Perform ADASYN oversampling on the test data\n",
        "X_test_resampled, y_test_resampled = adasyn.fit_resample(X_test, y_test)\n",
        "\n",
        "# Predict values for the oversampled test set\n",
        "y_pred = model.predict(X_test_resampled)\n",
        "\n",
        "# Calculate metrics\n",
        "tn, fp, fn, tp = confusion_matrix(y_test_resampled, y_pred).ravel()\n",
        "\n",
        "print(f'Confusion Matrix:\\n[[{tn} {fp}]\\n [{fn} {tp}]]')\n",
        "\n",
        "accuracy = accuracy_score(y_test_resampled, y_pred)\n",
        "precision = precision_score(y_test_resampled, y_pred)\n",
        "recall = recall_score(y_test_resampled, y_pred)\n",
        "f1_score = f1_score(y_test_resampled, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1_score}')\n",
        "\n",
        "# Calculate AUC using roc_auc_score function\n",
        "auc = roc_auc_score(y_test_resampled, y_pred)\n",
        "print(f'AUC: {auc}')\n",
        "\n",
        "# Calculate the KS statistic\n",
        "data_pd = pd.DataFrame({'Malo_Dias_tot': y_test_resampled, 'score': model.predict_proba(X_test_resampled)[:, 1]})\n",
        "data_pd['good'] = (data_pd['Malo_Dias_tot'] == 0).astype(int)\n",
        "data_pd['bad'] = (data_pd['Malo_Dias_tot'] == 1).astype(int)\n",
        "data_pd['bucket'] = (data_pd['score'].rank(pct=True) * 10).astype(int)\n",
        "grouped = data_pd.groupby('bucket', as_index=True)\n",
        "kstable = grouped.min().score.to_frame(name='min_score')\n",
        "kstable['max_score'] = grouped.max().score\n",
        "kstable['bads'] = grouped.sum().bad\n",
        "kstable['goods'] = grouped.sum().good\n",
        "kstable = kstable.reset_index()\n",
        "kstable['bad_rate'] = kstable.bads / (kstable.bads + kstable.goods)\n",
        "kstable['ks'] = (kstable.bads / kstable.bads.sum()).cumsum() - (kstable.goods / kstable.goods.sum()).cumsum()\n",
        "ks_value = kstable.ks.abs().max()\n",
        "\n",
        "print(f'KS Statistic: {ks_value:.3f}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "importances = model.named_steps['dt'].feature_importances_\n",
        "importance_df = pd.DataFrame(list(zip(feature_cols, importances)), columns=['Feature', 'Importance']).sort_values('Importance', ascending=False)\n",
        "print(importance_df.head(10))\n"
      ],
      "metadata": {
        "id": "Hgt1C2y_7lH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 0\n",
        "\n",
        "# Convert the Spark DataFrame to a Pandas DataFrame\n",
        "data_pd = df_2.toPandas()\n",
        "\n",
        "# Define the feature columns\n",
        "feature_cols = [col for col in data_pd.columns if col != 'Malo_Dias_tot']\n",
        "\n",
        "# Extract the feature matrix and label vector\n",
        "X = data_pd[feature_cols].values\n",
        "y = data_pd['Malo_Dias_tot'].values\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Create a Tomek Links object\n",
        "tl = TomekLinks()\n",
        "\n",
        "# Train the Decision Tree model without cross-validation\n",
        "dt = DecisionTreeClassifier(random_state=random_state)\n",
        "\n",
        "# Create a pipeline to chain Tomek Links and Decision Tree together\n",
        "pipeline = Pipeline([('tl', tl), ('dt', dt)])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model = pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Perform Tomek Links undersampling on the test data\n",
        "X_test_resampled, y_test_resampled = tl.fit_resample(X_test, y_test)\n",
        "\n",
        "# Predict values for the undersampled test set\n",
        "y_pred = model.predict(X_test_resampled)\n",
        "\n",
        "# Calculate metrics\n",
        "tn, fp, fn, tp = confusion_matrix(y_test_resampled, y_pred).ravel()\n",
        "\n",
        "print(f'Confusion Matrix:\\n[[{tn} {fp}]\\n [{fn} {tp}]]')\n",
        "\n",
        "accuracy = accuracy_score(y_test_resampled, y_pred)\n",
        "precision = precision_score(y_test_resampled, y_pred)\n",
        "recall = recall_score(y_test_resampled, y_pred)\n",
        "f1_score = f1_score(y_test_resampled, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1_score}')\n",
        "\n",
        "# Calculate AUC using roc_auc_score function\n",
        "auc = roc_auc_score(y_test_resampled, y_pred)\n",
        "print(f'AUC: {auc}')\n",
        "\n",
        "# Calculate the KS statistic\n",
        "data_pd = pd.DataFrame({'Malo_Dias_tot': y_test_resampled, 'score': model.predict_proba(X_test_resampled)[:, 1]})\n",
        "data_pd['good'] = (data_pd['Malo_Dias_tot'] == 0).astype(int)\n",
        "data_pd['bad'] = (data_pd['Malo_Dias_tot'] == 1).astype(int)\n",
        "data_pd['bucket'] = (data_pd['score'].rank(pct=True) * 10).astype(int)\n",
        "grouped = data_pd.groupby('bucket', as_index=True)\n",
        "kstable = grouped.min().score.to_frame(name='min_score')\n",
        "kstable['max_score'] = grouped.max().score\n",
        "kstable['bads'] = grouped.sum().bad\n",
        "kstable['goods'] = grouped.sum().good\n",
        "kstable = kstable.reset_index()\n",
        "kstable['bad_rate'] = kstable.bads / (kstable.bads + kstable.goods)\n",
        "kstable['ks'] = (kstable.bads / kstable.bads.sum()).cumsum() - (kstable.goods / kstable.goods.sum()).cumsum()\n",
        "ks_value = kstable.ks.abs().max()\n",
        "\n",
        "print(f'KS Statistic: {ks_value:.3f}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "importances = model.named_steps['dt'].feature_importances_\n",
        "importance_df = pd.DataFrame(list(zip(feature_cols, importances)), columns=['Feature', 'Importance']).sort_values('Importance', ascending=False)\n",
        "print(importance_df.head(10))\n"
      ],
      "metadata": {
        "id": "BwxKml3M-hdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, recall_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.initializers import glorot_uniform\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 0\n",
        "\n",
        "df_red = df_2.toPandas()\n",
        "columnas = df_red.columns.tolist()\n",
        "columnas.remove('Malo_Dias_tot')\n",
        "columnas.append('Malo_Dias_tot')\n",
        "df_red = df_red.reindex(columns=columnas)\n",
        "\n",
        "X = df_red.iloc[:,0:27].values\n",
        "y = df_red.iloc[:, 27].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "sc_X = StandardScaler()\n",
        "\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(30, input_shape=(X_train.shape[1],), kernel_initializer=glorot_uniform(seed=random_state), bias_initializer=glorot_uniform(seed=random_state)))\n",
        "model.add(Dense(1, activation='sigmoid', kernel_initializer=glorot_uniform(seed=random_state), bias_initializer=glorot_uniform(seed=random_state)))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "class_weight = {0: 0.5, 1: 2.5}\n",
        "\n",
        "model.fit(X_train, y_train, epochs=20,class_weight=class_weight)\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test,y_test)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(int)\n",
        "\n",
        "f1 = f1_score(y_test, y_pred_classes)\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "confusion = confusion_matrix(y_test, y_pred_classes)\n",
        "recall = recall_score(y_test, y_pred_classes)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "sensitivities = np.zeros(X_test.shape[1])\n",
        "for i in range(X_test.shape[1]):\n",
        "    X_test_perturbed = X_test.copy()\n",
        "    X_test_perturbed[:, i] += np.std(X_test[:, i])\n",
        "    y_pred_perturbed = model.predict(X_test_perturbed)\n",
        "    sensitivities[i] = np.mean(np.abs(y_pred_perturbed - y_pred))\n",
        "sorted_idx = np.argsort(sensitivities)[::-1]\n",
        "\n",
        "# Imprimir el ranking de características\n",
        "print(\"Ranking de características:\")\n",
        "for i in sorted_idx:\n",
        "    print(f\"{i}. Característica {i} ({sensitivities[i]:.3f})\")\n",
        "\n",
        "# Calculate the KS statistic\n",
        "data_pd = pd.DataFrame({'Malo_Dias_tot': y_test,\n",
        "                        'score': model.predict(X_test).ravel()})\n",
        "data_pd['good'] = (data_pd['Malo_Dias_tot'] == 0).astype(int)\n",
        "data_pd['bad'] = (data_pd['Malo_Dias_tot'] == 1).astype(int)\n",
        "data_pd['bucket'] = (data_pd['score'].rank(pct=True) * 10).astype(int)\n",
        "grouped = data_pd.groupby('bucket', as_index=True)\n",
        "kstable = grouped.min().score.to_frame(name='min_score')\n",
        "kstable['max_score'] = grouped.max().score\n",
        "kstable['bads'] = grouped.sum().bad\n",
        "kstable['goods'] = grouped.sum().good\n",
        "kstable = kstable.reset_index()\n",
        "kstable['bad_rate'] = kstable.bads / (kstable.bads + kstable.goods)\n",
        "kstable['ks'] = (kstable.bads / kstable.bads.sum()).cumsum() - \\\n",
        "                (kstable.goods / kstable.goods.sum()).cumsum()\n",
        "ks_value = kstable.ks.abs().max()\n",
        "\n",
        "print(f'KS Statistic: {ks_value:.3f}')\n",
        "print(f'F1 Score: {f1:.3f}')\n",
        "print(f'Accuracy: {accuracy:.3f}')\n",
        "print(f'Confusion Matrix:\\n{confusion}')\n",
        "print(f'Recall: {recall:.3f}')\n",
        "print(f'ROC-AUC: {roc_auc:.3f}')\n"
      ],
      "metadata": {
        "id": "D9zxLghq3oZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "# Establecer la semilla para el generador de números aleatorios\n",
        "random_state = 0\n",
        "\n",
        "# Dividir el conjunto de datos en conjuntos de entrenamiento y prueba\n",
        "train, test = df_2.randomSplit([0.8, 0.2], seed=random_state)\n",
        "\n",
        "# Definir las columnas de características\n",
        "feature_cols = [col for col in train.columns if col != 'Malo_Dias_tot']\n",
        "\n",
        "# Convertir el DataFrame de entrenamiento a DataFrame de Pandas\n",
        "train_pd = train.toPandas()\n",
        "\n",
        "# Aplicar SMOTE a los datos de entrenamiento\n",
        "sm = SMOTE(random_state=random_state)\n",
        "X_train, y_train = sm.fit_resample(train_pd[feature_cols], train_pd['Malo_Dias_tot'])\n",
        "\n",
        "# Convertir los datos remuestreados de nuevo a DataFrame de Spark\n",
        "train_resampled = spark.createDataFrame(pd.concat([X_train, y_train], axis=1))\n",
        "\n",
        "# Crear un VectorAssembler para combinar las columnas de características en una sola columna vectorial\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "\n",
        "# Crear un StandardScaler para estandarizar las características\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures')\n",
        "\n",
        "# Entrenar el modelo GBT con validación cruzada\n",
        "gbt = GBTClassifier(featuresCol='scaledFeatures', labelCol='Malo_Dias_tot', seed=random_state)\n",
        "\n",
        "# Crear un pipeline para encadenar el ensamblador, el escalador y GBT juntos\n",
        "pipeline = Pipeline(stages=[assembler, scaler, gbt])\n",
        "\n",
        "# Definir una cuadrícula de parámetros para la validación cruzada\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(gbt.maxDepth, [5, 10]) \\\n",
        "    .addGrid(gbt.maxIter, [10, 20]) \\\n",
        "    .build()\n",
        "\n",
        "# Crear un evaluador para la validación cruzada\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='Malo_Dias_tot', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
        "\n",
        "# Crear un validador cruzado con el pipeline, la cuadrícula de parámetros y el evaluador\n",
        "cv = CrossValidator(estimator=pipeline,\n",
        "                    estimatorParamMaps=paramGrid,\n",
        "                    evaluator=evaluator,\n",
        "                    numFolds=3,\n",
        "                    seed=random_state)\n",
        "\n",
        "# Ajustar el validador cruzado a los datos de entrenamiento remuestreados\n",
        "cvModel = cv.fit(train_resampled)\n",
        "\n",
        "# Convertir el DataFrame de prueba a DataFrame de Pandas\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Aplicar SMOTE a los datos de prueba\n",
        "X_test, y_test = sm.fit_resample(test_pd[feature_cols], test_pd['Malo_Dias_tot'])\n",
        "\n",
        "# Convertir los datos remuestreados de nuevo a DataFrame de Spark\n",
        "test_resampled = spark.createDataFrame(pd.concat([X_test, y_test], axis=1))\n",
        "\n",
        "# Predecir valores para el conjunto de prueba remuestreado con el mejor modelo encontrado por la validación cruzada\n",
        "predictions = cvModel.transform(test_resampled)\n",
        "\n",
        "# Calcular métricas\n",
        "tp = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 1)].count()\n",
        "tn = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 0)].count()\n",
        "fp = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 1)].count()\n",
        "fn = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 0)].count()\n",
        "\n",
        "print(f'Matriz de confusión:\\n[[{tn} {fp}]\\n [{fn} {tp}]]')\n",
        "\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(f'Exactitud: {accuracy}')\n",
        "print(f'Precisión: {precision}')\n",
        "print(f'Sensibilidad: {recall}')\n",
        "print(f'Puntuación F1: {f1_score}')\n",
        "\n",
        "# Calcular AUC usando BinaryClassificationEvaluator\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f'AUC: {auc}')\n",
        "\n",
        "# Definir una UDF para extraer el segundo elemento del vector de probabilidad\n",
        "probability_udf = udf(lambda v: float(v[1]), DoubleType())\n",
        "\n",
        "# Agregar una columna 'score' al DataFrame de predicciones\n",
        "predictions = predictions.withColumn('score', probability_udf('probability'))\n",
        "\n",
        "# Calcular la estadística KS\n",
        "data_pd = predictions.select('Malo_Dias_tot', 'score').toPandas()\n",
        "data_pd['good'] = (data_pd['Malo_Dias_tot'] == 0).astype(int)\n",
        "data_pd['bad'] = (data_pd['Malo_Dias_tot'] == 1).astype(int)\n",
        "data_pd['bucket'] = (data_pd['score'].rank(pct=True) * 10).astype(int)\n",
        "grouped = data_pd.groupby('bucket', as_index=True)\n",
        "kstable = grouped.min().score.to_frame(name='min_score')\n",
        "kstable['max_score'] = grouped.max().score\n",
        "kstable['bads'] = grouped.sum().bad\n",
        "kstable['goods'] = grouped.sum().good\n",
        "kstable = kstable.reset_index()\n",
        "kstable['bad_rate'] = kstable.bads / (kstable.bads + kstable.goods)\n",
        "kstable['ks'] = (kstable.bads / kstable.bads.sum()).cumsum() - (kstable.goods / kstable.goods.sum()).cumsum()\n",
        "ks_value = kstable.ks.abs().max()\n",
        "\n",
        "print(f'Estadística KS: {ks_value:.3f}')\n",
        "\n",
        "# Obtener las 10 variables más importantes del modelo\n",
        "bestModel = cvModel.bestModel\n",
        "importances = bestModel.stages[-1].featureImportances.toArray()\n",
        "importance_df = pd.DataFrame(list(zip(feature_cols, importances)), columns=['Característica', 'Importancia']).sort_values('Importancia', ascending=False)\n",
        "print(importance_df.head(10))\n"
      ],
      "metadata": {
        "id": "1tO61Jq-ky3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import TomekLinks\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "# Establecer la semilla para el generador de números aleatorios\n",
        "random_state = 0\n",
        "\n",
        "# Dividir el conjunto de datos en conjuntos de entrenamiento y prueba\n",
        "train, test = df_2.randomSplit([0.8, 0.2], seed=random_state)\n",
        "\n",
        "# Definir las columnas de características\n",
        "feature_cols = [col for col in train.columns if col != 'Malo_Dias_tot']\n",
        "\n",
        "# Convertir el DataFrame de entrenamiento a DataFrame de Pandas\n",
        "train_pd = train.toPandas()\n",
        "\n",
        "# Aplicar Tomek Links a los datos de entrenamiento\n",
        "tl = TomekLinks()\n",
        "X_train, y_train = tl.fit_resample(train_pd[feature_cols], train_pd['Malo_Dias_tot'])\n",
        "\n",
        "# Convertir los datos remuestreados de nuevo a DataFrame de Spark\n",
        "train_resampled = spark.createDataFrame(pd.concat([X_train, y_train], axis=1))\n",
        "\n",
        "# Crear un VectorAssembler para combinar las columnas de características en una sola columna vectorial\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "\n",
        "# Crear un StandardScaler para estandarizar las características\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures')\n",
        "\n",
        "# Entrenar el modelo GBT con validación cruzada\n",
        "gbt = GBTClassifier(featuresCol='scaledFeatures', labelCol='Malo_Dias_tot', seed=random_state)\n",
        "\n",
        "# Crear un pipeline para encadenar el ensamblador, el escalador y GBT juntos\n",
        "pipeline = Pipeline(stages=[assembler, scaler, gbt])\n",
        "\n",
        "# Definir una cuadrícula de parámetros para la validación cruzada\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(gbt.maxDepth, [5, 10]) \\\n",
        "    .addGrid(gbt.maxIter, [10, 20]) \\\n",
        "    .build()\n",
        "\n",
        "# Crear un evaluador para la validación cruzada\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='Malo_Dias_tot', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
        "\n",
        "# Crear un validador cruzado con el pipeline, la cuadrícula de parámetros y el evaluador\n",
        "cv = CrossValidator(estimator=pipeline,\n",
        "                    estimatorParamMaps=paramGrid,\n",
        "                    evaluator=evaluator,\n",
        "                    numFolds=3,\n",
        "                    seed=random_state)\n",
        "\n",
        "# Ajustar el validador cruzado a los datos de entrenamiento remuestreados\n",
        "cvModel = cv.fit(train_resampled)\n",
        "\n",
        "# Convertir el DataFrame de prueba a DataFrame de Pandas\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Aplicar Tomek Links a los datos de prueba\n",
        "X_test, y_test = tl.fit_resample(test_pd[feature_cols], test_pd['Malo_Dias_tot'])\n",
        "\n",
        "# Convertir los datos remuestreados de nuevo a DataFrame de Spark\n",
        "test_resampled = spark.createDataFrame(pd.concat([X_test, y_test], axis=1))\n",
        "\n",
        "# Predecir valores para el conjunto de prueba remuestreado con el mejor modelo encontrado por la validación cruzada\n",
        "predictions = cvModel.transform(test_resampled)\n",
        "\n",
        "# Calcular métricas\n",
        "tp = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 1)].count()\n",
        "tn = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 0)].count()\n",
        "fp = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 1)].count()\n",
        "fn = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 0)].count()\n",
        "\n",
        "print(f'Matriz de confusión:\\n[[{tn} {fp}]\\n [{fn} {tp}]]')\n",
        "\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(f'Exactitud: {accuracy}')\n",
        "print(f'Precisión: {precision}')\n",
        "print(f'Sensibilidad: {recall}')\n",
        "print(f'Puntuación F1: {f1_score}')\n",
        "\n",
        "# Calcular AUC usando BinaryClassificationEvaluator\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f'AUC: {auc}')\n",
        "\n",
        "# Definir una UDF para extraer el segundo elemento del vector de probabilidad\n",
        "probability_udf = udf(lambda v: float(v[1]), DoubleType())\n",
        "\n",
        "# Agregar una columna 'score' al DataFrame de predicciones\n",
        "predictions = predictions.withColumn('score', probability_udf('probability'))\n",
        "\n",
        "# Calcular la estadística KS\n",
        "data_pd = predictions.select('Malo_Dias_tot', 'score').toPandas()\n",
        "data_pd['good'] = (data_pd['Malo_Dias_tot'] == 0).astype(int)\n",
        "data_pd['bad'] = (data_pd['Malo_Dias_tot'] == 1).astype(int)\n",
        "data_pd['bucket'] = (data_pd['score'].rank(pct=True) * 10).astype(int)\n",
        "grouped = data_pd.groupby('bucket', as_index=True)\n",
        "kstable = grouped.min().score.to_frame(name='min_score')\n",
        "kstable['max_score'] = grouped.max().score\n",
        "kstable['bads'] = grouped.sum().bad\n",
        "kstable['goods'] = grouped.sum().good\n",
        "kstable = kstable.reset_index()\n",
        "kstable['bad_rate'] = kstable.bads / (kstable.bads + kstable.goods)\n",
        "kstable['ks'] = (kstable.bads / kstable.bads.sum()).cumsum() - (kstable.goods / kstable.goods.sum()).cumsum()\n",
        "ks_value = kstable.ks.abs().max()\n",
        "\n",
        "print(f'Estadística KS: {ks_value:.3f}')\n",
        "\n",
        "# Obtener las 10 variables más importantes del modelo\n",
        "bestModel = cvModel.bestModel\n",
        "importances = bestModel.stages[-1].featureImportances.toArray()\n",
        "importance_df = pd.DataFrame(list(zip(feature_cols, importances)), columns=['Característica', 'Importancia']).sort_values('Importancia', ascending=False)\n",
        "print(importance_df.head(10))\n",
        "\n"
      ],
      "metadata": {
        "id": "FxBoU8ZD-BAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "df_red = df_2.toPandas()\n",
        "columnas = df_red.columns.tolist()\n",
        "columnas.remove('Malo_Dias_tot')\n",
        "columnas.append('Malo_Dias_tot')\n",
        "df_red = df_red.reindex(columns=columnas)\n",
        "\n",
        "X = df_red.iloc[:,0:27].values\n",
        "y = df_red.iloc[:, 27].values\n",
        "\n",
        "# Set the random_state parameter to ensure reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "sc_X = StandardScaler()\n",
        "\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)\n",
        "\n",
        "# Create the model using TensorFlow instead of keras\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(30, input_shape=(X_train.shape[1],), kernel_initializer='glorot_uniform'))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "class_weight = {0: 0.5, 1: 2.5}\n",
        "\n",
        "model.fit(X_train, y_train, epochs=20,class_weight=class_weight)\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test,y_test)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(int)\n",
        "\n",
        "f1 = f1_score(y_test, y_pred_classes)\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "confusion = confusion_matrix(y_test, y_pred_classes)\n",
        "recall = recall_score(y_test, y_pred_classes)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "sensitivities = np.zeros(X_test.shape[1])\n",
        "for i in range(X_test.shape[1]):\n",
        "    X_test_perturbed = X_test.copy()\n",
        "    X_test_perturbed[:, i] += np.std(X_test[:, i])\n",
        "    y_pred_perturbed = model.predict(X_test_perturbed)\n",
        "    sensitivities[i] = np.mean(np.abs(y_pred_perturbed - y_pred))\n",
        "\n",
        "# Get the feature names from the DataFrame\n",
        "feature_names = df_red.columns[:-1]\n",
        "\n",
        "# Sort the features by their sensitivities\n",
        "sorted_idx = np.argsort(sensitivities)[::-1]\n",
        "\n",
        "# Print the top 10 most important features with their names\n",
        "print(\"Top 10 most important features:\")\n",
        "for i in range(10):\n",
        "    idx = sorted_idx[i]\n",
        "    print(f\"{i + 1}. {feature_names[idx]} ({sensitivities[idx]:.3f})\")\n",
        "\n",
        "# Calculate the KS statistic\n",
        "data_pd = pd.DataFrame({'Malo_Dias_tot': y_test,\n",
        "                        'score': model.predict(X_test).ravel()})\n",
        "data_pd['good'] = (data_pd['Malo_Dias_tot'] == 0).astype(int)\n",
        "data_pd['bad'] = (data_pd['Malo_Dias_tot'] == 1).astype(int)\n",
        "data_pd['bucket'] = (data_pd['score'].rank(pct=True) * 10).astype(int)\n",
        "grouped = data_pd.groupby('bucket', as_index=True)\n",
        "kstable = grouped.min().score.to_frame(name='min_score')\n",
        "kstable['max_score'] = grouped.max().score\n",
        "kstable['bads'] = grouped.sum().bad\n",
        "kstable['goods'] = grouped.sum().good\n",
        "kstable = kstable.reset_index()\n",
        "kstable['bad_rate'] = kstable.bads / (kstable.bads + kstable.goods)\n",
        "kstable['ks'] = (kstable.bads / kstable.bads.sum()).cumsum() - \\\n",
        "                (kstable.goods / kstable.goods.sum()).cumsum()\n",
        "ks_value = kstable.ks.abs().max()\n",
        "\n",
        "print(f'KS Statistic: {ks_value:.3f}')\n",
        "print(f'F1 Score: {f1:.3f}')\n",
        "print(f'Accuracy: {accuracy:.3f}')\n",
        "print(f'Confusion Matrix:\\n{confusion}')\n",
        "print(f'Recall: {recall:.3f}')\n",
        "print(f'ROC-AUC: {roc_auc:.3f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "mfA4BLG0vTPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#con train random forest\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Define features and label\n",
        "features = df8.columns\n",
        "features.remove('label')\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Create the Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=seed)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Define the parameter grid for cross-validation\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
        "\n",
        "# Define the evaluator for cross-validation\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create the cross-validator object\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model = cv.fit(train)\n",
        "\n",
        "# Make predictions on the training data\n",
        "predictions_train = model.transform(train)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics for training data\n",
        "evaluator_train_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc_train = evaluator_train_roc_auc.evaluate(predictions_train)\n",
        "evaluator_train_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_train = evaluator_train_accuracy.evaluate(predictions_train)\n",
        "\n",
        "print(f\"Training ROC-AUC: {roc_auc_train:.3f}\")\n",
        "print(f\"Training Accuracy: {accuracy_train:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix for training data\n",
        "predictionAndLabels_train = predictions_train.select(\"prediction\", \"label\").rdd\n",
        "metrics_train = MulticlassMetrics(predictionAndLabels_train)\n",
        "confusion_matrix_train = metrics_train.confusionMatrix().toArray()\n",
        "print(f\"Training Confusion matrix:\\n{confusion_matrix_train}\")\n",
        "\n",
        "# Manually calculate recall and F1 score for training data\n",
        "TP_train = confusion_matrix_train[1, 1]\n",
        "FP_train = confusion_matrix_train[0, 1]\n",
        "FN_train = confusion_matrix_train[1, 0]\n",
        "precision_manual_train = TP_train / (TP_train + FP_train)\n",
        "recall_manual_train = TP_train / (TP_train + FN_train)\n",
        "f1_manual_train = 2 * (precision_manual_train * recall_manual_train) / (precision_manual_train + recall_manual_train)\n",
        "print(f\"Training Recall (manually calculated): {recall_manual_train:.3f}\")\n",
        "print(f\"Training F1 (manually calculated): {f1_manual_train:.3f}\")\n",
        "\n",
        "# Calculate KS statistic for training data using custom function\n",
        "data_pd = predictions_test.select('probability', 'label').toPandas()\n",
        "data_pd['score'] = data_pd['probability'].apply(lambda x: float(x[1]))\n",
        "data_pd['good'] = (data_pd['label'] == 0).astype(int)\n",
        "data_pd['bad'] = (data_pd['label'] == 1).astype(int)\n",
        "data_pd['bucket'] = (data_pd['score'].rank(pct=True) * 10).astype(int)\n",
        "grouped = data_pd.groupby('bucket', as_index=True)\n",
        "kstable = grouped.min().score.to_frame(name='min_score')\n",
        "kstable['max_score'] = grouped.max().score\n",
        "kstable['bads'] = grouped.sum().bad\n",
        "kstable['goods'] = grouped.sum().good\n",
        "kstable.reset_index(inplace=True)\n",
        "kstable['bad_rate'] = kstable.bads / (kstable.bads + kstable.goods)\n",
        "kstable['ks'] = (kstable.bads / kstable.bads.sum()).cumsum() - \\\n",
        "                (kstable.goods / kstable.goods.sum()).cumsum()\n",
        "ks_value_test_custom_function = kstable.ks.abs().max()\n",
        "print(f\"Training KS (using custom function): {ks_value_test_custom_function:.3f}\")\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions_test = model.transform(test)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics for test data\n",
        "evaluator_test_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc_test = evaluator_test_roc_auc.evaluate(predictions_test)\n",
        "evaluator_test_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_test = evaluator_test_accuracy.evaluate(predictions_test)\n",
        "\n",
        "print(f\"Test ROC-AUC: {roc_auc_test:.3f}\")\n",
        "print(f\"Test Accuracy: {accuracy_test:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix for test data\n",
        "predictionAndLabels_test = predictions_test.select(\"prediction\", \"label\").rdd\n",
        "metrics_test = MulticlassMetrics(predictionAndLabels_test)\n",
        "confusion_matrix_test = metrics_test.confusionMatrix().toArray()\n",
        "print(f\"Test Confusion matrix:\\n{confusion_matrix_test}\")\n",
        "\n",
        "# Manually calculate recall and F1 score for test data\n",
        "TP_test = confusion_matrix_test[1, 1]\n",
        "FP_test = confusion_matrix_test[0, 1]\n",
        "FN_test = confusion_matrix_test[1, 0]\n",
        "precision_manual_test = TP_test / (TP_test + FP_test)\n",
        "recall_manual_test = TP_test / (TP_test + FN_test)\n",
        "f1_manual_test = 2 * (precision_manual_test * recall_manual_test) / (precision_manual_test + recall_manual_test)\n",
        "print(f\"Test Recall (manually calculated): {recall_manual_train:.3f}\")\n",
        "print(f\"Test F1 (manually calculated): {f1_manual_train:.3f}\")\n",
        "\n",
        "# Calculate KS statistic for test data using custom function\n",
        "data_pd = predictions_train.select('probability', 'label').toPandas()\n",
        "data_pd['score'] = data_pd['probability'].apply(lambda x: float(x[1]))\n",
        "data_pd['good'] = (data_pd['label'] == 0).astype(int)\n",
        "data_pd['bad'] = (data_pd['label'] == 1).astype(int)\n",
        "data_pd['bucket'] = (data_pd['score'].rank(pct=True) * 10).astype(int)\n",
        "grouped = data_pd.groupby('bucket', as_index=True)\n",
        "kstable = grouped.min().score.to_frame(name='min_score')\n",
        "kstable['max_score'] = grouped.max().score\n",
        "kstable['bads'] = grouped.sum().bad\n",
        "kstable['goods'] = grouped.sum().good\n",
        "kstable.reset_index(inplace=True)\n",
        "kstable['bad_rate'] = kstable.bads / (kstable.bads + kstable.goods)\n",
        "kstable['ks'] = (kstable.bads / kstable.bads.sum()).cumsum() - \\\n",
        "                (kstable.goods / kstable.goods.sum()).cumsum()\n",
        "ks_value_train_custom_function = kstable.ks.abs().max()\n",
        "print(f\"Test KS (using custom function): {ks_value_train_custom_function:.3f}\")\n",
        "\n",
        "# Get the most important features\n",
        "importances = model.bestModel.stages[-1].featureImportances\n",
        "important_features = sorted(zip(importances, features), reverse=True)\n",
        "print(\"Most important features:\")\n",
        "for importance, feature in important_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n"
      ],
      "metadata": {
        "id": "_cI1WH82zj4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from imblearn.under_sampling import NearMiss\n",
        "import pandas as pd\n",
        "\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Convert the training data to a Pandas DataFrame\n",
        "train_pd = train.toPandas()\n",
        "\n",
        "# Define features and label\n",
        "features = df8.columns\n",
        "features.remove('label')\n",
        "\n",
        "# Separate the features and label\n",
        "X = train_pd[features]\n",
        "y = train_pd['label']\n",
        "\n",
        "# Perform NearMiss undersampling\n",
        "nm = NearMiss()\n",
        "X_resampled, y_resampled = nm.fit_resample(X, y)\n",
        "\n",
        "# Convert the resampled data back to a PySpark DataFrame\n",
        "train_undersampled_pd = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "train_undersampled = spark.createDataFrame(train_undersampled_pd)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Create the Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=seed)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Define the parameter grid for cross-validation\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
        "\n",
        "# Define the evaluator for cross-validation\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create the cross-validator object\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
        "\n",
        "# Fit the model on the undersampled training data\n",
        "model = cv.fit(train_undersampled)\n",
        "\n",
        "# Make predictions on the undersampled training data\n",
        "predictions_train = model.transform(train_undersampled)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics for training data\n",
        "evaluator_train_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc_train = evaluator_train_roc_auc.evaluate(predictions_train)\n",
        "evaluator_train_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_train = evaluator_train_accuracy.evaluate(predictions_train)\n",
        "\n",
        "print(f\"Training ROC-AUC: {roc_auc_train:.3f}\")\n",
        "print(f\"Training Accuracy: {accuracy_train:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix for training data\n",
        "predictionAndLabels_train = predictions_train.select(\"prediction\", \"label\").rdd\n",
        "metrics_train = MulticlassMetrics(predictionAndLabels_train)\n",
        "confusion_matrix_train = metrics_train.confusionMatrix().toArray()\n",
        "print(f\"Training Confusion matrix:\\n{confusion_matrix_train}\")\n",
        "\n",
        "# Manually calculate recall and F1 score for training data\n",
        "TP_train = confusion_matrix_train[1, 1]\n",
        "FP_train = confusion_matrix_train[0, 1]\n",
        "FN_train = confusion_matrix_train[1, 0]\n",
        "precision_manual_train = TP_train / (TP_train + FP_train)\n",
        "recall_manual_train = TP_train / (TP_train + FN_train)\n",
        "f1_manual_train = 2 * (precision_manual_train * recall_manual_train) / (precision_manual_train + recall_manual_train)\n",
        "print(f\"Training Recall (manually calculated): {recall_manual_train:.3f}\")\n",
        "print(f\"Training F1 (manually calculated): {f1_manual_train:.3f}\")\n",
        "\n",
        "# Convert the test data to a Pandas DataFrame\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Separate the features and label in the test data\n",
        "X_test = test_pd[features]\n",
        "y_test = test_pd['label']\n",
        "\n",
        "# Perform NearMiss undersampling on the test data\n",
        "X_test_resampled, y_test_resampled = nm.fit_resample(X_test, y_test)\n",
        "\n",
        "# Convert the resampled test data back to a PySpark DataFrame\n",
        "test_undersampled_pd = pd.concat([X_test_resampled, y_test_resampled], axis=1)\n",
        "test_undersampled = spark.createDataFrame(test_undersampled_pd)\n",
        "\n",
        "# Make predictions on the undersampled test data\n",
        "predictions_test = model.transform(test_undersampled)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics for test data\n",
        "evaluator_test_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc_test = evaluator_test_roc_auc.evaluate(predictions_test)\n",
        "evaluator_test_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_test = evaluator_test_accuracy.evaluate(predictions_test)\n",
        "\n",
        "print(f\"Test ROC-AUC: {roc_auc_test:.3f}\")\n",
        "print(f\"Test Accuracy: {accuracy_test:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix for test data\n",
        "predictionAndLabels_test = predictions_test.select(\"prediction\", \"label\").rdd\n",
        "metrics_test = MulticlassMetrics(predictionAndLabels_test)\n",
        "confusion_matrix_test = metrics_test.confusionMatrix().toArray()\n",
        "print(f\"Test Confusion matrix:\\n{confusion_matrix_test}\")\n",
        "\n",
        "# Manually calculate recall and F1 score for test data\n",
        "TP_test = confusion_matrix_test[1, 1]\n",
        "FP_test = confusion_matrix_test[0, 1]\n",
        "FN_test = confusion_matrix_test[1, 0]\n",
        "precision_manual_test = TP_test / (TP_test + FP_test)\n",
        "recall_manual_test = TP_test / (TP_test + FN_test)\n",
        "f1_manual_test = 2 * (precision_manual_test * recall_manual_test) / (precision_manual_test + recall_manual_test)\n",
        "print(f\"Test Recall (manually calculated): {recall_manual_train:.3f}\")\n",
        "print(f\"Test F1 (manually calculated): {f1_manual_train:.3f}\")\n",
        "\n",
        "# Get the most important features\n",
        "importances = model.bestModel.stages[-1].featureImportances\n",
        "important_features = sorted(zip(importances, features), reverse=True)\n",
        "print(\"Most important features:\")\n",
        "for importance, feature in important_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n"
      ],
      "metadata": {
        "id": "OXLRZnWJzoZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "import pandas as pd\n",
        "\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Convert the training data to a Pandas DataFrame\n",
        "train_pd = train.toPandas()\n",
        "\n",
        "# Define features and label\n",
        "features = df8.columns\n",
        "features.remove('label')\n",
        "\n",
        "# Separate the features and label\n",
        "X = train_pd[features]\n",
        "y = train_pd['label']\n",
        "\n",
        "# Perform Tomek Links undersampling\n",
        "tl = TomekLinks()\n",
        "X_resampled, y_resampled = tl.fit_resample(X, y)\n",
        "\n",
        "# Convert the resampled data back to a PySpark DataFrame\n",
        "train_undersampled_pd = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "train_undersampled = spark.createDataFrame(train_undersampled_pd)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Create the Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=seed)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Define the parameter grid for cross-validation\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
        "\n",
        "# Define the evaluator for cross-validation\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create the cross-validator object\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
        "\n",
        "# Fit the model on the undersampled training data\n",
        "model = cv.fit(train_undersampled)\n",
        "\n",
        "# Make predictions on the undersampled training data\n",
        "predictions_train = model.transform(train_undersampled)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics for training data\n",
        "evaluator_train_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc_train = evaluator_train_roc_auc.evaluate(predictions_train)\n",
        "evaluator_train_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_train = evaluator_train_accuracy.evaluate(predictions_train)\n",
        "\n",
        "print(f\"Training ROC-AUC: {roc_auc_train:.3f}\")\n",
        "print(f\"Training Accuracy: {accuracy_train:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix for training data\n",
        "predictionAndLabels_train = predictions_train.select(\"prediction\", \"label\").rdd\n",
        "metrics_train = MulticlassMetrics(predictionAndLabels_train)\n",
        "confusion_matrix_train = metrics_train.confusionMatrix().toArray()\n",
        "print(f\"Training Confusion matrix:\\n{confusion_matrix_train}\")\n",
        "\n",
        "# Manually calculate recall and F1 score for training data\n",
        "TP_train = confusion_matrix_train[1, 1]\n",
        "FP_train = confusion_matrix_train[0, 1]\n",
        "FN_train = confusion_matrix_train[1, 0]\n",
        "precision_manual_train = TP_train / (TP_train + FP_train)\n",
        "recall_manual_train = TP_train / (TP_train + FN_train)\n",
        "f1_manual_train = 2 * (precision_manual_train * recall_manual_train) / (precision_manual_train + recall_manual_train)\n",
        "print(f\"Training Recall (manually calculated): {recall_manual_train:.3f}\")\n",
        "print(f\"Training F1 (manually calculated): {f1_manual_train:.3f}\")\n",
        "\n",
        "# Convert the test data to a Pandas DataFrame\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Separate the features and label in the test data\n",
        "X_test = test_pd[features]\n",
        "y_test = test_pd['label']\n",
        "\n",
        "# Perform Tomek Links undersampling on the test data\n",
        "X_test_resampled, y_test_resampled = tl.fit_resample(X_test, y_test)\n",
        "\n",
        "# Convert the resampled test data back to a PySpark DataFrame\n",
        "test_undersampled_pd = pd.concat([X_test_resampled, y_test_resampled], axis=1)\n",
        "test_undersampled = spark.createDataFrame(test_undersampled_pd)\n",
        "\n",
        "# Make predictions on the undersampled test data\n",
        "predictions_test = model.transform(test_undersampled)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics for test data\n",
        "evaluator_test_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc_test = evaluator_test_roc_auc.evaluate(predictions_test)\n",
        "evaluator_test_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_test = evaluator_test_accuracy.evaluate(predictions_test)\n",
        "\n",
        "print(f\"Test ROC-AUC: {roc_auc_test:.3f}\")\n",
        "print(f\"Test Accuracy: {accuracy_test:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix for test data\n",
        "predictionAndLabels_test = predictions_test.select(\"prediction\", \"label\").rdd\n",
        "metrics_test = MulticlassMetrics(predictionAndLabels_test)\n",
        "confusion_matrix_test = metrics_test.confusionMatrix().toArray()\n",
        "print(f\"Test Confusion matrix:\\n{confusion_matrix_test}\")\n",
        "\n",
        "# Manually calculate recall and F1 score for test data\n",
        "TP_test = confusion_matrix_test[1, 1]\n",
        "FP_test = confusion_matrix_test[0, 1]\n",
        "FN_test = confusion_matrix_test[1, 0]\n",
        "precision_manual_test = TP_test / (TP_test + FP_test)\n",
        "recall_manual_test = TP_test / (TP_test + FN_test)\n",
        "f1_manual_test = 2 * (precision_manual_test * recall_manual_test) / (precision_manual_test + recall_manual_test)\n",
        "print(f\"Test Recall (manually calculated): {recall_manual_train:.3f}\")\n",
        "print(f\"Test F1 (manually calculated): {f1_manual_train:.3f}\")\n",
        "\n",
        "# Get the most important features\n",
        "importances = model.bestModel.stages[-1].featureImportances\n",
        "important_features = sorted(zip(importances, features), reverse=True)\n",
        "print(\"Most important features:\")\n",
        "for importance, feature in important_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n",
        "\n",
        "def calc_ks(data):\n",
        "    data_pd=data.toPandas()\n",
        "    data_pd['good']=(data_pd['label']==0).astype(int)\n",
        "    data_pd['bad']=(data_pd['label']==1).astype(int)\n",
        "    data_pd['bucket']=(data_pd['score'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data_pd.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().score.to_frame(name='min_score')\n",
        "    kstable['max_score']=grouped.max().score\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n"
      ],
      "metadata": {
        "id": "n7pr4f6vAFGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Convert the training data to a Pandas DataFrame\n",
        "train_pd = train.toPandas()\n",
        "\n",
        "# Define features and label\n",
        "features = df8.columns\n",
        "features.remove('label')\n",
        "\n",
        "# Separate the features and label\n",
        "X = train_pd[features]\n",
        "y = train_pd['label']\n",
        "\n",
        "# Perform SMOTE oversampling\n",
        "sm = SMOTE(random_state=seed)\n",
        "X_resampled, y_resampled = sm.fit_resample(X, y)\n",
        "\n",
        "# Convert the resampled data back to a PySpark DataFrame\n",
        "train_oversampled_pd = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "train_oversampled = spark.createDataFrame(train_oversampled_pd)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Create the Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=seed)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Define the parameter grid for cross-validation\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
        "\n",
        "# Define the evaluator for cross-validation\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create the cross-validator object\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
        "\n",
        "# Fit the model on the oversampled training data\n",
        "model = cv.fit(train_oversampled)\n",
        "\n",
        "# Convert the test data to a Pandas DataFrame\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Separate the features and label in the test data\n",
        "X_test = test_pd[features]\n",
        "y_test = test_pd['label']\n",
        "\n",
        "# Perform SMOTE oversampling on the test data\n",
        "X_test_resampled, y_test_resampled = sm.fit_resample(X_test, y_test)\n",
        "\n",
        "# Convert the resampled test data back to a PySpark DataFrame\n",
        "test_oversampled_pd = pd.concat([X_test_resampled, y_test_resampled], axis=1)\n",
        "test_oversampled = spark.createDataFrame(test_oversampled_pd)\n",
        "\n",
        "# Make predictions on the oversampled test data\n",
        "predictions = model.transform(test_oversampled)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc = evaluator.evaluate(predictions)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "metrics = MulticlassMetrics(predictionAndLabels)\n",
        "confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "print(f\"Matriz de confusión:\\n{confusion_matrix}\")\n",
        "\n",
        "# Manually calculate recall and F1 score\n",
        "TP = confusion_matrix[1, 1]\n",
        "FP = confusion_matrix[0, 1]\n",
        "FN = confusion_matrix[1, 0]\n",
        "precision_manual = TP / (TP + FP)\n",
        "recall_manual = TP / (TP + FN)\n",
        "f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
        "print(f\"Recall (calculado manualmente): {recall_manual:.3f}\")\n",
        "print(f\"F1 (calculado manualmente): {f1_manual:.3f}\")\n",
        "\n",
        "# Get the most important features\n",
        "importances = model.bestModel.stages[-1].featureImportances\n",
        "important_features = sorted(zip(importances, features), reverse=True)\n",
        "print(\"Variables más importantes:\")\n",
        "for importance, feature in important_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n",
        "\n",
        "def calc_ks(data):\n",
        "    data_pd=data.toPandas()\n",
        "    data_pd['good']=(data_pd['label']==0).astype(int)\n",
        "    data_pd['bad']=(data_pd['label']==1).astype(int)\n",
        "    data_pd['bucket']=(data_pd['score'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data_pd.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().score.to_frame(name='min_score')\n",
        "    kstable['max_score']=grouped.max().score\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "score_udf=udf(lambda v:float(v[0]),DoubleType())\n",
        "predictions=predictions.withColumn('score',score_udf('probability'))\n",
        "ks_value=calc_ks(predictions)\n",
        "print(f\"Estadística KS: {ks_value:.3f}\")"
      ],
      "metadata": {
        "id": "hw8ZnK--Bevx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import pandas as pd\n",
        "\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Convert the training data to a Pandas DataFrame\n",
        "train_pd = train.toPandas()\n",
        "\n",
        "# Define features and label\n",
        "features = df8.columns\n",
        "features.remove('label')\n",
        "\n",
        "# Separate the features and label\n",
        "X = train_pd[features]\n",
        "y = train_pd['label']\n",
        "\n",
        "# Perform SMOTE oversampling\n",
        "sm = SMOTE(random_state=seed)\n",
        "X_resampled, y_resampled = sm.fit_resample(X, y)\n",
        "\n",
        "# Convert the resampled data back to a PySpark DataFrame\n",
        "train_oversampled_pd = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "train_oversampled = spark.createDataFrame(train_oversampled_pd)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Create the Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=seed)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Define the parameter grid for cross-validation\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
        "\n",
        "# Define the evaluator for cross-validation\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create the cross-validator object\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
        "\n",
        "# Fit the model on the oversampled training data\n",
        "model = cv.fit(train_oversampled)\n",
        "\n",
        "# Make predictions on the oversampled training data\n",
        "predictions_train = model.transform(train_oversampled)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics for training data\n",
        "evaluator_train_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc_train = evaluator_train_roc_auc.evaluate(predictions_train)\n",
        "evaluator_train_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_train = evaluator_train_accuracy.evaluate(predictions_train)\n",
        "\n",
        "print(f\"Training ROC-AUC: {roc_auc_train:.3f}\")\n",
        "print(f\"Training Accuracy: {accuracy_train:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix for training data\n",
        "predictionAndLabels_train = predictions_train.select(\"prediction\", \"label\").rdd\n",
        "metrics_train = MulticlassMetrics(predictionAndLabels_train)\n",
        "confusion_matrix_train = metrics_train.confusionMatrix().toArray()\n",
        "print(f\"Training Confusion matrix:\\n{confusion_matrix_train}\")\n",
        "\n",
        "# Manually calculate recall and F1 score for training data\n",
        "TP_train = confusion_matrix_train[1, 1]\n",
        "FP_train = confusion_matrix_train[0, 1]\n",
        "FN_train = confusion_matrix_train[1, 0]\n",
        "precision_manual_train = TP_train / (TP_train + FP_train)\n",
        "recall_manual_train = TP_train / (TP_train + FN_train)\n",
        "f1_manual_train = 2 * (precision_manual_train * recall_manual_train) / (precision_manual_train + recall_manual_train)\n",
        "print(f\"Training Recall (manually calculated): {recall_manual_train:.3f}\")\n",
        "print(f\"Training F1 (manually calculated): {f1_manual_train:.3f}\")\n",
        "\n",
        "# Convert the test data to a Pandas DataFrame\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Separate the features and label in the test data\n",
        "X_test = test_pd[features]\n",
        "y_test = test_pd['label']\n",
        "\n",
        "# Perform SMOTE oversampling on the test data\n",
        "X_test_resampled, y_test_resampled = sm.fit_resample(X_test, y_test)\n",
        "\n",
        "# Convert the resampled test data back to a PySpark DataFrame\n",
        "test_oversampled_pd = pd.concat([X_test_resampled, y_test_resampled], axis=1)\n",
        "test_oversampled = spark.createDataFrame(test_oversampled_pd)\n",
        "\n",
        "# Make predictions on the oversampled test data\n",
        "predictions_test = model.transform(test_oversampled)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics for test data\n",
        "evaluator_test_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc_test = evaluator_test_roc_auc.evaluate(predictions_test)\n",
        "evaluator_test_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_test = evaluator_test_accuracy.evaluate(predictions_test)\n",
        "\n",
        "print(f\"Test ROC-AUC: {roc_auc_test:.3f}\")\n",
        "print(f\"Test Accuracy: {accuracy_test:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix for test data\n",
        "predictionAndLabels_test = predictions_test.select(\"prediction\", \"label\").rdd\n",
        "metrics_test = MulticlassMetrics(predictionAndLabels_test)\n",
        "confusion_matrix_test = metrics_test.confusionMatrix().toArray()\n",
        "print(f\"Test Confusion matrix:\\n{confusion_matrix_test}\")\n",
        "\n",
        "# Manually calculate recall and F1 score for test data\n",
        "TP_test = confusion_matrix_test[1, 1]\n",
        "FP_test = confusion_matrix_test[0, 1]\n",
        "FN_test = confusion_matrix_test[1, 0]\n",
        "precision_manual_test = TP_test / (TP_test + FP_test)\n",
        "recall_manual_test = TP_test / (TP_test + FN_test)\n",
        "f1_manual_test = 2 * (precision_manual_test * recall_manual_test) / (precision_manual_test + recall_manual_test)\n",
        "print(f\"Test Recall (manually calculated): {recall_manual_train:.3f}\")\n",
        "print(f\"Test F1 (manually calculated): {f1_manual_train:.3f}\")\n",
        "\n",
        "# Get the most important features\n",
        "importances = model.bestModel.stages[-1].featureImportances\n",
        "important_features = sorted(zip(importances, features), reverse=True)\n",
        "print(\"Most important features:\")\n",
        "for importance, feature in important_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n"
      ],
      "metadata": {
        "id": "CTojiEvKHC-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from imblearn.over_sampling import ADASYN\n",
        "import pandas as pd\n",
        "\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Convert the training data to a Pandas DataFrame\n",
        "train_pd = train.toPandas()\n",
        "\n",
        "# Define features and label\n",
        "features = df8.columns\n",
        "features.remove('label')\n",
        "\n",
        "# Separate the features and label\n",
        "X = train_pd[features]\n",
        "y = train_pd['label']\n",
        "\n",
        "# Perform ADASYN oversampling\n",
        "adasyn = ADASYN(random_state=seed)\n",
        "X_resampled, y_resampled = adasyn.fit_resample(X, y)\n",
        "\n",
        "# Convert the resampled data back to a PySpark DataFrame\n",
        "train_oversampled_pd = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "train_oversampled = spark.createDataFrame(train_oversampled_pd)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Create the Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=seed)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Define the parameter grid for cross-validation\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
        "\n",
        "# Define the evaluator for cross-validation\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create the cross-validator object\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
        "\n",
        "# Fit the model on the oversampled training data\n",
        "model = cv.fit(train_oversampled)\n",
        "\n",
        "# Make predictions on the oversampled training data\n",
        "predictions_train = model.transform(train_oversampled)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics for training data\n",
        "evaluator_train_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc_train = evaluator_train_roc_auc.evaluate(predictions_train)\n",
        "evaluator_train_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_train = evaluator_train_accuracy.evaluate(predictions_train)\n",
        "\n",
        "print(f\"Training ROC-AUC: {roc_auc_train:.3f}\")\n",
        "print(f\"Training Accuracy: {accuracy_train:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix for training data\n",
        "predictionAndLabels_train = predictions_train.select(\"prediction\", \"label\").rdd\n",
        "metrics_train = MulticlassMetrics(predictionAndLabels_train)\n",
        "confusion_matrix_train = metrics_train.confusionMatrix().toArray()\n",
        "print(f\"Training Confusion matrix:\\n{confusion_matrix_train}\")\n",
        "\n",
        "# Manually calculate recall and F1 score for training data\n",
        "TP_train = confusion_matrix_train[1, 1]\n",
        "FP_train = confusion_matrix_train[0, 1]\n",
        "FN_train = confusion_matrix_train[1, 0]\n",
        "precision_manual_train = TP_train / (TP_train + FP_train)\n",
        "recall_manual_train = TP_train / (TP_train + FN_train)\n",
        "f1_manual_train = 2 * (precision_manual_train * recall_manual_train) / (precision_manual_train + recall_manual_train)\n",
        "print(f\"Training Recall (manually calculated): {recall_manual_train:.3f}\")\n",
        "print(f\"Training F1 (manually calculated): {f1_manual_train:.3f}\")\n",
        "\n",
        "# Convert the test data to a Pandas DataFrame\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Separate the features and label in the test data\n",
        "X_test = test_pd[features]\n",
        "y_test = test_pd['label']\n",
        "\n",
        "# Perform ADASYN oversampling on the test data\n",
        "X_test_resampled, y_test_resampled = adasyn.fit_resample(X_test, y_test)\n",
        "\n",
        "# Convert the resampled test data back to a PySpark DataFrame\n",
        "test_oversampled_pd = pd.concat([X_test_resampled, y_test_resampled], axis=1)\n",
        "test_oversampled = spark.createDataFrame(test_oversampled_pd)\n",
        "\n",
        "# Make predictions on the oversampled test data\n",
        "predictions_test = model.transform(test_oversampled)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics for test data\n",
        "evaluator_test_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc_test = evaluator_test_roc_auc.evaluate(predictions_test)\n",
        "evaluator_test_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_test = evaluator_test_accuracy.evaluate(predictions_test)\n",
        "\n",
        "print(f\"Test ROC-AUC: {roc_auc_test:.3f}\")\n",
        "print(f\"Test Accuracy: {accuracy_test:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix for test data\n",
        "predictionAndLabels_test = predictions_test.select(\"prediction\", \"label\").rdd\n",
        "metrics_test = MulticlassMetrics(predictionAndLabels_test)\n",
        "confusion_matrix_test = metrics_test.confusionMatrix().toArray()\n",
        "print(f\"Test Confusion matrix:\\n{confusion_matrix_test}\")\n",
        "\n",
        "# Manually calculate recall and F1 score for test data\n",
        "TP_test = confusion_matrix_test[1, 1]\n",
        "FP_test = confusion_matrix_test[0, 1]\n",
        "FN_test = confusion_matrix_test[1, 0]\n",
        "precision_manual_test = TP_test / (TP_test + FP_test)\n",
        "recall_manual_test = TP_test / (TP_test + FN_test)\n",
        "f1_manual_test = 2 * (precision_manual_test * recall_manual_test) / (precision_manual_test + recall_manual_test)\n",
        "print(f\"Test Recall (manually calculated): {recall_manual_train:.3f}\")\n",
        "print(f\"Test F1 (manually calculated): {f1_manual_train:.3f}\")\n",
        "\n",
        "# Get the most important features\n",
        "importances = model.bestModel.stages[-1].featureImportances\n",
        "important_features = sorted(zip(importances, features), reverse=True)\n",
        "print(\"Most important features:\")\n",
        "for importance, feature in important_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n"
      ],
      "metadata": {
        "id": "O2upkJJkISUU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
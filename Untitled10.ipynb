{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYBJrKr6JV/nf117KHQYsm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristValen/Acciones-RNR/blob/main/Untitled10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cA7MCKZGE1EO"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import VectorUDT\n",
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(\"double\"))\n",
        "\n",
        "# Establecer la semilla para reproducibilidad\n",
        "seed = 12345\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Definir características y etiqueta\n",
        "features = df8.columns\n",
        "features.remove('label')\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Crear el modelo de Random Forest\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=seed)\n",
        "\n",
        "# Crear el pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Definir la cuadrícula de parámetros para validación cruzada\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
        "\n",
        "# Definir el evaluador para validación cruzada\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
        "\n",
        "model = cv.fit(train)\n",
        "\n",
        "predictions_train = model.transform(train)\n",
        "\n",
        "evaluator_train_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc_train = evaluator_train_roc_auc.evaluate(predictions_train)\n",
        "evaluator_train_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_train = evaluator_train_accuracy.evaluate(predictions_train)\n",
        "\n",
        "print(f\"Entrenamiento ROC-AUC: {roc_auc_train:.3f}\")\n",
        "print(f\"Entrenamiento Precisión: {accuracy_train:.3f}\")\n",
        "\n",
        "# Calcular la matriz de confusión para los datos de entrenamiento\n",
        "predictionAndLabels_train = predictions_train.select(\"prediction\", \"label\").rdd\n",
        "metrics_train = MulticlassMetrics(predictionAndLabels_train)\n",
        "confusion_matrix_train = metrics_train.confusionMatrix().toArray()\n",
        "print(f\"Matriz de confusión de entrenamiento:\\n{confusion_matrix_train}\")\n",
        "\n",
        "# Calcular manualmente la recuperación y el puntaje F1 para los datos de entrenamiento\n",
        "TP_train = confusion_matrix_train[1, 1]\n",
        "FP_train = confusion_matrix_train[0, 1]\n",
        "FN_train = confusion_matrix_train[1, 0]\n",
        "precision_manual_train = TP_train / (TP_train + FP_train)\n",
        "recall_manual_train = TP_train / (TP_train + FN_train)\n",
        "f1_manual_train = 2 * precision_manual_train * recall_manual_train / (precision_manual_train + recall_manual_train)\n",
        "print(f\"Recuperación de entrenamiento (calculada manualmente): {recall_manual_train:.3f}\")\n",
        "print(f\"Puntaje F1 de entrenamiento (calculado manualmente): {f1_manual_train:.3f}\")\n",
        "\n",
        "def calc_ks(data):\n",
        "    data_pd = data.toPandas()\n",
        "    data_pd['good'] = (data_pd['label'] == 0).astype(int)\n",
        "    data_pd['bad'] = (data_pd['label'] == 1).astype(int)\n",
        "    from pyspark.ml.linalg import VectorUDT\n",
        "\n",
        "    if isinstance(data.schema[\"probability\"].dataType, VectorUDT):\n",
        "        # Si la columna \"probability\" contiene objetos DenseVector, extrae el valor relevante\n",
        "        data_pd['bucket'] = (data_pd['probability'].apply(lambda x: x[1]).rank(pct=True) * 10).astype(int)\n",
        "    else:\n",
        "        # Si la columna \"probability\" contiene valores de tipo float, aplica directamente el método rank\n",
        "        data_pd['bucket'] = (data_pd['probability'].rank(pct=True) * 10).astype(int)\n",
        "    grouped = data_pd.groupby('bucket', as_index=True)\n",
        "    kstable = grouped.min().probability.to_frame(name='min_probability')\n",
        "    kstable['max_probability'] = grouped.max().probability\n",
        "    kstable['bads'] = grouped.sum().bad\n",
        "    kstable['goods'] = grouped.sum().good\n",
        "    kstable.reset_index(inplace=True)\n",
        "    kstable['bad_rate'] = kstable.bads / (kstable.bads + kstable.goods)\n",
        "    kstable['ks'] = (kstable.bads / kstable.bads.sum()).cumsum() - \\\n",
        "                    (kstable.goods / kstable.goods.sum()).cumsum()\n",
        "    ks_value = kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "# Calcular el estadístico KS para los datos de entrenamiento\n",
        "ks_value_train = calc_ks(predictions_train)\n",
        "print(f\"Entrenamiento KS: {ks_value_train:.3f}\")\n",
        "\n",
        "# Obtener las características más importantes\n",
        "importances = model.bestModel.stages[-1].featureImportances\n",
        "important_features = sorted(zip(importances, features), reverse=True)\n",
        "print(\"Características más importantes:\")\n",
        "for importance, feature in important_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}
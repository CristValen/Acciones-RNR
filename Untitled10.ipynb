{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOX3RR8+I+C12CwT83co/y+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristValen/Acciones-RNR/blob/main/Untitled10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Define features and label\n",
        "features = df8.columns\n",
        "features.remove('label')\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Create the Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=seed)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Define the parameter grid for cross-validation\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
        "\n",
        "# Define the evaluator for cross-validation\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create the cross-validator object\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model = cv.fit(train)\n",
        "\n",
        "# Make predictions on the training data\n",
        "predictions_train = model.transform(train)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics for training data\n",
        "evaluator_train_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc_train = evaluator_train_roc_auc.evaluate(predictions_train)\n",
        "evaluator_train_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_train = evaluator_train_accuracy.evaluate(predictions_train)\n",
        "\n",
        "print(f\"Training ROC-AUC: {roc_auc_train:.3f}\")\n",
        "print(f\"Training Accuracy: {accuracy_train:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix for training data\n",
        "predictionAndLabels_train = predictions_train.select(\"prediction\", \"label\").rdd\n",
        "metrics_train = MulticlassMetrics(predictionAndLabels_train)\n",
        "confusion_matrix_train = metrics_train.confusionMatrix().toArray()\n",
        "print(f\"Training Confusion matrix:\\n{confusion_matrix_train}\")\n",
        "\n",
        "# Manually calculate recall and F1 score for training data\n",
        "TP_train = confusion_matrix_train[1, 1]\n",
        "FP_train = confusion_matrix_train[0, 1]\n",
        "FN_train = confusion_matrix_train[1, 0]\n",
        "precision_manual_train = TP_train / (TP_train + FP_train)\n",
        "recall_manual_train = TP_train / (TP_train + FN_train)\n",
        "f1_manual_train = 2 * (precision_manual_train * recall_manual_train) / (precision_manual_train + recall_manual_train)\n",
        "print(f\"Training Recall (manually calculated): {recall_manual_train:.3f}\")\n",
        "print(f\"Training F1 (manually calculated): {f1_manual_train:.3f}\")\n",
        "\n",
        "def calc_ks(data):\n",
        "    data_pd=data.toPandas()\n",
        "    data_pd['good']=(data_pd['label']==0).astype(int)\n",
        "    data_pd['bad']=(data_pd['label']==1).astype(int)\n",
        "    data_pd['bucket']=(data_pd['score'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data_pd.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().score.to_frame(name='min_score')\n",
        "    kstable['max_score']=grouped.max().score\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "# Define a user-defined function to extract the probability of class 1\n",
        "extract_probability = udf(lambda v: float(v[1]), DoubleType())\n",
        "\n",
        "# Create a new column with the probability of class 1 in the predictions DataFrame\n",
        "predictions_train = predictions_train.withColumn('score', extract_probability('probability'))\n",
        "\n",
        "# Calculate the KS statistic for the training data\n",
        "ks_value = calc_ks(predictions_train)\n",
        "print(f\"Training KS: {ks_value:.3f}\")"
      ],
      "metadata": {
        "id": "dFZABeDLL0ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
        "from pyspark.sql.functions import col, udf, when, percent_rank\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Define features and label\n",
        "features = df8.columns\n",
        "features.remove('label')\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Create the Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=seed)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Define the parameter grid for cross-validation\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
        "\n",
        "# Define the evaluator for cross-validation\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create the cross-validator object\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model = cv.fit(train)\n",
        "\n",
        "# Make predictions on the training data\n",
        "predictions_train = model.transform(train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions_test = model.transform(test)\n",
        "\n",
        "# Create a UDF to extract the score from the probability column\n",
        "def extract_score(vector):\n",
        "    return float(vector[1])\n",
        "\n",
        "extract_score_udf = udf(extract_score, DoubleType())\n",
        "\n",
        "# Create the score column in the predictions DataFrames\n",
        "predictions_train = predictions_train.withColumn('score', extract_score_udf('probability'))\n",
        "predictions_test = predictions_test.withColumn('score', extract_score_udf('probability'))\n",
        "\n",
        "# Convert the predictions to an RDD and calculate metrics using MulticlassMetrics\n",
        "predictionAndLabels_train = predictions_train.select(\"prediction\", \"label\").rdd\n",
        "metrics_train = MulticlassMetrics(predictionAndLabels_train)\n",
        "confusion_matrix_train = metrics_train.confusionMatrix().toArray()\n",
        "print(f\"Training Confusion matrix:\\n{confusion_matrix_train}\")\n",
        "\n",
        "predictionAndLabels_test = predictions_test.select(\"prediction\", \"label\").rdd\n",
        "metrics_test = MulticlassMetrics(predictionAndLabels_test)\n",
        "confusion_matrix_test = metrics_test.confusionMatrix().toArray()\n",
        "print(f\"Test Confusion matrix:\\n{confusion_matrix_test}\")\n",
        "\n",
        "# Manually calculate recall and F1 score for training data\n",
        "TP_train = confusion_matrix_train[1, 1]\n",
        "FP_train = confusion_matrix_train[0, 1]\n",
        "FN_train = confusion_matrix_train[1, 0]\n",
        "\n",
        "precision_train = TP_train / (TP_train + FP_train)\n",
        "recall_train = TP_train / (TP_train + FN_train)\n",
        "f1_score_train = 2 * (precision_train * recall_train) / (precision_train + recall_train)\n",
        "\n",
        "print(f\"Training Precision: {precision_train}\")\n",
        "print(f\"Training Recall: {recall_train}\")\n",
        "print(f\"Training F1 Score: {f1_score_train}\")\n",
        "\n",
        "# Manually calculate recall and F1 score for test data\n",
        "TP_test = confusion_matrix_test[1, 1]\n",
        "FP_test = confusion_matrix_test[0, 1]\n",
        "FN_test = confusion_matrix_test[1, 0]\n",
        "\n",
        "precision_test = TP_test / (TP_test + FP_test)\n",
        "recall_test = TP_test / (TP_test + FN_test)\n",
        "f1_score_test = 2 * (precision_test * recall_test) / (precision_test + recall_test)\n",
        "\n",
        "print(f\"Test Precision: {precision_test}\")\n",
        "print(f\"Test Recall: {recall_test}\")\n",
        "print(f\"Test F1 Score: {f1_score_test}\")\n",
        "\n",
        "def calc_ks(data):\n",
        "    data_pd=data.withColumn('good', when(col('label') == 0, 1).otherwise(0)) \\\n",
        "                .withColumn('bad', when(col('label') == 1, 1).otherwise(0)) \\\n",
        "                .withColumn('bucket', (percent_rank().over(Window.orderBy('score'))*10).cast(IntegerType()))\n",
        "    grouped=data_pd.groupBy('bucket')\n",
        "    kstable=grouped.agg(min(col('score')).alias('min_score'), max(col('score')).alias('max_score'), sum(col('bad')).alias('bads'), sum(col('good')).alias('goods'))\n",
        "    kstable=kstable.withColumn('bad_rate', col('bads')/(col('bads')+col('goods')))\n",
        "    kstable=kstable.withColumn('ks', (sum(col('bads')).over(Window.orderBy('bucket'))/kstable.select(sum(col('bads'))).collect()[0][0])-(sum(col('goods')).over(Window.orderBy('bucket'))/kstable.select(sum(col('goods'))).collect()[0][0]))\n",
        "    ks_value=kstable.select(max(abs(col('ks')))).collect()[0][0]\n",
        "    return ks_value\n",
        "\n",
        "ks_value_train=calc_ks(predictions_train)\n",
        "ks_value_test=calc_ks(predictions_test)\n",
        "\n",
        "print(f\"Training KS: {ks_value_train}\")\n",
        "print(f\"Test KS: {ks_value_test}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "f9YfrL0s59ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "from pyspark.sql import Row\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 0\n",
        "\n",
        "# Convert the Spark DataFrame to a Pandas DataFrame\n",
        "data_pd = df_2.toPandas()\n",
        "\n",
        "# Define the feature columns\n",
        "feature_cols = [col for col in data_pd.columns if col != 'Malo_Dias_tot']\n",
        "\n",
        "# Extract the feature matrix and label vector\n",
        "X = data_pd[feature_cols].values\n",
        "y = data_pd['Malo_Dias_tot'].values\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Create a SMOTE object\n",
        "smote = SMOTE(random_state=random_state)\n",
        "\n",
        "# Train the Decision Tree model without cross-validation\n",
        "dt = DecisionTreeClassifier(random_state=random_state)\n",
        "\n",
        "# Create a pipeline to chain SMOTE and Decision Tree together\n",
        "pipeline = Pipeline([('smote', smote), ('dt', dt)])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model = pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Perform SMOTE oversampling on the test data\n",
        "X_test_resampled, y_test_resampled = smote.fit_resample(X_test, y_test)\n",
        "\n",
        "# Predict values for the oversampled test set\n",
        "y_pred = model.predict(X_train_resampled)\n",
        "\n",
        "# Calculate metrics\n",
        "tn, fp, fn, tp = confusion_matrix(y_train_resampled, y_pred).ravel()\n",
        "\n",
        "print(f'Confusion Matrix:\\n[[{tn} {fp}]\\n [{fn} {tp}]]')\n",
        "\n",
        "accuracy = accuracy_score(y_train_resampled, y_pred)\n",
        "precision = precision_score(y_train_resampled, y_pred)\n",
        "recall = recall_score(y_train_resampled, y_pred)\n",
        "f1_score = f1_score(y_train_resampled, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1_score}')\n",
        "\n",
        "# Convert the true labels and predicted scores to int data type\n",
        "y_train_resampled = y_train_resampled.astype(int)\n",
        "y_pred = y_pred.astype(int)\n",
        "\n",
        "# Create a list of Row objects containing the true labels and predicted scores\n",
        "rows = [Row(label=int(label), score=int(score)) for label, score in zip(y_train_resampled, y_pred)]\n",
        "\n",
        "# Convert the list of Row objects to a PySpark DataFrame\n",
        "predictions_train = spark.createDataFrame(rows)\n",
        "\n",
        "def calc_ks(data):\n",
        "    data_pd=data.withColumn('good', when(col('label') == 0, 1).otherwise(0)) \\\n",
        "                .withColumn('bad', when(col('label') == 1, 1).otherwise(0)) \\\n",
        "                .withColumn('bucket', (percent_rank().over(Window.orderBy('score'))*10).cast(IntegerType()))\n",
        "    grouped=data_pd.groupBy('bucket')\n",
        "    kstable=grouped.agg(min(col('score')).alias('min_score'), max(col('score')).alias('max_score'), sum(col('bad')).alias('bads'), sum(col('good')).alias('goods'))\n",
        "    kstable=kstable.withColumn('bad_rate', col('bads')/(col('bads')+col('goods')))\n",
        "    kstable=kstable.withColumn('ks', (sum(col('bads')).over(Window.orderBy('bucket'))/kstable.select(sum(col('bads'))).collect()[0][0])-(sum(col('goods')).over(Window.orderBy('bucket'))/kstable.select(sum(col('goods'))).collect()[0][0]))\n",
        "    ks_value=kstable.select(max(abs(col('ks')))).collect()[0][0]\n",
        "    return ks_value\n",
        "\n",
        "ks_value_train=calc_ks(predictions_train)\n",
        "\n",
        "print(f\"Training KS: {ks_value_train}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nscpUVuClnLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### con funcion\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Define features and label\n",
        "features = df8.columns\n",
        "features.remove('label')\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Create the Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=seed)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Define the parameter grid for cross-validation\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
        "\n",
        "# Define the evaluator for cross-validation\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create the cross-validator object\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model = cv.fit(train)\n",
        "\n",
        "def calculate_metrics(predictions):\n",
        "    # Calculate ROC-AUC and accuracy metrics\n",
        "    evaluator_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "    roc_auc = evaluator_roc_auc.evaluate(predictions)\n",
        "    evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "    accuracy = evaluator_accuracy.evaluate(predictions)\n",
        "\n",
        "    # Calculate the confusion matrix\n",
        "    predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "    metrics = MulticlassMetrics(predictionAndLabels)\n",
        "    confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "\n",
        "    # Manually calculate recall and F1 score\n",
        "    TP = confusion_matrix[1, 1]\n",
        "    FP = confusion_matrix[0, 1]\n",
        "    FN = confusion_matrix[1, 0]\n",
        "    precision_manual = TP / (TP + FP)\n",
        "    recall_manual = TP / (TP + FN)\n",
        "    f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
        "\n",
        "    return roc_auc, accuracy, precision_manual, recall_manual, f1_manual, confusion_matrix\n",
        "\n",
        "# Make predictions on the training data and calculate metrics for training data\n",
        "predictions_train = model.transform(train)\n",
        "train_roc_auc, train_accuracy, train_precision, train_recall, train_f1Score, train_confusion_matrix = calculate_metrics(predictions_train)\n",
        "\n",
        "# Print training metrics\n",
        "print(f\"Training ROC-AUC: {train_roc_auc:.3f}\")\n",
        "print(f\"Training Accuracy: {train_accuracy:.3f}\")\n",
        "print(f\"Training Precision: {train_precision:.3f}\")\n",
        "print(f\"Training Recall: {train_recall:.3f}\")\n",
        "print(f\"Training F1: {train_f1Score:.3f}\")\n",
        "print(f\"Training Confusion matrix:\\n{train_confusion_matrix}\")\n",
        "\n",
        "# Make predictions on the test data and calculate metrics for test data\n",
        "predictions_test = model.transform(test)\n",
        "test_roc_auc, test_accuracy, test_precision, test_recall, test_f1Score, test_confusion_matrix = calculate_metrics(predictions_test)\n",
        "\n",
        "# Print test metrics\n",
        "print(f\"Test ROC-AUC: {test_roc_auc:.3f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
        "print(f\"Test Precision: {test_precision:.3f}\")\n",
        "print(f\"Test Recall: {test_recall:.3f}\")\n",
        "print(f\"Test F1: {test_f1Score:.3f}\")\n",
        "print(f\"Test Confusion matrix:\\n{test_confusion_matrix}\")\n",
        "\n",
        "# Make predictions on the df_oot data and calculate metrics for df_oot data\n",
        "predictions_oot = model.transform(df_oot)\n",
        "oot_roc_auc, oot_accuracy, oot_precision, oot_recall, oot_f1Score, oot_confusion_matrix = calculate_metrics(predictions_oot)\n",
        "\n",
        "# Print df_oot metrics\n",
        "print(f\"df_oot ROC-AUC: {oot_roc_auc:.3f}\")\n",
        "print(f\"df_oot Accuracy: {oot_accuracy:.3f}\")\n",
        "print(f\"df_oot Precision: {oot_precision:.3f}\")\n",
        "print(f\"df_oot Recall: {oot_recall:.3f}\")\n",
        "print(f\"df_oot F1: {oot_f1Score:.3f}\")\n",
        "print(f\"df_oot Confusion matrix:\\n{oot_confusion_matrix}\")\n"
      ],
      "metadata": {
        "id": "1LnrmAvvodnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Establecer la semilla para el generador de números aleatorios\n",
        "random_state = 0\n",
        "\n",
        "# Calcular el número de ejemplos en cada clase\n",
        "class_counts = df_2.groupBy('Malo_Dias_tot').count().collect()\n",
        "num_positives = class_counts[1][1]\n",
        "num_negatives = class_counts[0][1]\n",
        "\n",
        "# Calcular el número de ejemplos negativos a mantener\n",
        "num_to_keep = int(num_positives / num_negatives * num_negatives)\n",
        "\n",
        "# Seleccionar un subconjunto aleatorio de la clase mayoritaria\n",
        "majority_subset = df_2.filter(col('Malo_Dias_tot') == 0).orderBy(rand(seed=random_state)).limit(num_to_keep)\n",
        "\n",
        "# Combinar el subconjunto mayoritario con la clase minoritaria para crear los datos submuestreados\n",
        "undersampled_data = majority_subset.union(df_2.filter(col('Malo_Dias_tot') == 1))\n",
        "\n",
        "# Dividir el conjunto de datos en conjuntos de entrenamiento y prueba\n",
        "train, test = undersampled_data.randomSplit([0.7, 0.3], seed=random_state)\n",
        "\n",
        "# Definir las columnas de características\n",
        "feature_cols = [col for col in train.columns if col != 'Malo_Dias_tot']\n",
        "\n",
        "# Crear un VectorAssembler para combinar las columnas de características en una sola columna vectorial\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "\n",
        "# Crear un StandardScaler para estandarizar las características\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=True)\n",
        "\n",
        "# Entrenar el modelo de Máquina de Vectores de Soporte sin validación cruzada\n",
        "svm = LinearSVC(featuresCol='scaledFeatures', labelCol='Malo_Dias_tot', maxIter=10, regParam=0.1)\n",
        "\n",
        "# Crear un pipeline para encadenar el ensamblador, el escalador y el SVM juntos\n",
        "pipeline = Pipeline(stages=[assembler, scaler, svm])\n",
        "\n",
        "# Ajustar el pipeline a los datos de entrenamiento\n",
        "model = pipeline.fit(train)\n",
        "\n",
        "\n",
        "def calculate_metrics(predictions):\n",
        "    # Calculate ROC-AUC and accuracy metrics\n",
        "    evaluator_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "    roc_auc = evaluator_roc_auc.evaluate(predictions)\n",
        "    evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "    accuracy = evaluator_accuracy.evaluate(predictions)\n",
        "\n",
        "    # Calculate the confusion matrix\n",
        "    predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "    metrics = MulticlassMetrics(predictionAndLabels)\n",
        "    confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "\n",
        "    # Manually calculate recall and F1 score\n",
        "    TP = confusion_matrix[1, 1]\n",
        "    FP = confusion_matrix[0, 1]\n",
        "    FN = confusion_matrix[1, 0]\n",
        "    precision_manual = TP / (TP + FP)\n",
        "    recall_manual = TP / (TP + FN)\n",
        "    f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
        "\n",
        "    return roc_auc, accuracy, precision_manual, recall_manual, f1_manual, confusion_matrix\n",
        "\n",
        "# Make predictions on the training data and calculate metrics for training data\n",
        "predictions_train = model.transform(train)\n",
        "train_roc_auc, train_accuracy, train_precision, train_recall, train_f1Score, train_confusion_matrix = calculate_metrics(predictions_train)\n",
        "\n",
        "# Print training metrics\n",
        "print(f\"Training ROC-AUC: {train_roc_auc:.3f}\")\n",
        "print(f\"Training Accuracy: {train_accuracy:.3f}\")\n",
        "print(f\"Training Precision: {train_precision:.3f}\")\n",
        "print(f\"Training Recall: {train_recall:.3f}\")\n",
        "print(f\"Training F1: {train_f1Score:.3f}\")\n",
        "print(f\"Training Confusion matrix:\\n{train_confusion_matrix}\")\n",
        "\n",
        "# Make predictions on the test data and calculate metrics for test data\n",
        "predictions_test = model.transform(test)\n",
        "test_roc_auc, test_accuracy, test_precision, test_recall, test_f1Score, test_confusion_matrix = calculate_metrics(predictions_test)\n",
        "\n",
        "# Print test metrics\n",
        "print(f\"Test ROC-AUC: {test_roc_auc:.3f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
        "print(f\"Test Precision: {test_precision:.3f}\")\n",
        "print(f\"Test Recall: {test_recall:.3f}\")\n",
        "print(f\"Test F1: {test_f1Score:.3f}\")\n",
        "print(f\"Test Confusion matrix:\\n{test_confusion_matrix}\")\n",
        "\n",
        "# Make predictions on the df_oot data and calculate metrics for df_oot data\n",
        "predictions_oot = model.transform(df_oot)\n",
        "oot_roc_auc, oot_accuracy, oot_precision, oot_recall, oot_f1Score, oot_confusion_matrix = calculate_metrics(predictions_oot)\n",
        "\n",
        "# Print df_oot metrics\n",
        "print(f\"df_oot ROC-AUC: {oot_roc_auc:.3f}\")\n",
        "print(f\"df_oot Accuracy: {oot_accuracy:.3f}\")\n",
        "print(f\"df_oot Precision: {oot_precision:.3f}\")\n",
        "print(f\"df_oot Recall: {oot_recall:.3f}\")\n",
        "print(f\"df_oot F1: {oot_f1Score:.3f}\")\n",
        "print(f\"df_oot Confusion matrix:\\n{oot_confusion_matrix}\")\n",
        "\n",
        "#no reproducible"
      ],
      "metadata": {
        "id": "NTin1c3Q5xZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#intentar que sea reproducible\n",
        "random_state = 0\n",
        "\n",
        "# Calcular el número de ejemplos en cada clase\n",
        "class_counts = df_2.groupBy('Malo_Dias_tot').count().collect()\n",
        "num_positives = class_counts[1][1]\n",
        "num_negatives = class_counts[0][1]\n",
        "\n",
        "# Calcular el número de ejemplos negativos a mantener\n",
        "num_to_keep = int(num_positives / num_negatives * num_negatives)\n",
        "\n",
        "# Seleccionar un subconjunto aleatorio de la clase mayoritaria\n",
        "majority_subset = df_2.filter(col('Malo_Dias_tot') == 0).orderBy(rand(seed=random_state)).limit(num_to_keep)\n",
        "\n",
        "# Combinar el subconjunto mayoritario con la clase minoritaria para crear los datos submuestreados\n",
        "undersampled_data = majority_subset.union(df_2.filter(col('Malo_Dias_tot') == 1))\n",
        "\n",
        "# Dividir el conjunto de datos en conjuntos de entrenamiento y prueba\n",
        "train, test = undersampled_data.randomSplit([0.7, 0.3], seed=random_state)\n",
        "\n",
        "# Definir las columnas de características\n",
        "feature_cols = [col for col in train.columns if col != 'Malo_Dias_tot']\n",
        "\n",
        "# Crear un VectorAssembler para combinar las columnas de características en una sola columna vectorial\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "\n",
        "# Crear un StandardScaler para estandarizar las características\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=True)\n",
        "\n",
        "# Entrenar el modelo de Máquina de Vectores de Soporte sin validación cruzada\n",
        "svm = LinearSVC(featuresCol='scaledFeatures', labelCol='Malo_Dias_tot', maxIter=10, regParam=0.1)\n",
        "\n",
        "# Crear un pipeline para encadenar el ensamblador, el escalador y el SVM juntos\n",
        "pipeline = Pipeline(stages=[assembler, scaler, svm])\n",
        "\n",
        "# Ajustar el pipeline a los datos de entrenamiento\n",
        "model = pipeline.fit(train)\n",
        "\n",
        "\n",
        "def calculate_metrics(predictions):\n",
        "    # Calculate ROC-AUC and accuracy metrics\n",
        "    evaluator_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "    roc_auc = evaluator_roc_auc.evaluate(predictions)\n",
        "    evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "    accuracy = evaluator_accuracy.evaluate(predictions)\n",
        "\n",
        "    # Calculate the confusion matrix\n",
        "    predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "    metrics = MulticlassMetrics(predictionAndLabels)\n",
        "    confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "\n",
        "    # Manually calculate recall and F1 score\n",
        "    TP = confusion_matrix[1, 1]\n",
        "    FP = confusion_matrix[0, 1]\n",
        "    FN = confusion_matrix[1, 0]\n",
        "    precision_manual = TP / (TP + FP)\n",
        "    recall_manual = TP / (TP + FN)\n",
        "    f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
        "\n",
        "    return roc_auc, accuracy, precision_manual, recall_manual, f1_manual, confusion_matrix\n",
        "\n",
        "# Make predictions on the training data and calculate metrics for training data\n",
        "predictions_train = model.transform(train)\n",
        "train_roc_auc, train_accuracy, train_precision, train_recall, train_f1Score, train_confusion_matrix = calculate_metrics(predictions_train)\n",
        "\n",
        "# Print training metrics\n",
        "print(f\"Training ROC-AUC: {train_roc_auc:.3f}\")\n",
        "print(f\"Training Accuracy: {train_accuracy:.3f}\")\n",
        "print(f\"Training Precision: {train_precision:.3f}\")\n",
        "print(f\"Training Recall: {train_recall:.3f}\")\n",
        "print(f\"Training F1: {train_f1Score:.3f}\")\n",
        "print(f\"Training Confusion matrix:\\n{train_confusion_matrix}\")\n",
        "\n",
        "# Make predictions on the test data and calculate metrics for test data\n",
        "predictions_test = model.transform(test)\n",
        "test_roc_auc, test_accuracy, test_precision, test_recall, test_f1Score, test_confusion_matrix = calculate_metrics(predictions_test)\n",
        "\n",
        "# Print test metrics\n",
        "print(f\"Test ROC-AUC: {test_roc_auc:.3f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
        "print(f\"Test Precision: {test_precision:.3f}\")\n",
        "print(f\"Test Recall: {test_recall:.3f}\")\n",
        "print(f\"Test F1: {test_f1Score:.3f}\")\n",
        "print(f\"Test Confusion matrix:\\n{test_confusion_matrix}\")\n",
        "\n",
        "# Make predictions on the df_oot data and calculate metrics for df_oot data\n",
        "predictions_oot = model.transform(df_oot)\n",
        "oot_roc_auc, oot_accuracy, oot_precision, oot_recall, oot_f1Score, oot_confusion_matrix = calculate_metrics(predictions_oot)\n",
        "\n",
        "# Print df_oot metrics\n",
        "print(f\"df_oot ROC-AUC: {oot_roc_auc:.3f}\")\n",
        "print(f\"df_oot Accuracy: {oot_accuracy:.3f}\")\n",
        "print(f\"df_oot Precision: {oot_precision:.3f}\")\n",
        "print(f\"df_oot Recall: {oot_recall:.3f}\")\n",
        "print(f\"df_oot F1: {oot_f1Score:.3f}\")\n",
        "print(f\"df_oot Confusion matrix:\\n{oot_confusion_matrix}\")\n"
      ],
      "metadata": {
        "id": "qhFGWQDf_0rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "# Rename the label column and cast it to DoubleType\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Define the feature columns\n",
        "feature_cols = [col for col in df8.columns if col != 'label']\n",
        "\n",
        "# Extract the feature matrix and label vector\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "train = assembler.transform(train)\n",
        "test = assembler.transform(test)\n",
        "\n",
        "# Convert the train and test DataFrames to Pandas DataFrames\n",
        "train_pd = train.toPandas()\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Create a SMOTE object\n",
        "smote = SMOTE(random_state=seed)\n",
        "\n",
        "# Perform SMOTE oversampling on the train and test data\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(train_pd[feature_cols], train_pd['label'])\n",
        "X_test_resampled, y_test_resampled = smote.fit_resample(test_pd[feature_cols], test_pd['label'])\n",
        "\n",
        "# Convert the resampled train and test data back to Spark DataFrames\n",
        "train_resampled_pd = pd.concat([pd.DataFrame(X_train_resampled, columns=feature_cols), pd.Series(y_train_resampled, name='label')], axis=1)\n",
        "train_resampled = spark.createDataFrame(train_resampled_pd)\n",
        "train_resampled = assembler.transform(train_resampled)\n",
        "\n",
        "test_resampled_pd = pd.concat([pd.DataFrame(X_test_resampled, columns=feature_cols), pd.Series(y_test_resampled, name='label')], axis=1)\n",
        "test_resampled = spark.createDataFrame(test_resampled_pd)\n",
        "test_resampled = assembler.transform(test_resampled)\n",
        "\n",
        "# Train the Decision Tree model without cross-validation\n",
        "dt = DecisionTreeClassifier(seed=seed)\n",
        "\n",
        "# Fit the model to the resampled training data\n",
        "model = dt.fit(train_resampled)\n",
        "\n",
        "def calculate_metrics(predictions):\n",
        "    # Calculate ROC-AUC and accuracy metrics\n",
        "    evaluator_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "    roc_auc = evaluator_roc_auc.evaluate(predictions)\n",
        "    evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "    accuracy = evaluator_accuracy.evaluate(predictions)\n",
        "\n",
        "    # Calculate the confusion matrix\n",
        "    predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "    metrics = MulticlassMetrics(predictionAndLabels)\n",
        "    confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "\n",
        "    # Manually calculate recall and F1 score\n",
        "    TP = confusion_matrix[1, 1]\n",
        "    FP = confusion_matrix[0, 1]\n",
        "    FN = confusion_matrix[1, 0]\n",
        "    precision_manual = TP / (TP + FP)\n",
        "    recall_manual = TP / (TP + FN)\n",
        "    f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
        "\n",
        "    return roc_auc, accuracy, precision_manual, recall_manual, f1_manual, confusion_matrix\n",
        "\n",
        "# Make predictions on the resampled training data and calculate metrics for training data\n",
        "predictions_train = model.transform(train_resampled)\n",
        "train_roc_auc, train_accuracy, train_precision, train_recall, train_f1Score, train_confusion_matrix = calculate_metrics(predictions_train)\n",
        "\n",
        "# Print training metrics\n",
        "print(f\"Training ROC-AUC: {train_roc_auc:.3f}\")\n",
        "print(f\"Training Accuracy: {train_accuracy:.3f}\")\n",
        "print(f\"Training Precision: {train_precision:.3f}\")\n",
        "print(f\"Training Recall: {train_recall:.3f}\")\n",
        "print(f\"Training F1: {train_f1Score:.3f}\")\n",
        "print(f\"Training Confusion matrix:\\n{train_confusion_matrix}\")\n",
        "\n",
        "# Make predictions on the resampled test data and calculate metrics for test data\n",
        "predictions_test = model.transform(test_resampled)\n",
        "test_roc_auc, test_accuracy, test_precision, test_recall, test_f1Score, test_confusion_matrix = calculate_metrics(predictions_test)\n",
        "\n",
        "# Print test metrics\n",
        "print(f\"Test ROC-AUC: {test_roc_auc:.3f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
        "print(f\"Test Precision: {test_precision:.3f}\")\n",
        "print(f\"Test Recall: {test_recall:.3f}\")\n",
        "print(f\"Test F1: {test_f1Score:.3f}\")\n",
        "print(f\"Test Confusion matrix:\\n{test_confusion_matrix}\")\n",
        "\n",
        "# Extract the feature matrix and label vector for df_oot2\n",
        "df_oot2 = assembler.transform(df_oot2)\n",
        "\n",
        "# Make predictions on the df_oot2 data and calculate metrics for df_oot2 data\n",
        "predictions_oot2 = model.transform(df_oot2)\n",
        "oot2_roc_auc, oot2_accuracy, oot2_precision, oot2_recall, oot2_f1Score, oot2_confusion_matrix = calculate_metrics(predictions_oot2)\n",
        "\n",
        "# Print df_oot2 metrics\n",
        "print(f\"df_oot2 ROC-AUC: {oot2_roc_auc:.3f}\")\n",
        "print(f\"df_oot2 Accuracy: {oot2_accuracy:.3f}\")\n",
        "print(f\"df_oot2 Precision: {oot2_precision:.3f}\")\n",
        "print(f\"df_oot2 Recall: {oot2_recall:.3f}\")\n",
        "print(f\"df_oot2 F1: {oot2_f1Score:.3f}\")\n",
        "print(f\"df_oot2 Confusion matrix:\\n{oot2_confusion_matrix}\")\n",
        "\n",
        "# Extract the feature importances from the trained model\n",
        "importances = model.featureImportances\n",
        "\n",
        "# Create a dictionary of feature names and their importances\n",
        "importance_dict = dict(zip(feature_cols, importances))\n",
        "\n",
        "# Sort the dictionary by importance in descending order\n",
        "sorted_importance_dict = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Extract the top 10 most important features and their importance values\n",
        "top_10_features = sorted_importance_dict[:10]\n",
        "\n",
        "# Print the top 10 most important features and their importance values\n",
        "print(\"Top 10 most important features:\")\n",
        "for feature, importance in top_10_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-OcxNxE7dEQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "# Rename the label column and cast it to DoubleType\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Define the feature columns\n",
        "feature_cols = [col for col in df8.columns if col != 'label']\n",
        "\n",
        "# Extract the feature matrix and label vector\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "train = assembler.transform(train)\n",
        "test = assembler.transform(test)\n",
        "\n",
        "# Convert the train and test DataFrames to Pandas DataFrames\n",
        "train_pd = train.toPandas()\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Create an ADASYN object\n",
        "adasyn = ADASYN(random_state=seed)\n",
        "\n",
        "# Perform ADASYN oversampling on the train and test data\n",
        "X_train_resampled, y_train_resampled = adasyn.fit_resample(train_pd[feature_cols], train_pd['label'])\n",
        "X_test_resampled, y_test_resampled = adasyn.fit_resample(test_pd[feature_cols], test_pd['label'])\n",
        "\n",
        "# Convert the resampled train and test data back to Spark DataFrames\n",
        "train_resampled_pd = pd.concat([pd.DataFrame(X_train_resampled, columns=feature_cols), pd.Series(y_train_resampled, name='label')], axis=1)\n",
        "train_resampled = spark.createDataFrame(train_resampled_pd)\n",
        "train_resampled = assembler.transform(train_resampled)\n",
        "\n",
        "test_resampled_pd = pd.concat([pd.DataFrame(X_test_resampled, columns=feature_cols), pd.Series(y_test_resampled, name='label')], axis=1)\n",
        "test_resampled = spark.createDataFrame(test_resampled_pd)\n",
        "test_resampled = assembler.transform(test_resampled)\n",
        "\n",
        "# Train the Decision Tree model without cross-validation\n",
        "dt = DecisionTreeClassifier(seed=seed)\n",
        "\n",
        "# Fit the model to the resampled training data\n",
        "model = dt.fit(train_resampled)\n",
        "\n",
        "def calculate_metrics(predictions):\n",
        "    # Calculate ROC-AUC and accuracy metrics\n",
        "    evaluator_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "    roc_auc = evaluator_roc_auc.evaluate(predictions)\n",
        "    evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "    accuracy = evaluator_accuracy.evaluate(predictions)\n",
        "\n",
        "    # Calculate the confusion matrix\n",
        "    predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "    metrics = MulticlassMetrics(predictionAndLabels)\n",
        "    confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "\n",
        "    # Manually calculate recall and F1 score\n",
        "    TP = confusion_matrix[1, 1]\n",
        "    FP = confusion_matrix[0, 1]\n",
        "    FN = confusion_matrix[1, 0]\n",
        "    precision_manual = TP / (TP + FP)\n",
        "    recall_manual = TP / (TP + FN)\n",
        "    f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
        "\n",
        "    return roc_auc, accuracy, precision_manual, recall_manual, f1_manual, confusion_matrix\n",
        "\n",
        "# Make predictions on the resampled training data and calculate metrics for training data\n",
        "predictions_train = model.transform(train_resampled)\n",
        "train_roc_auc, train_accuracy, train_precision, train_recall, train_f1Score, train_confusion_matrix = calculate_metrics(predictions_train)\n",
        "\n",
        "# Print training metrics\n",
        "print(f\"Training ROC-AUC: {train_roc_auc:.3f}\")\n",
        "print(f\"Training Accuracy: {train_accuracy:.3f}\")\n",
        "print(f\"Training Precision: {train_precision:.3f}\")\n",
        "print(f\"Training Recall: {train_recall:.3f}\")\n",
        "print(f\"Training F1: {train_f1Score:.3f}\")\n",
        "print(f\"Training Confusion matrix:\\n{train_confusion_matrix}\")\n",
        "\n",
        "# Make predictions on the resampled test data and calculate metrics for test data\n",
        "predictions_test = model.transform(test_resampled)\n",
        "test_roc_auc, test_accuracy, test_precision, test_recall, test_f1Score, test_confusion_matrix = calculate_metrics(predictions_test)\n",
        "\n",
        "# Print test metrics\n",
        "print(f\"Test ROC-AUC: {test_roc_auc:.3f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
        "print(f\"Test Precision: {test_precision:.3f}\")\n",
        "print(f\"Test Recall: {test_recall:.3f}\")\n",
        "print(f\"Test F1: {test_f1Score:.3f}\")\n",
        "print(f\"Test Confusion matrix:\\n{test_confusion_matrix}\")\n",
        "\n",
        "# Extract the feature matrix and label vector for df_oot2\n",
        "df_oot2 = assembler.transform(df_oot2)\n",
        "\n",
        "# Make predictions on the df_oot2 data and calculate metrics for df_oot2 data\n",
        "predictions_oot2 = model.transform(df_oot2)\n",
        "oot2_roc_auc, oot2_accuracy, oot2_precision, oot2_recall, oot2_f1Score, oot2_confusion_matrix = calculate_metrics(predictions_oot2)\n",
        "\n",
        "# Print df_oot2 metrics\n",
        "print(f\"df_oot2 ROC-AUC: {oot2_roc_auc:.3f}\")\n",
        "print(f\"df_oot2 Accuracy: {oot2_accuracy:.3f}\")\n",
        "print(f\"df_oot2 Precision: {oot2_precision:.3f}\")\n",
        "print(f\"df_oot2 Recall: {oot2_recall:.3f}\")\n",
        "print(f\"df_oot2 F1: {oot2_f1Score:.3f}\")\n",
        "print(f\"df_oot2 Confusion matrix:\\n{oot2_confusion_matrix}\")\n",
        "\n",
        "# Extract the feature importances from the trained model\n",
        "importances = model.featureImportances\n",
        "\n",
        "# Create a dictionary of feature names and their importances\n",
        "importance_dict = dict(zip(feature_cols, importances))\n",
        "\n",
        "# Sort the dictionary by importance in descending order\n",
        "sorted_importance_dict = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Extract the top 10 most important features and their importance values\n",
        "top_10_features = sorted_importance_dict[:10]\n",
        "\n",
        "# Print the top 10 most important features and their importance values\n",
        "print(\"Top 10 most important features:\")\n",
        "for feature, importance in top_10_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n"
      ],
      "metadata": {
        "id": "m8yxrhx7DtUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import TomekLinks\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "# Rename the label column and cast it to DoubleType\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Define the feature columns\n",
        "feature_cols = [col for col in df8.columns if col != 'label']\n",
        "\n",
        "# Extract the feature matrix and label vector\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "train = assembler.transform(train)\n",
        "test = assembler.transform(test)\n",
        "\n",
        "# Convert the train and test DataFrames to Pandas DataFrames\n",
        "train_pd = train.toPandas()\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Create a TomekLinks object\n",
        "tl = TomekLinks()\n",
        "\n",
        "# Perform Tomek Links undersampling on the train and test data\n",
        "X_train_resampled, y_train_resampled = tl.fit_resample(train_pd[feature_cols], train_pd['label'])\n",
        "X_test_resampled, y_test_resampled = tl.fit_resample(test_pd[feature_cols], test_pd['label'])\n",
        "\n",
        "# Convert the resampled train and test data back to Spark DataFrames\n",
        "train_resampled_pd = pd.concat([pd.DataFrame(X_train_resampled, columns=feature_cols), pd.Series(y_train_resampled, name='label')], axis=1)\n",
        "train_resampled = spark.createDataFrame(train_resampled_pd)\n",
        "train_resampled = assembler.transform(train_resampled)\n",
        "\n",
        "test_resampled_pd = pd.concat([pd.DataFrame(X_test_resampled, columns=feature_cols), pd.Series(y_test_resampled, name='label')], axis=1)\n",
        "test_resampled = spark.createDataFrame(test_resampled_pd)\n",
        "test_resampled = assembler.transform(test_resampled)\n",
        "\n",
        "# Train the Decision Tree model without cross-validation\n",
        "dt = DecisionTreeClassifier(seed=seed)\n",
        "\n",
        "# Fit the model to the resampled training data\n",
        "model = dt.fit(train_resampled)\n",
        "\n",
        "def calculate_metrics(predictions):\n",
        "    # Calculate ROC-AUC and accuracy metrics\n",
        "    evaluator_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "    roc_auc = evaluator_roc_auc.evaluate(predictions)\n",
        "    evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "    accuracy = evaluator_accuracy.evaluate(predictions)\n",
        "\n",
        "    # Calculate the confusion matrix\n",
        "    predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "    metrics = MulticlassMetrics(predictionAndLabels)\n",
        "    confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "\n",
        "    # Manually calculate recall and F1 score\n",
        "    TP = confusion_matrix[1, 1]\n",
        "    FP = confusion_matrix[0, 1]\n",
        "    FN = confusion_matrix[1, 0]\n",
        "    precision_manual = TP / (TP + FP)\n",
        "    recall_manual = TP / (TP + FN)\n",
        "    f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
        "\n",
        "    return roc_auc, accuracy, precision_manual, recall_manual, f1_manual, confusion_matrix\n",
        "\n",
        "# Make predictions on the resampled training data and calculate metrics for training data\n",
        "predictions_train = model.transform(train_resampled)\n",
        "train_roc_auc, train_accuracy, train_precision, train_recall, train_f1Score, train_confusion_matrix = calculate_metrics(predictions_train)\n",
        "\n",
        "# Print training metrics\n",
        "print(f\"Training ROC-AUC: {train_roc_auc:.3f}\")\n",
        "print(f\"Training Accuracy: {train_accuracy:.3f}\")\n",
        "print(f\"Training Precision: {train_precision:.3f}\")\n",
        "print(f\"Training Recall: {train_recall:.3f}\")\n",
        "print(f\"Training F1: {train_f1Score:.3f}\")\n",
        "print(f\"Training Confusion matrix:\\n{train_confusion_matrix}\")\n",
        "\n",
        "# Make predictions on the resampled test data and calculate metrics for test data\n",
        "predictions_test = model.transform(test_resampled)\n",
        "test_roc_auc, test_accuracy, test_precision, test_recall, test_f1Score, test_confusion_matrix = calculate_metrics(predictions_test)\n",
        "\n",
        "# Print test metrics\n",
        "print(f\"Test ROC-AUC: {test_roc_auc:.3f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
        "print(f\"Test Precision: {test_precision:.3f}\")\n",
        "print(f\"Test Recall: {test_recall:.3f}\")\n",
        "print(f\"Test F1: {test_f1Score:.3f}\")\n",
        "print(f\"Test Confusion matrix:\\n{test_confusion_matrix}\")\n",
        "\n",
        "# Extract the feature matrix and label vector for df_oot2\n",
        "df_oot2 = assembler.transform(df_oot2)\n",
        "\n",
        "# Make predictions on the df_oot2 data and calculate metrics for df_oot2 data\n",
        "predictions_oot2 = model.transform(df_oot2)\n",
        "oot2_roc_auc, oot2_accuracy, oot2_precision, oot2_recall, oot2_f1Score, oot2_confusion_matrix = calculate_metrics(predictions_oot2)\n",
        "\n",
        "# Print df_oot2 metrics\n",
        "print(f\"df_oot2 ROC-AUC: {oot2_roc_auc:.3f}\")\n",
        "print(f\"df_oot2 Accuracy: {oot2_accuracy:.3f}\")\n",
        "print(f\"df_oot2 Precision: {oot2_precision:.3f}\")\n",
        "print(f\"df_oot2 Recall: {oot2_recall:.3f}\")\n",
        "print(f\"df_oot2 F1: {oot2_f1Score:.3f}\")\n",
        "print(f\"df_oot2 Confusion matrix:\\n{oot2_confusion_matrix}\")\n",
        "\n",
        "# Extract the feature importances from the trained model\n",
        "importances = model.featureImportances\n",
        "\n",
        "# Create a dictionary of feature names and their importances\n",
        "importance_dict = dict(zip(feature_cols, importances))\n",
        "\n",
        "# Sort the dictionary by importance in descending order\n",
        "sorted_importance_dict = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Extract the top 10 most important features and their importance values\n",
        "top_10_features = sorted_importance_dict[:10]\n",
        "\n",
        "# Print the top 10 most important features and their importance values\n",
        "print(\"Top 10 most important features:\")\n",
        "for feature, importance in top_10_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n"
      ],
      "metadata": {
        "id": "6ECRldgxFI-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#smote sin cross\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "# Rename the label column and cast it to DoubleType\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Define the feature columns\n",
        "feature_cols = [col for col in df8.columns if col != 'label']\n",
        "\n",
        "# Extract the feature matrix and label vector\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "train = assembler.transform(train)\n",
        "test = assembler.transform(test)\n",
        "\n",
        "# Convert the train and test DataFrames to Pandas DataFrames\n",
        "train_pd = train.toPandas()\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Create a SMOTE object\n",
        "smote = SMOTE(random_state=seed)\n",
        "\n",
        "# Perform SMOTE oversampling on the train and test data\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(train_pd[feature_cols], train_pd['label'])\n",
        "X_test_resampled, y_test_resampled = smote.fit_resample(test_pd[feature_cols], test_pd['label'])\n",
        "\n",
        "# Convert the resampled train and test data back to Spark DataFrames\n",
        "train_resampled_pd = pd.concat([pd.DataFrame(X_train_resampled, columns=feature_cols), pd.Series(y_train_resampled, name='label')], axis=1)\n",
        "train_resampled = spark.createDataFrame(train_resampled_pd)\n",
        "train_resampled = assembler.transform(train_resampled)\n",
        "\n",
        "test_resampled_pd = pd.concat([pd.DataFrame(X_test_resampled, columns=feature_cols), pd.Series(y_test_resampled, name='label')], axis=1)\n",
        "test_resampled = spark.createDataFrame(test_resampled_pd)\n",
        "test_resampled = assembler.transform(test_resampled)\n",
        "\n",
        "# Train the GBT model without cross-validation\n",
        "gbt = GBTClassifier(seed=seed)\n",
        "\n",
        "# Fit the model to the resampled training data\n",
        "model = gbt.fit(train_resampled)\n",
        "\n",
        "def calculate_metrics(predictions):\n",
        "    # Calculate ROC-AUC and accuracy metrics\n",
        "    evaluator_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "    roc_auc = evaluator_roc_auc.evaluate(predictions)\n",
        "    evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "    accuracy = evaluator_accuracy.evaluate(predictions)\n",
        "\n",
        "    # Calculate the confusion matrix\n",
        "    predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "    metrics = MulticlassMetrics(predictionAndLabels)\n",
        "    confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "\n",
        "    # Manually calculate recall and F1 score\n",
        "    TP = confusion_matrix[1, 1]\n",
        "    FP = confusion_matrix[0, 1]\n",
        "    FN = confusion_matrix[1, 0]\n",
        "    precision_manual = TP / (TP + FP)\n",
        "    recall_manual = TP / (TP + FN)\n",
        "    f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
        "\n",
        "    return roc_auc, accuracy, precision_manual, recall_manual, f1_manual, confusion_matrix\n",
        "\n",
        "# Make predictions on the resampled training data and calculate metrics for training data\n",
        "predictions_train = model.transform(train_resampled)\n",
        "train_roc_auc, train_accuracy, train_precision, train_recall, train_f1Score, train_confusion_matrix = calculate_metrics(predictions_train)\n",
        "\n",
        "# Print training metrics\n",
        "print(f\"Training ROC-AUC: {train_roc_auc:.3f}\")\n",
        "print(f\"Training Accuracy: {train_accuracy:.3f}\")\n",
        "print(f\"Training Precision: {train_precision:.3f}\")\n",
        "print(f\"Training Recall: {train_recall:.3f}\")\n",
        "print(f\"Training F1: {train_f1Score:.3f}\")\n",
        "print(f\"Training Confusion matrix:\\n{train_confusion_matrix}\")\n",
        "\n",
        "# Make predictions on the resampled test data and calculate metrics for test data\n",
        "predictions_test = model.transform(test_resampled)\n",
        "test_roc_auc, test_accuracy, test_precision, test_recall, test_f1Score, test_confusion_matrix = calculate_metrics(predictions_test)\n",
        "\n",
        "# Print test metrics\n",
        "print(f\"Test ROC-AUC: {test_roc_auc:.3f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
        "print(f\"Test Precision: {test_precision:.3f}\")\n",
        "print(f\"Test Recall: {test_recall:.3f}\")\n",
        "print(f\"Test F1: {test_f1Score:.3f}\")\n",
        "print(f\"Test Confusion matrix:\\n{test_confusion_matrix}\")\n",
        "\n",
        "# Extract the feature matrix and label vector for df_oot2\n",
        "df_oot2 = assembler.transform(df_oot2)\n",
        "\n",
        "# Make predictions on the df_oot2 data and calculate metrics for df_oot2 data\n",
        "predictions_oot2 = model.transform(df_oot2)\n",
        "oot2_roc_auc, oot2_accuracy, oot2_precision, oot2_recall, oot2_f1Score, oot2_confusion_matrix = calculate_metrics(predictions_oot2)\n",
        "\n",
        "# Print df_oot2 metrics\n",
        "print(f\"df_oot2 ROC-AUC: {oot2_roc_auc:.3f}\")\n",
        "print(f\"df_oot2 Accuracy: {oot2_accuracy:.3f}\")\n",
        "print(f\"df_oot2 Precision: {oot2_precision:.3f}\")\n",
        "print(f\"df_oot2 Recall: {oot2_recall:.3f}\")\n",
        "print(f\"df_oot2 F1: {oot2_f1Score:.3f}\")\n",
        "print(f\"df_oot2 Confusion matrix:\\n{oot2_confusion_matrix}\")\n",
        "\n",
        "# Extract the feature importances from the trained model\n",
        "importances = model.featureImportances\n",
        "\n",
        "# Create a dictionary of feature names and their importances\n",
        "importance_dict = dict(zip(feature_cols, importances))\n",
        "\n",
        "# Sort the dictionary by importance in descending order\n",
        "sorted_importance_dict = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Extract the top 10 most important features and their importance values\n",
        "top_10_features = sorted_importance_dict[:10]\n",
        "\n",
        "# Print the top 10 most important features and their importance values\n",
        "print(\"Top 10 most important features:\")\n",
        "for feature, importance in top_10_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n"
      ],
      "metadata": {
        "id": "YT6IYhUADtnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#smote con cross\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "# Rename the label column and cast it to DoubleType\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Define the feature columns\n",
        "feature_cols = [col for col in df8.columns if col != 'label']\n",
        "\n",
        "# Extract the feature matrix and label vector\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "train = assembler.transform(train)\n",
        "test = assembler.transform(test)\n",
        "\n",
        "# Convert the train and test DataFrames to Pandas DataFrames\n",
        "train_pd = train.toPandas()\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Create a SMOTE object\n",
        "smote = SMOTE(random_state=seed)\n",
        "\n",
        "# Perform SMOTE oversampling on the train and test data\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(train_pd[feature_cols], train_pd['label'])\n",
        "X_test_resampled, y_test_resampled = smote.fit_resample(test_pd[feature_cols], test_pd['label'])\n",
        "\n",
        "# Convert the resampled train and test data back to Spark DataFrames\n",
        "train_resampled_pd = pd.concat([pd.DataFrame(X_train_resampled, columns=feature_cols), pd.Series(y_train_resampled, name='label')], axis=1)\n",
        "train_resampled = spark.createDataFrame(train_resampled_pd)\n",
        "train_resampled = assembler.transform(train_resampled)\n",
        "\n",
        "test_resampled_pd = pd.concat([pd.DataFrame(X_test_resampled, columns=feature_cols), pd.Series(y_test_resampled, name='label')], axis=1)\n",
        "test_resampled = spark.createDataFrame(test_resampled_pd)\n",
        "test_resampled = assembler.transform(test_resampled)\n",
        "\n",
        "# Train the GBT model with cross-validation\n",
        "gbt = GBTClassifier(seed=seed)\n",
        "\n",
        "# Create a parameter grid for hyperparameter tuning\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(gbt.maxDepth, [2, 5, 10]) \\\n",
        "    .addGrid(gbt.maxIter, [10, 20]) \\\n",
        "    .build()\n",
        "\n",
        "# Create a BinaryClassificationEvaluator object for evaluating the model during cross-validation\n",
        "evaluator_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create a CrossValidator object for performing cross-validation\n",
        "cv = CrossValidator(estimator=gbt,\n",
        "                    estimatorParamMaps=paramGrid,\n",
        "                    evaluator=evaluator_roc_auc,\n",
        "                    numFolds=5,\n",
        "                    seed=seed)\n",
        "\n",
        "# Fit the model to the resampled training data using cross-validation\n",
        "model = cv.fit(train_resampled)\n",
        "\n",
        "def calculate_metrics(predictions):\n",
        "    # Calculate ROC-AUC and accuracy metrics\n",
        "    evaluator_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "    roc_auc = evaluator_roc_auc.evaluate(predictions)\n",
        "    evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "    accuracy = evaluator_accuracy.evaluate(predictions)\n",
        "\n",
        "    # Calculate the confusion matrix\n",
        "    predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "    metrics = MulticlassMetrics(predictionAndLabels)\n",
        "    confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "\n",
        "    # Manually calculate recall and F1 score\n",
        "    TP = confusion_matrix[1, 1]\n",
        "    FP = confusion_matrix[0, 1]\n",
        "    FN = confusion_matrix[1, 0]\n",
        "    precision_manual = TP / (TP + FP)\n",
        "    recall_manual = TP / (TP + FN)\n",
        "    f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
        "\n",
        "    return roc_auc, accuracy, precision_manual, recall_manual, f1_manual, confusion_matrix\n",
        "\n",
        "# Make predictions on the resampled training data and calculate metrics for training data\n",
        "predictions_train = model.transform(train_resampled)\n",
        "train_roc_auc, train_accuracy, train_precision, train_recall, train_f1Score, train_confusion_matrix = calculate_metrics(predictions_train)\n",
        "\n",
        "# Print training metrics\n",
        "print(f\"Training ROC-AUC: {train_roc_auc:.3f}\")\n",
        "print(f\"Training Accuracy: {train_accuracy:.3f}\")\n",
        "print(f\"Training Precision: {train_precision:.3f}\")\n",
        "print(f\"Training Recall: {train_recall:.3f}\")\n",
        "print(f\"Training F1: {train_f1Score:.3f}\")\n",
        "print(f\"Training Confusion matrix:\\n{train_confusion_matrix}\")\n",
        "\n",
        "# Make predictions on the resampled test data and calculate metrics for test data\n",
        "predictions_test = model.transform(test_resampled)\n",
        "test_roc_auc, test_accuracy, test_precision, test_recall, test_f1Score, test_confusion_matrix = calculate_metrics(predictions_test)\n",
        "\n",
        "# Print test metrics\n",
        "print(f\"Test ROC-AUC: {test_roc_auc:.3f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
        "print(f\"Test Precision: {test_precision:.3f}\")\n",
        "print(f\"Test Recall: {test_recall:.3f}\")\n",
        "print(f\"Test F1: {test_f1Score:.3f}\")\n",
        "print(f\"Test Confusion matrix:\\n{test_confusion_matrix}\")\n",
        "\n",
        "# Extract the feature matrix and label vector for df_oot2\n",
        "df_oot2 = assembler.transform(df_oot2)\n",
        "\n",
        "# Make predictions on the df_oot2 data and calculate metrics for df_oot2 data\n",
        "predictions_oot2 = model.transform(df_oot2)\n",
        "oot2_roc_auc, oot2_accuracy, oot2_precision, oot2_recall, oot2_f1Score, oot2_confusion_matrix = calculate_metrics(predictions_oot2)\n",
        "\n",
        "# Print df_oot2 metrics\n",
        "print(f\"df_oot2 ROC-AUC: {oot2_roc_auc:.3f}\")\n",
        "print(f\"df_oot2 Accuracy: {oot2_accuracy:.3f}\")\n",
        "print(f\"df_oot2 Precision: {oot2_precision:.3f}\")\n",
        "print(f\"df_oot2 Recall: {oot2_recall:.3f}\")\n",
        "print(f\"df_oot2 F1: {oot2_f1Score:.3f}\")\n",
        "print(f\"df_oot2 Confusion matrix:\\n{oot2_confusion_matrix}\")\n",
        "\n",
        "# Extract the feature importances from the trained model\n",
        "importances = model.bestModel.featureImportances\n",
        "\n",
        "# Create a dictionary of feature names and their importances\n",
        "importance_dict = dict(zip(feature_cols, importances))\n",
        "\n",
        "# Sort the dictionary by importance in descending order\n",
        "sorted_importance_dict = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Extract the top 10 most important features and their importance values\n",
        "top_10_features = sorted_importance_dict[:10]\n",
        "\n",
        "# Print the top 10 most important features and their importance values\n",
        "print(\"Top 10 most important features:\")\n",
        "for feature, importance in top_10_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n"
      ],
      "metadata": {
        "id": "9T2FBGPUX2P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Convertir el DataFrame de Spark a un DataFrame de Pandas\n",
        "df6 = df5.toPandas()\n",
        "\n",
        "# Calcular la matriz de correlación de Spearman\n",
        "corr_matrix = spearmanr(df6).correlation\n",
        "\n",
        "# Crear una máscara booleana para el triángulo superior de la matriz de correlación\n",
        "upper = np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool)\n",
        "\n",
        "# Aplicar la máscara a la matriz de correlación para obtener el triángulo superior\n",
        "corr_matrix_upper = corr_matrix[upper]\n",
        "\n",
        "# Encontrar todas las columnas en el triángulo superior de la matriz de correlación con un valor absoluto de correlación mayor o igual a 0.4 y no igual a 'Malo_Dias_tot'\n",
        "to_drop = [column for column in df6.columns if any(corr_matrix_upper[np.where(df6.columns == column)[0]] >= 0.4) and column != 'Malo_Dias_tot']\n",
        "\n",
        "# Eliminar estas columnas del DataFrame df4 y almacenar el DataFrame resultante en df_2\n",
        "df_2 = df4.drop(*to_drop)\n"
      ],
      "metadata": {
        "id": "ALf1PosEr7TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Convertir el DataFrame de Spark a un DataFrame de Pandas\n",
        "df6 = df5.toPandas()\n",
        "\n",
        "# Calcular la matriz de correlación de Spearman\n",
        "corr_matrix = spearmanr(df6).correlation\n",
        "\n",
        "# Crear una máscara booleana para las variables con una correlación igual o mayor a 0.4\n",
        "mask = np.abs(corr_matrix) >= 0.4\n",
        "\n",
        "# Crear un mapa de calor para visualizar la matriz de correlación\n",
        "sns.heatmap(corr_matrix, cmap='coolwarm', center=0, mask=mask, annot=True, fmt='', xticklabels=df6.columns, yticklabels=df6.columns)\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qIIHnq1o23eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "\n",
        "# Convertir el DataFrame de Spark a un DataFrame de Pandas\n",
        "df6 = df5.toPandas()\n",
        "\n",
        "# Calcular la matriz de correlación de Spearman\n",
        "corr_matrix = spearmanr(df6).correlation\n",
        "\n",
        "# Crear una máscara booleana para el triángulo superior de la matriz de correlación\n",
        "upper = np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool)\n",
        "\n",
        "# Aplicar la máscara a la matriz de correlación para obtener el triángulo superior\n",
        "corr_matrix_upper = corr_matrix[upper]\n",
        "\n",
        "# Encontrar todas las columnas en el triángulo superior de la matriz de correlación con un valor absoluto de correlación mayor o igual a 0.4 y no igual a 'Malo_Dias_tot'\n",
        "to_drop = [column for column in df6.columns if any(corr_matrix_upper[np.where(df6.columns == column)[0]] >= 0.4) and column != 'Malo_Dias_tot']\n",
        "\n",
        "# Eliminar estas columnas del DataFrame df4 y almacenar el DataFrame resultante en df_2\n",
        "df_2 = df4.drop(*to_drop)\n",
        "\n",
        "# Calcular la información mutua entre cada par de características\n",
        "mi_matrix = np.zeros((df_2.shape[1], df_2.shape[1]))\n",
        "for i in range(df_2.shape[1]):\n",
        "    for j in range(i+1, df_2.shape[1]):\n",
        "        mi_matrix[i, j] = mutual_info_regression(df_2.iloc[:, i].values.reshape(-1, 1), df_2.iloc[:, j])[0]\n",
        "        mi_matrix[j, i] = mi_matrix[i, j]\n",
        "\n",
        "\n",
        "# Crear una máscara booleana para el triángulo superior de la matriz de información mutua\n",
        "upper_mi = np.triu(np.ones(mi_matrix.shape), k=1).astype(np.bool)\n",
        "\n",
        "# Aplicar la máscara a la matriz de información mutua para obtener el triángulo superior\n",
        "mi_matrix_upper = mi_matrix[upper_mi]\n",
        "\n",
        "# Ordenar las características por su información mutua con otras características en orden descendente\n",
        "features = df_2.columns[np.argsort(mi_matrix_upper)[::-1]]\n",
        "\n",
        "# Seleccionar las características más relevantes para tu modelo\n",
        "selected_features = features[:10]\n",
        "\n"
      ],
      "metadata": {
        "id": "e3I0l7fHZROb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Si deseas mostrar las métricas de f1, recall, accuracy, auc, las matrices de confusión y las 10 variables más importantes para 3 técnicas de machine learning distintas en un informe en RMarkdown y Shiny, puedes seguir los mismos pasos que te mencioné anteriormente, pero con algunas modificaciones en el código.\n",
        "\n",
        "Aquí tienes un ejemplo de código RMarkdown que puedes utilizar como punto de partida:\n",
        "\n",
        "```rmarkdown\n",
        "---\n",
        "title: \"Informe de métricas y variables importantes\"\n",
        "output: html_document\n",
        "runtime: shiny\n",
        "---\n",
        "\n",
        "```{r setup, include=FALSE}\n",
        "knitr::opts_chunk$set(echo = TRUE)\n",
        "```\n",
        "\n",
        "## Técnica 1\n",
        "\n",
        "### Métricas\n",
        "\n",
        "```{r}\n",
        "# Definir las métricas para la técnica 1\n",
        "f1_t1 <- 0.8\n",
        "recall_t1 <- 0.9\n",
        "accuracy_t1 <- 0.95\n",
        "auc_t1 <- 0.98\n",
        "\n",
        "# Mostrar las métricas en una tabla\n",
        "data.frame(F1 = f1_t1, Recall = recall_t1, Accuracy = accuracy_t1, AUC = auc_t1) %>%\n",
        "  knitr::kable()\n",
        "```\n",
        "\n",
        "### Matriz de confusión\n",
        "\n",
        "```{r}\n",
        "# Definir la matriz de confusión para la técnica 1\n",
        "confusion_matrix_t1 <- matrix(c(100, 10, 5, 85), nrow = 2)\n",
        "\n",
        "# Mostrar la matriz de confusión en una tabla\n",
        "confusion_matrix_t1 %>%\n",
        "  knitr::kable()\n",
        "```\n",
        "\n",
        "### Variables importantes\n",
        "\n",
        "```{r}\n",
        "# Definir las 10 variables más importantes para la técnica 1\n",
        "important_variables_t1 <- c(\"var1\", \"var2\", \"var3\", \"var4\", \"var5\", \"var6\", \"var7\", \"var8\", \"var9\", \"var10\")\n",
        "\n",
        "# Mostrar las 10 variables más importantes en una tabla\n",
        "important_variables_t1 %>%\n",
        "  knitr::kable()\n",
        "```\n",
        "\n",
        "## Técnica 2\n",
        "\n",
        "### Métricas\n",
        "\n",
        "```{r}\n",
        "# Definir las métricas para la técnica 2\n",
        "f1_t2 <- 0.85\n",
        "recall_t2 <- 0.92\n",
        "accuracy_t2 <- 0.96\n",
        "auc_t2 <- 0.99\n",
        "\n",
        "# Mostrar las métricas en una tabla\n",
        "data.frame(F1 = f1_t2, Recall = recall_t2, Accuracy = accuracy_t2, AUC = auc_t2) %>%\n",
        "  knitr::kable()\n",
        "```\n",
        "\n",
        "### Matriz de confusión\n",
        "\n",
        "```{r}\n",
        "# Definir la matriz de confusión para la técnica 2\n",
        "confusion_matrix_t2 <- matrix(c(105, 5, 7, 83), nrow = 2)\n",
        "\n",
        "# Mostrar la matriz de confusión en una tabla\n",
        "confusion_matrix_t2 %>%\n",
        "  knitr::kable()\n",
        "```\n",
        "\n",
        "### Variables importantes\n",
        "\n",
        "```{r}\n",
        "# Definir las 10 variables más importantes para la técnica 2\n",
        "important_variables_t2 <- c(\"var11\", \"var12\", \"var13\", \"var14\", \"var15\", \"var16\", \"var17\", \"var18\", \"var19\", \"var20\")\n",
        "\n",
        "# Mostrar las 10 variables más importantes en una tabla\n",
        "important_variables_t2 %>%\n",
        "  knitr::kable()\n",
        "```\n",
        "\n",
        "## Técnica 3\n",
        "\n",
        "### Métricas\n",
        "\n",
        "```{r}\n",
        "# Definir las métricas para la técnica 3\n",
        "f1_t3 <- 0.82\n",
        "recall_t3 <- 0.93\n",
        "accuracy_t3 <- 0.97\n",
        "auc_t3 <- 0.95\n",
        "\n",
        "# Mostrar las métricas en una tabla\n",
        "data.frame(F1 = f1_t3, Recall = recall_t3, Accuracy = accuracy_t3, AUC = auc_t3) %>%\n",
        "  knitr::kable()\n",
        "```\n",
        "\n",
        "### Matriz de confusión\n",
        "\n",
        "```{r}\n",
        "# Definir la matriz de confusión para la técnica 3\n",
        "confusion_matrix_t3 <- matrix(c(102,8,6,84), nrow =2)\n",
        "\n",
        "# Mostrar la matriz de confusión en una tabla\n",
        "confusion_matrix_t3 %>%\n",
        "knitr::kable()\n",
        "```\n",
        "\n",
        "### Variables importantes\n",
        "\n",
        "```{r}\n",
        "# Definir las diez variables más importantes para la técnica tres.\n",
        "important_variables_3<-c(\"variable21\",\"variable22\",\"variable23\",\"variable24\",\"variable25\",\"variable26\",\"variable27\",\"variable28\",\"variable29\",\"variable30\")\n",
        "\n",
        "# Mostrar las diez variables más importantes en una tabla.\n",
        "important_variables_3 %>%\n",
        "knitr::kable()\n",
        "```\n",
        "```\n",
        "\n",
        "Este código RMarkdown crea un informe que muestra las métricas, la matriz de confusión y las 10 variables más importantes para 3 técnicas de machine learning distintas en tablas.\n",
        "\n",
        "Espero que esto te ayude. ¿Hay algo más en lo que pueda ayudarte? 😊"
      ],
      "metadata": {
        "id": "YQz8wK5Bm2qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Cargamos el conjunto de datos\n",
        "df6 = df5.toPandas()\n",
        "\n",
        "# Separamos las variables predictoras y la variable objetivo\n",
        "X = df6.drop('target', axis=1)\n",
        "y = df6['target']\n",
        "\n",
        "# Calculamos el mutual information entre las variables predictoras y la variable objetivo\n",
        "mi = mutual_info_regression(X, y)\n",
        "mi /= np.max(mi)\n",
        "\n",
        "# Seleccionamos las variables con un mutual information mayor a un umbral determinado\n",
        "threshold = 0.5\n",
        "selected_columns = X.columns[mi > threshold]\n",
        "\n",
        "# Calculamos la correlación de Spearman entre las variables seleccionadas\n",
        "corr, _ = spearmanr(X[selected_columns])\n",
        "\n",
        "# Eliminamos las variables con una correlación de Spearman igual o superior a 0.4\n",
        "corr_threshold = 0.4\n",
        "to_remove = []\n",
        "for i in range(corr.shape[0]):\n",
        "    for j in range(i+1, corr.shape[1]):\n",
        "        if abs(corr[i, j]) >= corr_threshold:\n",
        "            # Comparamos el mutual information de ambas variables y eliminamos la que tenga el menor valor\n",
        "            if mi[i] > mi[j]:\n",
        "                to_remove.append(j)\n",
        "            else:\n",
        "                to_remove.append(i)\n",
        "selected_columns = np.delete(selected_columns, to_remove)\n",
        "\n",
        "# Mostramos las variables seleccionadas\n",
        "print('Variables seleccionadas:', selected_columns)\n"
      ],
      "metadata": {
        "id": "M6-WIc_WYSsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import NearMiss\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "# Rename the target column to 'label' and cast it to DoubleType\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the data into training and test sets with a fixed random seed for reproducibility\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Convert the training data to a Pandas DataFrame\n",
        "train_pd = train.toPandas()\n",
        "\n",
        "# Define features and label\n",
        "features = df8.columns\n",
        "features.remove('label')\n",
        "\n",
        "# Separate the features and label\n",
        "X = train_pd[features]\n",
        "y = train_pd['label']\n",
        "\n",
        "# Perform NearMiss undersampling\n",
        "nm = NearMiss()\n",
        "X_resampled, y_resampled = nm.fit_resample(X, y)\n",
        "\n",
        "# Convert the resampled data back to a PySpark DataFrame\n",
        "train_undersampled_pd = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "train_undersampled = spark.createDataFrame(train_undersampled_pd)\n",
        "\n",
        "# Create a VectorAssembler to combine feature columns into a single vector column\n",
        "assembler = VectorAssembler(inputCols=features, outputCol='features')\n",
        "\n",
        "# Create a StandardScaler to standardize features\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=True)\n",
        "\n",
        "# Create a LinearSVC model\n",
        "svm = LinearSVC(featuresCol='scaledFeatures', labelCol='label', maxIter=10, regParam=0.1)\n",
        "\n",
        "# Create a pipeline with the assembler, scaler, and SVM stages\n",
        "pipeline = Pipeline(stages=[assembler, scaler, svm])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model = pipeline.fit(train_undersampled)\n",
        "\n",
        "\n",
        "def calculate_metrics(predictions):\n",
        "    # Calculate ROC-AUC and accuracy metrics\n",
        "    evaluator_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "    roc_auc = evaluator_roc_auc.evaluate(predictions)\n",
        "    evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "    accuracy = evaluator_accuracy.evaluate(predictions)\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "    metrics = MulticlassMetrics(predictionAndLabels)\n",
        "    confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "\n",
        "    # Manually calculate precision, recall, and F1 score\n",
        "    TP = confusion_matrix[1, 1]\n",
        "    FP = confusion_matrix[0, 1]\n",
        "    FN = confusion_matrix[1, 0]\n",
        "    precision_manual = TP / (TP + FP)\n",
        "    recall_manual = TP / (TP + FN)\n",
        "    f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
        "\n",
        "    return roc_auc, accuracy, precision_manual, recall_manual, f1_manual, confusion_matrix\n",
        "\n",
        "# Make predictions on training data and calculate metrics for training data\n",
        "predictions_train = model.transform(train_undersampled)\n",
        "train_roc_auc, train_accuracy, train_precision, train_recall, train_f1Score, train_confusion_matrix = calculate_metrics(predictions_train)\n",
        "\n",
        "# Print training metrics\n",
        "print(f\"Training ROC-AUC: {train_roc_auc:.3f}\")\n",
        "print(f\"Training Accuracy: {train_accuracy:.3f}\")\n",
        "print(f\"Training Precision: {train_precision:.3f}\")\n",
        "print(f\"Training Recall: {train_recall:.3f}\")\n",
        "print(f\"Training F1: {train_f1Score:.3f}\")\n",
        "print(f\"Training Confusion matrix:\\n{train_confusion_matrix}\")\n",
        "\n",
        "# Make predictions on test data and calculate metrics for test data\n",
        "predictions_test = model.transform(test)\n",
        "test_roc_auc, test_accuracy, test_precision, test_recall, test_f1Score, test_confusion_matrix = calculate_metrics(predictions_test)\n",
        "\n",
        "# Print test metrics\n",
        "print(f\"Test ROC-AUC: {test_roc_auc:.3f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
        "print(f\"Test Precision: {test_precision:.3f}\")\n",
        "print(f\"Test Recall: {test_recall:.3f}\")\n",
        "print(f\"Test F1: {test_f1Score:.3f}\")\n",
        "print(f\"Test Confusion matrix:\\n{test_confusion_matrix}\")\n",
        "\n",
        "# Make predictions on df_oot data and calculate metrics for df_oot data\n",
        "predictions_oot = model.transform(df_oot)\n",
        "oot_roc_auc, oot_accuracy, oot_precision, oot_recall, oot_f1Score, oot_confusion_matrix = calculate_metrics(predictions_oot)\n",
        "\n",
        "# Print df_oot metrics\n",
        "print(f\"df_oot ROC-AUC: {oot_roc_auc:.3f}\")\n",
        "print(f\"df_oot Accuracy: {oot_accuracy:.3f}\")\n",
        "print(f\"df_oot Precision: {oot_precision:.3f}\")\n",
        "print(f\"df_oot Recall: {oot_recall:.3f}\")\n",
        "print(f\"df_oot F1: {oot_f1Score:.3f}\")\n",
        "print(f\"df_oot Confusion matrix:\\n{oot_confusion_matrix}\")\n"
      ],
      "metadata": {
        "id": "3z4xA6RGvU8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_features(model, feature_names):\n",
        "    # Get the coefficients of the LinearSVC model\n",
        "    coefficients = model.stages[-1].coefficients.toArray()\n",
        "\n",
        "    # Create a DataFrame with the feature names and coefficients\n",
        "    feature_importances_df = pd.DataFrame({'feature': feature_names, 'importance': coefficients})\n",
        "\n",
        "    # Sort the DataFrame by the absolute value of the coefficients in descending order\n",
        "    feature_importances_df['abs_importance'] = feature_importances_df['importance'].abs()\n",
        "    feature_importances_df.sort_values(by='abs_importance', ascending=False, inplace=True)\n",
        "\n",
        "    # Return the top 10 features by importance\n",
        "    return feature_importances_df.head(10)\n"
      ],
      "metadata": {
        "id": "TauOcD_iifTk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
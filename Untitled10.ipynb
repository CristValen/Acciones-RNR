{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPp1LhRKwEuddsOFkGAERZG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristValen/Acciones-RNR/blob/main/Untitled10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import DoubleType, IntegerType\n",
        "from imblearn.under_sampling import NearMiss\n",
        "import pandas as pd\n",
        "\n",
        "df8 = df_2.withColumnRenamed('Malo_Dias_tot', 'label')\n",
        "df8 = df8.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 12345\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=seed)\n",
        "\n",
        "# Convert the training data to a Pandas DataFrame\n",
        "train_pd = train.toPandas()\n",
        "\n",
        "# Define features and label\n",
        "features = df8.columns\n",
        "features.remove('label')\n",
        "\n",
        "# Separate the features and label\n",
        "X = train_pd[features]\n",
        "y = train_pd['label']\n",
        "\n",
        "# Perform NearMiss undersampling\n",
        "nm = NearMiss()\n",
        "X_resampled, y_resampled = nm.fit_resample(X, y)\n",
        "\n",
        "# Convert the resampled data back to a PySpark DataFrame\n",
        "train_undersampled_pd = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "train_undersampled = spark.createDataFrame(train_undersampled_pd)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Create the Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=seed)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Define the parameter grid for cross-validation\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
        "\n",
        "# Define the evaluator for cross-validation\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create the cross-validator object\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
        "\n",
        "# Fit the model on the undersampled training data\n",
        "model = cv.fit(train_undersampled)\n",
        "\n",
        "# Make predictions on the undersampled training data\n",
        "predictions_train = model.transform(train_undersampled)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics for training data\n",
        "evaluator_train_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc_train = evaluator_train_roc_auc.evaluate(predictions_train)\n",
        "evaluator_train_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_train = evaluator_train_accuracy.evaluate(predictions_train)\n",
        "\n",
        "print(f\"Training ROC-AUC: {roc_auc_train:.3f}\")\n",
        "print(f\"Training Accuracy: {accuracy_train:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix for training data\n",
        "predictionAndLabels_train = predictions_train.select(\"prediction\", \"label\").rdd\n",
        "metrics_train = MulticlassMetrics(predictionAndLabels_train)\n",
        "confusion_matrix_train = metrics_train.confusionMatrix().toArray()\n",
        "print(f\"Training Confusion matrix:\\n{confusion_matrix_train}\")\n",
        "\n",
        "# Manually calculate recall and F1 score for training data\n",
        "TP_train = confusion_matrix_train[1, 1]\n",
        "FP_train = confusion_matrix_train[0, 1]\n",
        "FN_train = confusion_matrix_train[1, 0]\n",
        "precision_manual_train = TP_train / (TP_train + FP_train)\n",
        "recall_manual_train = TP_train / (TP_train + FN_train)\n",
        "f1_manual_train = 2 * (precision_manual_train * recall_manual_train) / (precision_manual_train + recall_manual_train)\n",
        "print(f\"Training Recall (manually calculated): {recall_manual_train:.3f}\")\n",
        "print(f\"Training F1 (manually calculated): {f1_manual_train:.3f}\")\n",
        "\n",
        "# Convert the test data to a Pandas DataFrame\n",
        "test_pd = test.toPandas()\n",
        "\n",
        "# Separate the features and label in the test data\n",
        "X_test = test_pd[features]\n",
        "y_test = test_pd['label']\n",
        "\n",
        "# Perform NearMiss undersampling on the test data\n",
        "X_test_resampled, y_test_resampled = nm.fit_resample(X_test, y_test)\n",
        "\n",
        "# Convert the resampled test data back to a PySpark DataFrame\n",
        "test_undersampled_pd = pd.concat([X_test_resampled, y_test_resampled], axis=1)\n",
        "test_undersampled = spark.createDataFrame(test_undersampled_pd)\n",
        "\n",
        "# Make predictions on the undersampled test data\n",
        "predictions_test = model.transform(test_undersampled)\n",
        "\n",
        "# Calculate ROC-AUC and accuracy metrics for test data\n",
        "evaluator_test_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc_test = evaluator_test_roc_auc.evaluate(predictions_test)\n",
        "evaluator_test_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_test = evaluator_test_accuracy.evaluate(predictions_test)\n",
        "\n",
        "print(f\"Test ROC-AUC: {roc_auc_test:.3f}\")\n",
        "print(f\"Test Accuracy: {accuracy_test:.3f}\")\n",
        "\n",
        "# Calculate the confusion matrix for test data\n",
        "predictionAndLabels_test = predictions_test.select(\"prediction\", \"label\").rdd\n",
        "metrics_test = MulticlassMetrics(predictionAndLabels_test)\n",
        "confusion_matrix_test = metrics_test.confusionMatrix().toArray()\n",
        "print(f\"Test Confusion matrix:\\n{confusion_matrix_test}\")\n",
        "\n",
        "# Manually calculate recall and F1 score for test data\n",
        "TP_test = confusion_matrix_test[1, 1]\n",
        "FP_test = confusion_matrix_test[0, 1]\n",
        "FN_test = confusion_matrix_test[1, 0]\n",
        "precision_manual_test = TP_test / (TP_test + FP_test)\n",
        "recall_manual_test = TP_test / (TP_test + FN_test)\n",
        "f1_manual_test = 2 * (precision_manual_test * recall_manual_test) / (precision_manual_\\<EUGPSCoordinates>test + recall_manual_train)\n",
        "print(f\"Test Recall (manually calculated): {recall_manual_train:.3f}\")\n",
        "print(f\"Test F1 (manually calculated): {f1_manual_train:.3f}\")\n",
        "\n",
        "# Get the most important features\n",
        "importances = model.bestModel.stages[-1].featureImportances\n",
        "important_features = sorted(zip(importances, features), reverse=True)\n",
        "print(\"Most important features:\")\n",
        "for importance, feature in important_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")\n",
        "\n",
        "# Define a user-defined function to extract the probability of class 1\n",
        "extract_probability = udf(lambda v: float(v[1]), DoubleType())\n",
        "\n",
        "# Create a new column with the probability of class 1 in the predictions_train and predictions_test DataFrames\n",
        "predictions_train = predictions_train.withColumn('score', extract_probability('probability'))\n",
        "predictions_test = predictions_test.withColumn('score', extract_probability('probability'))\n",
        "\n",
        "def calc_ks(data):\n",
        "    data_pd=data.toPandas()\n",
        "    data_pd['good']=(data_pd['label']==0).astype(int)\n",
        "    data_pd['bad']=(data_pd['label']==1).astype(int)\n",
        "    data_pd['bucket']=(data_pd['score'].rank(pct=True)*10).astype(int)\n",
        "    grouped=data_pd.groupby('bucket',as_index=True)\n",
        "    kstable=grouped.min().score.to_frame(name='min_score')\n",
        "    kstable['max_score']=grouped.max().score\n",
        "    kstable['bads']=grouped.sum().bad\n",
        "    kstable['goods']=grouped.sum().good\n",
        "    kstable=kstable.reset_index()\n",
        "    kstable['bad_rate']=kstable.bads/(kstable.bads+kstable.goods)\n",
        "    kstable['ks']=(kstable.bads/kstable.bads.sum()).cumsum()-(kstable.goods/kstable.goods.sum()).cumsum()\n",
        "    ks_value=kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "# Calculate the KS statistic for the training data\n",
        "ks_value_train = calc_ks(predictions_train)\n",
        "print(f\"Training KS: {ks_value_train:.3f}\")\n",
        "\n",
        "# Calculate the KS statistic for the test data\n",
        "ks_value_test = calc_ks(predictions_test)\n",
        "print(f\"Test KS: {ks_value_test:.3f}\")\n"
      ],
      "metadata": {
        "id": "dFZABeDLL0ay"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
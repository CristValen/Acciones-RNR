{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMB9S9keNbBtfSSam0bt3h"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import pickle\n",
        "\n",
        "# Inicia la sesión de Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Asignar categorías a números\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Datos de ejemplo\n",
        "data2 = spark.createDataFrame([\n",
        "    (0, \"casa\", \"x\", \"m\", \"p\", \"r\"),\n",
        "    (1, \"piscina\", \"y\", \"n\", \"q\", \"s\"),\n",
        "    (2, \"jardín\", \"z\", \"o\", \"r\", \"t\"),\n",
        "    (3, \"casa\", \"x\", \"m\", \"p\", \"r\"),\n",
        "    (4, \"casa\", \"y\", \"n\", \"q\", \"s\"),\n",
        "    (5, \"jardín\", \"z\", \"o\", \"r\", \"t\"),\n",
        "    (6, \"garaje\", \"w\", \"p\", \"u\", \"v\")  # Ejemplo de nueva categoría\n",
        "], [\"id\", \"MCC\", \"lia_sbif\", \"lia_getnet\", \"segmento\", \"estado_Revision_Equipo\"])\n",
        "\n",
        "# Convertir a pandas DataFrame para usar OrdinalEncoder\n",
        "data_pd = data2.toPandas()\n",
        "\n",
        "# Definir las columnas categóricas\n",
        "categorical_cols = ['MCC', 'lia_sbif', 'lia_getnet', 'segmento', 'estado_Revision_Equipo']\n",
        "\n",
        "# Crear el codificador\n",
        "encoder = OrdinalEncoder()\n",
        "\n",
        "# Ajustar el codificador y transformar los datos\n",
        "data_pd[categorical_cols] = encoder.fit_transform(data_pd[categorical_cols])\n",
        "\n",
        "# Convertir de nuevo a Spark DataFrame\n",
        "data2 = spark.createDataFrame(data_pd)\n",
        "\n",
        "# Mostrar los datos transformados\n",
        "data2.show()\n",
        "\n",
        "# Guardar el codificador y las categorías en un archivo\n",
        "with open('ordinal_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump((encoder, encoder.categories_), f)"
      ],
      "metadata": {
        "id": "Rnv64rL0yJLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import array, struct, explode, col, collect_list, lit\n",
        "\n",
        "# Función para transponer cada fila manteniendo el RUT\n",
        "def transponer_fila(row):\n",
        "    transposed = []\n",
        "    for column, value in zip(df.columns[1:], row[1:]):\n",
        "        transposed.append((row[0], column, value))\n",
        "    return transposed\n",
        "\n",
        "# Convertir el DataFrame de filas a lista de filas transpuestas\n",
        "transposed_data = [item for row in df.collect() for item in transponer_fila(row)]\n",
        "\n",
        "# Crear un DataFrame transpuesto con RUT, nombre_variable y peso\n",
        "df_transpuesto = spark.createDataFrame(transposed_data, [\"rut\", \"nombre_variable\", \"peso\"])\n",
        "\n",
        "# Establecer el umbral para el riesgo alto\n",
        "umbral_riesgo_alto = 0.7\n",
        "\n",
        "# Identificar variables de riesgo alto\n",
        "df_riesgo_alto = df_transpuesto.filter(col(\"peso\") > umbral_riesgo_alto)\n",
        "\n",
        "# Recopilar las variables de riesgo alto por RUT\n",
        "df_resultado = df_riesgo_alto.groupBy(\"rut\").agg(collect_list(\"nombre_variable\").alias(\"variables_riesgo_alto\"))\n",
        "\n",
        "# Unir el resultado con el DataFrame original para mantener todos los datos\n",
        "df_final = df.join(df_resultado, on=\"rut\", how=\"left\")\n",
        "\n",
        "# Mostrar el DataFrame final\n",
        "df_final.show(truncate=False)"
      ],
      "metadata": {
        "id": "2_GcsQNfi2PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import array, struct, explode, col, sum, collect_list\n",
        "\n",
        "# Convertir las columnas en filas (transponer el DataFrame)\n",
        "df_transpuesto = df.select(explode(array([struct(lit(c).alias(\"nombre_variable\"), col(c).alias(\"peso\")) for c in df.columns])).alias(\"transpuesto\"))\n",
        "df_transpuesto = df_transpuesto.select(\"transpuesto.*\")\n",
        "\n",
        "# Establecer el umbral para el riesgo alto\n",
        "umbral_riesgo_alto = 0.7\n",
        "\n",
        "# Identificar variables de riesgo alto\n",
        "df_riesgo_alto = df_transpuesto.filter(col(\"peso\") > umbral_riesgo_alto)\n",
        "\n",
        "# Recopilar las variables de riesgo alto en una lista y agregarlas en una nueva columna\n",
        "df_resultado = df_riesgo_alto.groupBy().agg(collect_list(\"nombre_variable\").alias(\"variables_riesgo_alto\"))\n",
        "\n",
        "# Mostrar el DataFrame resultante\n",
        "df_resultado.show(truncate=False)"
      ],
      "metadata": {
        "id": "ENyiIiSThVHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, first, when\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"FiltrarProductos\").getOrCreate()\n",
        "\n",
        "# Datos de ejemplo\n",
        "data = [\n",
        "    ('12345678-9', 'cuenta corriente'),\n",
        "    ('12345678-9', 'tarjeta de crédito'),\n",
        "    ('98765432-1', 'cuenta corriente mx'),\n",
        "    ('98765432-1', 'préstamo personal'),\n",
        "    ('12345678-9', 'préstamo personal'),\n",
        "    ('98765432-1', 'tarjeta de crédito'),\n",
        "    ('98765432-1', 'seguro de vida')\n",
        "]\n",
        "\n",
        "# Crear DataFrame\n",
        "df = spark.createDataFrame(data, ['RUT', 'Producto'])\n",
        "\n",
        "# Crear una columna para marcar si es cuenta corriente\n",
        "df = df.withColumn('EsCuentaCorriente', when(col('Producto').isin(['cuenta corriente', 'cuenta corriente mx']), 1).otherwise(0))\n",
        "\n",
        "# Filtrar para obtener las cuentas corrientes o, si no hay, otro producto\n",
        "df_filtrado = df.groupBy('RUT').agg(\n",
        "    first(when(col('EsCuentaCorriente') == 1, col('Producto'))).alias('CuentaCorriente'),\n",
        "    first(when(col('EsCuentaCorriente') == 0, col('Producto'))).alias('OtroProducto')\n",
        ")\n",
        "\n",
        "# Seleccionar la columna adecuada\n",
        "df_resultado = df_filtrado.withColumn('ProductoFinal', when(col('CuentaCorriente').isNotNull(), col('CuentaCorriente')).otherwise(col('OtroProducto'))).select('RUT', 'ProductoFinal')\n",
        "\n",
        "# Mostrar el resultado\n",
        "df_resultado.show()"
      ],
      "metadata": {
        "id": "DHXDMJEBQ2-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, col, lit\n",
        "from pyspark.sql.types import IntegerType\n",
        "import pickle\n",
        "\n",
        "# Inicia la sesión de Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Leer categorías desde archivo\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Datos de ejemplo\n",
        "data2 = spark.createDataFrame([\n",
        "    (0, \"a\", \"x\", \"m\", \"p\", \"r\"),\n",
        "    (1, \"b\", \"y\", \"n\", \"q\", \"s\"),\n",
        "    (2, \"c\", \"z\", \"o\", \"r\", \"t\"),\n",
        "    (3, \"a\", \"x\", \"m\", \"p\", \"r\"),\n",
        "    (4, \"a\", \"y\", \"n\", \"q\", \"s\"),\n",
        "    (5, \"c\", \"z\", \"o\", \"r\", \"t\"),\n",
        "    (6, \"d\", \"w\", \"p\", \"u\", \"v\")  # Ejemplo de nueva categoría\n",
        "], [\"id\", \"MCC\", \"lia_sbif\", \"lia_getnet\", \"segmento\", \"estado_Revision_Equipo\"])\n",
        "\n",
        "# Definir las columnas categóricas\n",
        "categorical_cols = ['MCC', 'lia_sbif', 'lia_getnet', 'segmento', 'estado_Revision_Equipo']\n",
        "\n",
        "# Intentar cargar el diccionario desde un archivo\n",
        "try:\n",
        "    with open('category_to_index.pkl', 'rb') as f:\n",
        "        category_to_index = pickle.load(f)\n",
        "except FileNotFoundError:\n",
        "    # Si el archivo no existe, crear un diccionario vacío\n",
        "    category_to_index = {col: {} for col in categorical_cols}\n",
        "\n",
        "# Índice especial para nuevas categorías\n",
        "special_index = -1\n",
        "\n",
        "# Función para asignar índices automáticamente\n",
        "def assign_index(column, category):\n",
        "    if category not in category_to_index[column]:\n",
        "        return special_index  # Asignar índice especial a nuevas categorías\n",
        "    return category_to_index[column][category]\n",
        "\n",
        "# Registrar la UDF\n",
        "assign_index_udf = udf(assign_index, IntegerType())\n",
        "\n",
        "# Aplicar la UDF a cada columna categórica\n",
        "for column in categorical_cols:\n",
        "    data2 = data2.withColumn(column + \"_num\", assign_index_udf(lit(column), col(column)))\n",
        "\n",
        "# Mostrar los datos transformados\n",
        "data2.show()\n",
        "\n",
        "# Guardar el diccionario actualizado en un archivo\n",
        "with open('category_to_index.pkl', 'wb') as f:\n",
        "    pickle.dump(category_to_index, f)\n",
        "\n",
        "# Mostrar el diccionario de mapeo\n",
        "print(category_to_index)"
      ],
      "metadata": {
        "id": "7o27EXifiNa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import pickle\n",
        "\n",
        "# Inicia la sesión de Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Leer categorías desde archivo\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Datos de ejemplo\n",
        "data2 = spark.createDataFrame([\n",
        "    (0, \"casa\", \"x\", \"m\", \"p\", \"r\"),\n",
        "    (1, \"piscina\", \"y\", \"n\", \"q\", \"s\"),\n",
        "    (2, \"jardín\", \"z\", \"o\", \"r\", \"t\"),\n",
        "    (3, \"casa\", \"x\", \"m\", \"p\", \"r\"),\n",
        "    (4, \"casa\", \"y\", \"n\", \"q\", \"s\"),\n",
        "    (5, \"jardín\", \"z\", \"o\", \"r\", \"t\"),\n",
        "    (6, \"garaje\", \"w\", \"p\", \"u\", \"v\"),  # Ejemplo de nueva categoría\n",
        "    (7, \"terraza\", \"v\", \"q\", \"t\", \"u\")  # Nueva categoría\n",
        "], [\"id\", \"MCC\", \"lia_sbif\", \"lia_getnet\", \"segmento\", \"estado_Revision_Equipo\"])\n",
        "\n",
        "# Convertir a pandas DataFrame para usar OrdinalEncoder\n",
        "data_pd = data2.toPandas()\n",
        "\n",
        "# Cargar el codificador y las categorías desde el archivo\n",
        "with open('ordinal_encoder.pkl', 'rb') as f:\n",
        "    encoder, categories = pickle.load(f)\n",
        "\n",
        "# Actualizar el codificador con nuevas categorías\n",
        "for i, col in enumerate(categorical_cols):\n",
        "    new_categories = pd.Series(data_pd[col]).unique()\n",
        "    all_categories = list(categories[i]) + [cat for cat in new_categories if cat not in categories[i]]\n",
        "    encoder.categories_[i] = all_categories\n",
        "\n",
        "# Transformar los datos usando el codificador actualizado\n",
        "data_pd[categorical_cols] = encoder.transform(data_pd[categorical_cols])\n",
        "\n",
        "# Convertir de nuevo a Spark DataFrame\n",
        "data2 = spark.createDataFrame(data_pd)\n",
        "\n",
        "# Mostrar los datos transformados\n",
        "data2.show()\n",
        "\n",
        "# Guardar el codificador y las categorías actualizadas en un archivo\n",
        "with open('ordinal_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump((encoder, encoder.categories_), f)"
      ],
      "metadata": {
        "id": "hPwbzi_SyP8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Intentar cargar el diccionario desde un archivo\n",
        "try:\n",
        "    with open('category_to_index.pkl', 'rb') as f:\n",
        "        category_to_index = pickle.load(f)\n",
        "except FileNotFoundError:\n",
        "    # Si el archivo no existe, crear un diccionario vacío\n",
        "    category_to_index = {col: {} for col in categorical_cols}\n",
        "\n",
        "# Función para asignar índices automáticamente\n",
        "def assign_index(column, category):\n",
        "    if category not in category_to_index[column]:\n",
        "        category_to_index[column][category] = len(category_to_index[column])\n",
        "    return category_to_index[column][category]\n",
        "\n",
        "# Registrar la UDF\n",
        "assign_index_udf = udf(assign_index, IntegerType())\n",
        "\n",
        "# Aplicar la UDF a cada columna categórica\n",
        "for column in categorical_cols:\n",
        "    data2 = data2.withColumn(column + \"_num\", assign_index_udf(lit(column), col(column)))\n",
        "\n",
        "# Mostrar los datos transformados\n",
        "data2.show()\n",
        "\n",
        "# Guardar el diccionario actualizado en un archivo\n",
        "with open('category_to_index.pkl', 'wb') as f:\n",
        "    pickle.dump(category_to_index, f)\n",
        "\n",
        "# Mostrar el diccionario de mapeo\n",
        "print(category_to_index)"
      ],
      "metadata": {
        "id": "Ge-vl4EZgBdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, col, lit\n",
        "from pyspark.sql.types import IntegerType\n",
        "import pickle\n",
        "\n",
        "# Inicia la sesión de Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Ejemplo UDF para categorías\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Datos de ejemplo\n",
        "data2 = spark.createDataFrame([\n",
        "    (0, \"a\", \"x\", \"m\", \"p\", \"r\"),\n",
        "    (1, \"b\", \"y\", \"n\", \"q\", \"s\"),\n",
        "    (2, \"c\", \"z\", \"o\", \"r\", \"t\"),\n",
        "    (3, \"a\", \"x\", \"m\", \"p\", \"r\"),\n",
        "    (4, \"a\", \"y\", \"n\", \"q\", \"s\"),\n",
        "    (5, \"c\", \"z\", \"o\", \"r\", \"t\"),\n",
        "    (6, \"d\", \"w\", \"p\", \"u\", \"v\")  # Ejemplo de nueva categoría\n",
        "], [\"id\", \"MCC\", \"lia_sbif\", \"lia_getnet\", \"segmento\", \"estado_Revision_Equipo\"])\n",
        "\n",
        "# Definir las columnas categóricas\n",
        "categorical_cols = ['MCC', 'lia_sbif', 'lia_getnet', 'segmento', 'estado_Revision_Equipo']\n",
        "\n",
        "# Intentar cargar el diccionario desde un archivo\n",
        "try:\n",
        "    with open('category_to_index.pkl', 'rb') as f:\n",
        "        category_to_index = pickle.load(f)\n",
        "except FileNotFoundError:\n",
        "    # Si el archivo no existe, crear un diccionario vacío\n",
        "    category_to_index = {col: {} for col in categorical_cols}\n",
        "\n",
        "# Función para asignar índices automáticamente\n",
        "def assign_index(column, category):\n",
        "    if category not in category_to_index[column]:\n",
        "        category_to_index[column][category] = len(category_to_index[column])\n",
        "    return category_to_index[column][category]\n",
        "\n",
        "# Registrar la UDF\n",
        "assign_index_udf = udf(assign_index, IntegerType())\n",
        "\n",
        "# Aplicar la UDF a cada columna categórica\n",
        "for column in categorical_cols:\n",
        "    data2 = data2.withColumn(column + \"_num\", assign_index_udf(lit(column), col(column)))\n",
        "\n",
        "# Mostrar los datos transformados\n",
        "data2.show()\n",
        "\n",
        "# Guardar el diccionario en un archivo\n",
        "with open('category_to_index.pkl', 'wb') as f:\n",
        "    pickle.dump(category_to_index, f)\n",
        "\n",
        "# Mostrar el diccionario de mapeo\n",
        "print(category_to_index)"
      ],
      "metadata": {
        "id": "t_Ms3QK8cdOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_czqakCf_Px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "def procesar_direcciones(excel_path):\n",
        "    # Leer el archivo Excel\n",
        "    df = pd.read_excel(excel_path)\n",
        "\n",
        "    # Verificar si las columnas 'Nombre' y 'Dirección' existen\n",
        "    if 'Nombre' not in df.columns or 'Dirección' not in df.columns:\n",
        "        raise ValueError(\"El archivo Excel debe contener las columnas 'Nombre' y 'Dirección'\")\n",
        "\n",
        "    # Obtener las listas de nombres y direcciones\n",
        "    names = df['Nombre'].tolist()\n",
        "    addresses = df['Dirección'].tolist()\n",
        "\n",
        "    # Procesar las direcciones\n",
        "    processed_addresses = []\n",
        "    for address in addresses:\n",
        "        text = str(address).strip()\n",
        "        if text != \"nan\":\n",
        "            processed_addresses.append(text)\n",
        "\n",
        "    return names, processed_addresses\n",
        "\n",
        "# Ejemplo de uso\n",
        "excel_path = 'ruta/al/archivo.xlsx'\n",
        "try:\n",
        "    names, addresses = procesar_direcciones(excel_path)\n",
        "    print(\"Nombres:\", names)\n",
        "    print(\"Direcciones:\", addresses)\n",
        "except ValueError as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "xsaFiinzSzi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Leer el archivo Excel\n",
        "excel_path = 'ruta/al/archivo.xlsx'\n",
        "df = pd.read_excel(excel_path)\n",
        "\n",
        "# Verificar si las columnas 'Nombre' y 'Dirección' existen\n",
        "if 'Nombre' not in df.columns or 'Dirección' not in df.columns:\n",
        "    raise ValueError(\"El archivo Excel debe contener las columnas 'Nombre' y 'Dirección'\")\n",
        "\n",
        "# Continuar con el procesamiento si las columnas existen\n",
        "names = df['Nombre'].tolist()\n",
        "addresses = df['Dirección'].tolist()\n",
        "\n",
        "# Ejemplo de procesamiento adicional\n",
        "matched_entries = []\n",
        "for name, address in zip(names, addresses):\n",
        "    if name in pdf_text:\n",
        "        matched_entries.append((name, address))\n",
        "\n",
        "print(matched_entries)"
      ],
      "metadata": {
        "id": "YpN6J4y7LLhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "python app.py  pip install dash pandas plotly"
      ],
      "metadata": {
        "id": "alrAcpGZMlqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import dash\n",
        "from dash import dcc, html\n",
        "from dash.dependencies import Input, Output\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# Crear un DataFrame de ejemplo\n",
        "data = {\n",
        "    \"Nacionalidad\": [\"Chile\", \"Argentina\", \"Perú\", \"Chile\", \"Argentina\", \"Perú\"],\n",
        "    \"Domicilio\": [\"Santiago\", \"Buenos Aires\", \"Lima\", \"Valparaíso\", \"Córdoba\", \"Cusco\"],\n",
        "    \"Riesgo\": [\"Alto\", \"Medio\", \"Bajo\", \"Alto\", \"Medio\", \"Bajo\"],\n",
        "    \"penumdoc\": [\"12345678901\", \"98765432109\", \"12312312345\", \"23456789012\", \"87654321098\", \"32132132154\"]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Inicializar la aplicación Dash\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "# Layout de la aplicación\n",
        "app.layout = html.Div([\n",
        "    html.H1(\"Panel Interactivo de Riesgo Bancario\"),\n",
        "    dcc.Dropdown(\n",
        "        id='riesgo-dropdown',\n",
        "        options=[{'label': riesgo, 'value': riesgo} for riesgo in df['Riesgo'].unique()],\n",
        "        value='Alto'\n",
        "    ),\n",
        "    dcc.Graph(id='grafico-riesgo')\n",
        "])\n",
        "\n",
        "# Callback para actualizar el gráfico basado en el riesgo seleccionado\n",
        "@app.callback(\n",
        "    Output('grafico-riesgo', 'figure'),\n",
        "    [Input('riesgo-dropdown', 'value')]\n",
        ")\n",
        "def actualizar_grafico(riesgo_seleccionado):\n",
        "    df_filtrado = df[df['Riesgo'] == riesgo_seleccionado]\n",
        "    fig = px.bar(df_filtrado, x='Nacionalidad', y='penumdoc', color='Domicilio', title=f'Clientes con riesgo {riesgo_seleccionado}')\n",
        "    return fig\n",
        "\n",
        "# Ejecutar la aplicación\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=True)\n",
        ","
      ],
      "metadata": {
        "id": "KRSX117UPYOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import pickle\n",
        "\n",
        "# Inicia la sesión de Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Asignar categorías a números\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Datos de ejemplo\n",
        "data2 = spark.createDataFrame([\n",
        "    (0, \"casa\", \"x\", \"m\", \"p\", \"r\"),\n",
        "    (1, \"piscina\", \"y\", \"n\", \"q\", \"s\"),\n",
        "    (2, \"jardín\", \"z\", \"o\", \"r\", \"t\"),\n",
        "    (3, \"casa\", \"x\", \"m\", \"p\", \"r\"),\n",
        "    (4, \"casa\", \"y\", \"n\", \"q\", \"s\"),\n",
        "    (5, \"jardín\", \"z\", \"o\", \"r\", \"t\"),\n",
        "    (6, \"garaje\", \"w\", \"p\", \"u\", \"v\")  # Ejemplo de nueva categoría\n",
        "], [\"id\", \"MCC\", \"lia_sbif\", \"lia_getnet\", \"segmento\", \"estado_Revision_Equipo\"])\n",
        "\n",
        "# Convertir a pandas DataFrame para usar OrdinalEncoder\n",
        "data_pd = data2.toPandas()\n",
        "\n",
        "# Definir las columnas categóricas\n",
        "categorical_cols = ['MCC', 'lia_sbif', 'lia_getnet', 'segmento', 'estado_Revision_Equipo']\n",
        "\n",
        "# Crear el codificador\n",
        "encoder = OrdinalEncoder()\n",
        "\n",
        "# Ajustar el codificador y transformar los datos\n",
        "data_pd[categorical_cols] = encoder.fit_transform(data_pd[categorical_cols])\n",
        "\n",
        "# Convertir de nuevo a Spark DataFrame\n",
        "data2 = spark.createDataFrame(data_pd)\n",
        "\n",
        "# Mostrar los datos transformados\n",
        "data2.show()\n",
        "\n",
        "# Guardar el codificador y las categorías en un archivo\n",
        "with open('ordinal_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump((encoder, encoder.categories_), f)\n",
        "\n",
        "# Mostrar el mapeo de categorías a números\n",
        "for col, categories in zip(categorical_cols, encoder.categories_):\n",
        "    print(f\"Categorías para {col}:\")\n",
        "    for idx, category in enumerate(categories):\n",
        "        print(f\"  {category}: {idx}\")"
      ],
      "metadata": {
        "id": "bVzoDtRMOjbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import pickle\n",
        "\n",
        "# Inicia la sesión de Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Leer categorías desde archivo\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Datos de ejemplo\n",
        "data2 = spark.createDataFrame([\n",
        "    (0, \"casa\", \"x\", \"m\", \"p\", \"r\"),\n",
        "    (1, \"piscina\", \"y\", \"n\", \"q\", \"s\"),\n",
        "    (2, \"jardín\", \"z\", \"o\", \"r\", \"t\"),\n",
        "    (3, \"casa\", \"x\", \"m\", \"p\", \"r\"),\n",
        "    (4, \"casa\", \"y\", \"n\", \"q\", \"s\"),\n",
        "    (5, \"jardín\", \"z\", \"o\", \"r\", \"t\"),\n",
        "    (6, \"garaje\", \"w\", \"p\", \"u\", \"v\"),  # Ejemplo de nueva categoría\n",
        "    (7, \"terraza\", \"v\", \"q\", \"t\", \"u\")  # Nueva categoría\n",
        "], [\"id\", \"MCC\", \"lia_sbif\", \"lia_getnet\", \"segmento\", \"estado_Revision_Equipo\"])\n",
        "\n",
        "# Convertir a pandas DataFrame para usar OrdinalEncoder\n",
        "data_pd = data2.toPandas()\n",
        "\n",
        "# Cargar el codificador y las categorías desde el archivo\n",
        "with open('ordinal_encoder.pkl', 'rb') as f:\n",
        "    encoder, categories = pickle.load(f)\n",
        "\n",
        "# Actualizar el codificador con nuevas categorías\n",
        "for i, col in enumerate(categorical_cols):\n",
        "    new_categories = pd.Series(data_pd[col]).unique()\n",
        "    all_categories = list(categories[i]) + [cat for cat in new_categories if cat not in categories[i]]\n",
        "    encoder.categories_[i] = all_categories\n",
        "\n",
        "# Transformar los datos usando el codificador actualizado\n",
        "data_pd[categorical_cols] = encoder.transform(data_pd[categorical_cols])\n",
        "\n",
        "# Convertir de nuevo a Spark DataFrame\n",
        "data2 = spark.createDataFrame(data_pd)\n",
        "\n",
        "# Mostrar los datos transformados\n",
        "data2.show()\n",
        "\n",
        "# Guardar el codificador y las categorías actualizadas en un archivo\n",
        "with open('ordinal_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump((encoder, encoder.categories_), f)\n",
        "\n",
        "# Mostrar el mapeo de categorías a números\n",
        "for col, categories in zip(categorical_cols, encoder.categories_):\n",
        "    print(f\"Categorías para {col}:\")\n",
        "    for idx, category in enumerate(categories):\n",
        "        print(f\"  {category}: {idx}\")"
      ],
      "metadata": {
        "id": "rZsab-YoOoL5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
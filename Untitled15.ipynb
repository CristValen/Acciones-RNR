{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPExnRt8o5jL4+GAzz+qh77",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristValen/Acciones-RNR/blob/main/Untitled15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "G0jB3ijAszty",
        "outputId": "8fbac770-cdee-4b72-ca69-da3d2e42f7d8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'install' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ae9979fd4d2f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minstall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tidyverse\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'install' is not defined"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import trim, substring_index\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Supongamos que tienes dos dataframes, df1 y df2, que corresponden a 'tabla1' y 'tabla2'\n",
        "df1 = spark.table(\"tabla1\")\n",
        "df2 = spark.table(\"tabla2\")\n",
        "\n",
        "# Realizar el left join\n",
        "df_joined = df1.alias('b').join(df2.alias('i'),\n",
        "                                (trim(df1['pesecper']) == trim(df2['gen_clave'])) &\n",
        "                                (trim(df2['gen_tabla']) == '0230'),\n",
        "                                how='left')\n",
        "\n",
        "# Agregar la columna 'composicion'\n",
        "df_joined = df_joined.withColumn('composicion', substring_index(df2['gen_datos1'], '', 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Carga tus datos\n",
        "datos2 = spark.read.table(\"datos2\")\n",
        "datos3 = spark.read.table(\"datos3\")\n",
        "\n",
        "# Realiza la consulta\n",
        "result = datos2.alias('a').join(datos3.alias('x'), col('a.penumdoc') == col('x.penumdoc'), 'left') \\\n",
        "    .select(col('a.penumdoc').alias('penumdoc'),\n",
        "            when(col('x.penumper').isNull(), '').otherwise(col('x.penumper')).alias('penumper'),\n",
        "            when(col('x.pefecalt').isNull(), '').otherwise(col('x.pefecalt')).alias('pefecalt'),\n",
        "            when(col('x.petipper').isNull(), when(col('a.penumdoc').substr(1, 10).cast('int') >= 50000000, 'J').otherwise('F')).otherwise(col('x.petipper')).alias('petipper'),\n",
        "            col('a.origen').alias('origen')).distinct()\n",
        "\n",
        "# Guarda el resultado en una nueva tabla\n",
        "result.write.saveAsTable(\"gincumplimiento.tabla1\")\n"
      ],
      "metadata": {
        "id": "B6Hsd1tkQg37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtrado = df.filter(\n",
        "    (F.col(\"residencia\") != \"chile\") &\n",
        "    ((F.col(\"pais_tributario1\") != \"chile\") | (F.col(\"pais_tributario1\").isNull())) &\n",
        "    ((F.col(\"pais_tributario2\") != \"chile\") | (F.col(\"pais_tributario2\").isNull())) &\n",
        "    ((F.col(\"pais_tributario3\") != \"chile\") | (F.col(\"pais_tributario3\").isNull())) &\n",
        "    (F.col(\"pais_tributario1\") != F.col(\"residencia\")) &\n",
        "    (F.col(\"pais_tributario2\") != F.col(\"residencia\")) &\n",
        "    (F.col(\"pais_tributario3\") != F.col(\"residencia\"))\n",
        ")"
      ],
      "metadata": {
        "id": "FtsimEugkszz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df = df.filter(\n",
        "    (~F.col('pais de residencia').isin(['CHILE', 'ESTADOS UNIDOS'])) &\n",
        "    (F.col('autoclasificacion').isNull()) &\n",
        "    (F.col('pais tributario 1').isNull()) &\n",
        "    (F.col('pais tributario 2').isNull()) &\n",
        "    (F.col('pais tributario 3').isNull()) &\n",
        "    (F.col('ccp') == 'personas fisica') &\n",
        "    (F.col('gestion_form') == 'aprobado')\n",
        ")\n"
      ],
      "metadata": {
        "id": "R72TO4WtD6Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df = df.filter(\n",
        "    (F.col('pais de residencia') == 'ESTADOS UNIDOS') &\n",
        "    (F.col('autoclasificacion').isNull()) &\n",
        "    (F.col('pais tributario 1').isNull()) &\n",
        "    (F.col('pais tributario 2').isNull()) &\n",
        "    (F.col('pais tributario 3').isNull()) &\n",
        "    (F.col('ips').isNull())\n",
        ")\n"
      ],
      "metadata": {
        "id": "qoPYuu2VFVH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df_f = df_f.filter(\n",
        "    (~F.col('tributario1').isin(['CHILE', 'AUSTRALIA', 'BERMUDA', 'ISLAS VIRGENES BRITANICAS', 'BVI', 'ISLAS CAIMAN', 'JAPON']) | F.col('tributario1').isNull()) &\n",
        "    (~F.col('tributario2').isin(['CHILE', 'AUSTRALIA', 'BERMUDA', 'ISLAS VIRGENES BRITANICAS', 'BVI', 'ISLAS CAIMAN', 'JAPON']) | F.col('tributario2').isNull()) &\n",
        "    (~F.col('tributario3').isin(['CHILE', 'AUSTRALIA', 'BERMUDA', 'ISLAS VIRGENES BRITANICAS', 'BVI', 'ISLAS CAIMAN', 'JAPON']) | F.col('tributario3').isNull()) &\n",
        "    ((F.col('tributario1').isNotNull() & (F.length(F.col('tributario1')) > 0)) |\n",
        "     (F.col('tributario2').isNotNull() & (F.length(F.col('tributario2')) > 0)) |\n",
        "     (F.col('tributario3').isNotNull() & (F.length(F.col('tributario3')) > 0))) &\n",
        "    (F.col('tipo_doc_tin').isNull())\n",
        ")"
      ],
      "metadata": {
        "id": "v4xFrtNJJCrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Supongamos que tus tablas están en una lista de DataFrames\n",
        "tablas = [supuesto_1_m, supuesto_2_m, supuesto_3_m, ..., supuesto_13_m]\n",
        "\n",
        "# Creas un DataFrame vacío para almacenar los RUT de todos los supuestos anteriores\n",
        "ruts_previos = spark.createDataFrame([], tablas[0].schema)\n",
        "\n",
        "for i, tabla in enumerate(tablas, start=1):  # Para cada una de tus 13 tablas\n",
        "    # Verificas si la tabla está vacía\n",
        "    if tabla.rdd.isEmpty():\n",
        "        print(f'Supuesto {i}: La tabla está vacía')\n",
        "        continue\n",
        "\n",
        "    # Filtras los RUT que no están en ruts_previos\n",
        "    ruts_i = tabla.subtract(ruts_previos)\n",
        "\n",
        "    # Imprimes los RUT únicos para el supuesto i\n",
        "    print(f'Supuesto {i}:')\n",
        "    ruts_i.show()\n",
        "\n",
        "    # Añades los RUT de la tabla i a ruts_previos\n",
        "    ruts_previos = ruts_previos.union(tabla)\n",
        "\n"
      ],
      "metadata": {
        "id": "UvwwXODrTGXe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
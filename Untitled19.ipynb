{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnEfnv0rbCJpI6YLcNPhXc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTYfA5cQIEpp",
        "outputId": "643d1198-470a-460f-f022-345624ea966b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Iniciar una sesión de Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Python Spark SQL basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, row_number, when\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"RemoveDuplicates\").getOrCreate()\n",
        "\n",
        "# Supongamos que tienes un DataFrame con tus datos\n",
        "data = [\n",
        "    (7101287883, '1010225434452', '1234', '9876'),\n",
        "    (7101287883, '510035767005', '5678', '5432'),\n",
        "    (7101287883, '1010225434452', '1234', '9876'),\n",
        "    (5102225434452, '510035767005', '4321', '6543'),\n",
        "    (5102225434452, '72278398', '8765', '2109'),\n",
        "    (5102225434452, '510035767005', '4321', '6543')\n",
        "]\n",
        "\n",
        "columns = ['RUT', 'cuenta_corriente', 'tarjeta_credito', 'prestamo']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Crear una columna de índice para mantener el orden original\n",
        "window_spec = Window.partitionBy().orderBy(\"RUT\")\n",
        "df = df.withColumn(\"index\", row_number().over(window_spec))\n",
        "\n",
        "# Función para eliminar duplicados y dejar en blanco las celdas duplicadas\n",
        "def remove_duplicates(df, column):\n",
        "    window_spec = Window.partitionBy(\"RUT\", column).orderBy(\"index\")\n",
        "    df = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
        "    df = df.withColumn(column, when(col(\"row_num\") == 1, col(column)).otherwise(\"\"))\n",
        "    return df.drop(\"row_num\")\n",
        "\n",
        "# Aplicar la función a cada columna\n",
        "df = remove_duplicates(df, \"cuenta_corriente\")\n",
        "df = remove_duplicates(df, \"tarjeta_credito\")\n",
        "df = remove_duplicates(df, \"prestamo\")\n",
        "\n",
        "# Ordenar por índice y eliminar la columna de índice\n",
        "df = df.orderBy(\"index\").drop(\"index\")\n",
        "\n",
        "# Eliminar filas donde todas las columnas de productos están en blanco\n",
        "df = df.filter(~((col(\"cuenta_corriente\") == \"\") & (col(\"tarjeta_credito\") == \"\") & (col(\"prestamo\") == \"\")))\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "3tXoNbdop3aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, row_number, when\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"RemoveDuplicates\").getOrCreate()\n",
        "\n",
        "# Supongamos que tienes un DataFrame con tus datos\n",
        "data = [\n",
        "    (7101287883, '1010225434452', '1234', '9876'),\n",
        "    (7101287883, '510035767005', '5678', '5432'),\n",
        "    (7101287883, '1010225434452', '1234', '9876'),\n",
        "    (5102225434452, '510035767005', '4321', '6543'),\n",
        "    (5102225434452, '72278398', '8765', '2109'),\n",
        "    (5102225434452, '510035767005', '4321', '6543')\n",
        "]\n",
        "\n",
        "columns = ['RUT', 'cuenta_corriente', 'tarjeta_credito', 'prestamo']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Crear una columna de índice para mantener el orden original\n",
        "window_spec = Window.partitionBy().orderBy(\"RUT\")\n",
        "df = df.withColumn(\"index\", row_number().over(window_spec))\n",
        "\n",
        "# Función para eliminar duplicados y dejar en blanco las celdas duplicadas\n",
        "def remove_duplicates(df, column):\n",
        "    window_spec = Window.partitionBy(\"RUT\", column).orderBy(\"index\")\n",
        "    df = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
        "    df = df.withColumn(column, when(col(\"row_num\") == 1, col(column)).otherwise(\"\"))\n",
        "    return df.drop(\"row_num\")\n",
        "\n",
        "# Aplicar la función a cada columna\n",
        "df = remove_duplicates(df, \"cuenta_corriente\")\n",
        "df = remove_duplicates(df, \"tarjeta_credito\")\n",
        "df = remove_duplicates(df, \"prestamo\")\n",
        "\n",
        "# Ordenar por índice y eliminar la columna de índice\n",
        "df = df.orderBy(\"index\").drop(\"index\")\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "-2iD7wX_kS9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, col, array_distinct\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"ExpandRows\").getOrCreate()\n",
        "\n",
        "# Supongamos que tienes un DataFrame con tus datos\n",
        "data = [\n",
        "    (7101287883, ['1010225434452', '510035767005', '72278398', '1010225434452'], ['1234', '5678', '1234'], ['9876', '5432', '9876']),\n",
        "    (5102225434452, ['510035767005', '72278398', '510035767005'], ['4321', '8765', '4321'], ['6543', '2109', '6543'])\n",
        "]\n",
        "\n",
        "columns = ['RUT', 'cuenta_corriente', 'tarjeta_credito', 'prestamo']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Eliminar duplicados dentro de cada array\n",
        "df = df.withColumn(\"cuenta_corriente\", array_distinct(col(\"cuenta_corriente\"))) \\\n",
        "       .withColumn(\"tarjeta_credito\", array_distinct(col(\"tarjeta_credito\"))) \\\n",
        "       .withColumn(\"prestamo\", array_distinct(col(\"prestamo\")))\n",
        "\n",
        "# Dividir las cuentas y expandir las filas para cada producto\n",
        "df_expanded = df.withColumn(\"cuenta_corriente\", explode(col(\"cuenta_corriente\"))) \\\n",
        "                .withColumn(\"tarjeta_credito\", explode(col(\"tarjeta_credito\"))) \\\n",
        "                .withColumn(\"prestamo\", explode(col(\"prestamo\")))\n",
        "\n",
        "df_expanded.show()"
      ],
      "metadata": {
        "id": "ZL9aS1Z9hX0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, explode, col, row_number, when\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"ExpandRows\").getOrCreate()\n",
        "\n",
        "# Supongamos que tienes un DataFrame con tus datos\n",
        "data = [\n",
        "    (7101287883, ['1010225434452', '510035767005', '72278398'], ['1234', '5678'], ['9876', '5432']),\n",
        "    (5102225434452, ['510035767005', '72278398'], ['4321', '8765'], ['6543', '2109'])\n",
        "]\n",
        "\n",
        "columns = ['RUT', 'cuenta_corriente', 'tarjeta_credito', 'prestamo']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Dividir las cuentas y expandir las filas para cada producto\n",
        "df_expanded = df.withColumn(\"cuenta_corriente\", explode(col(\"cuenta_corriente\"))) \\\n",
        "                .withColumn(\"tarjeta_credito\", explode(col(\"tarjeta_credito\"))) \\\n",
        "                .withColumn(\"prestamo\", explode(col(\"prestamo\")))\n",
        "\n",
        "# Crear una ventana para numerar las filas\n",
        "window_spec = Window.partitionBy(\"RUT\").orderBy(\"RUT\")\n",
        "\n",
        "# Numerar las filas y dejar en blanco las repeticiones\n",
        "df_expanded = df_expanded.withColumn(\"row_num\", row_number().over(window_spec))\n",
        "\n",
        "df_final = df_expanded.withColumn(\"cuenta_corriente\",\n",
        "                                  when(col(\"row_num\") == 1, col(\"cuenta_corriente\")).otherwise(\"\")) \\\n",
        "                      .withColumn(\"tarjeta_credito\",\n",
        "                                  when(col(\"row_num\") == 1, col(\"tarjeta_credito\")).otherwise(\"\")) \\\n",
        "                      .withColumn(\"prestamo\",\n",
        "                                  when(col(\"row_num\") == 1, col(\"prestamo\")).otherwise(\"\")) \\\n",
        "                      .drop(\"row_num\")\n",
        "\n",
        "df_final.show()"
      ],
      "metadata": {
        "id": "fkbOMuPofIF8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QiN3K1COhWkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, explode, col, row_number, when\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"ExpandRows\").getOrCreate()\n",
        "\n",
        "# Supongamos que tienes un DataFrame con tus datos\n",
        "data = [\n",
        "    (7101287883, ['1010225434452', '510035767005', '72278398'], ['1234', '5678'], ['9876', '5432']),\n",
        "    (5102225434452, ['510035767005', '72278398'], ['4321', '8765'], ['6543', '2109'])\n",
        "]\n",
        "\n",
        "columns = ['RUT', 'cuenta_corriente', 'tarjeta_credito', 'prestamo']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Dividir las cuentas y expandir las filas para cada producto\n",
        "df_expanded = df.withColumn(\"cuenta_corriente\", explode(col(\"cuenta_corriente\"))) \\\n",
        "                .withColumn(\"tarjeta_credito\", explode(col(\"tarjeta_credito\"))) \\\n",
        "                .withColumn(\"prestamo\", explode(col(\"prestamo\")))\n",
        "\n",
        "# Crear una ventana para numerar las filas\n",
        "window_spec = Window.partitionBy(\"RUT\").orderBy(\"RUT\")\n",
        "\n",
        "# Numerar las filas y dejar en blanco las repeticiones\n",
        "df_expanded = df_expanded.withColumn(\"row_num\", row_number().over(window_spec))\n",
        "\n",
        "df_final = df_expanded.withColumn(\"cuenta_corriente\",\n",
        "                                  when(col(\"row_num\") == 1, col(\"cuenta_corriente\")).otherwise(\"\")) \\\n",
        "                      .withColumn(\"tarjeta_credito\",\n",
        "                                  when(col(\"row_num\") == 1, col(\"tarjeta_credito\")).otherwise(\"\")) \\\n",
        "                      .withColumn(\"prestamo\",\n",
        "                                  when(col(\"row_num\") == 1, col(\"prestamo\")).otherwise(\"\")) \\\n",
        "                      .drop(\"row_num\")\n",
        "\n",
        "df_final.show()"
      ],
      "metadata": {
        "id": "OmukygLndH9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, explode, col, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"ExpandRows\").getOrCreate()\n",
        "\n",
        "# Supongamos que tienes un DataFrame con tus datos\n",
        "data = [\n",
        "    (7101287883, ['1010225434452', '510035767005', '72278398'], ['1234', '5678'], ['9876', '5432']),\n",
        "    (5102225434452, ['510035767005', '72278398'], ['4321', '8765'], ['6543', '2109'])\n",
        "]\n",
        "\n",
        "columns = ['RUT', 'cuenta_corriente', 'tarjeta_credito', 'prestamo']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Dividir las cuentas y expandir las filas para cada producto\n",
        "df_expanded = df.withColumn(\"cuenta_corriente\", explode(col(\"cuenta_corriente\"))) \\\n",
        "                .withColumn(\"tarjeta_credito\", explode(col(\"tarjeta_credito\"))) \\\n",
        "                .withColumn(\"prestamo\", explode(col(\"prestamo\")))\n",
        "\n",
        "# Crear una ventana para numerar las filas\n",
        "window_spec = Window.partitionBy(\"RUT\").orderBy(\"RUT\")\n",
        "\n",
        "# Numerar las filas y dejar en blanco las repeticiones\n",
        "df_expanded = df_expanded.withColumn(\"row_num\", row_number().over(window_spec))\n",
        "\n",
        "df_final = df_expanded.withColumn(\"cuenta_corriente\",\n",
        "                                  col(\"cuenta_corriente\").when(col(\"row_num\") == 1, col(\"cuenta_corriente\")).otherwise(\"\")) \\\n",
        "                      .withColumn(\"tarjeta_credito\",\n",
        "                                  col(\"tarjeta_credito\").when(col(\"row_num\") == 1, col(\"tarjeta_credito\")).otherwise(\"\")) \\\n",
        "                      .withColumn(\"prestamo\",\n",
        "                                  col(\"prestamo\").when(col(\"row_num\") == 1, col(\"prestamo\")).otherwise(\"\")) \\\n",
        "                      .drop(\"row_num\")\n",
        "\n",
        "df_final.show()"
      ],
      "metadata": {
        "id": "5jy-JWAobO61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, explode, col, regexp_replace\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"ExpandRows\").getOrCreate()\n",
        "\n",
        "# Supongamos que tienes un DataFrame con tus datos\n",
        "data = [\n",
        "    (7101287883, '[1010225434452, 510035767005, 72278398]', '[1234, 5678]', '[9876, 5432]'),\n",
        "    (5102225434452, '[510035767005, 72278398]', '[4321, 8765]', '[6543, 2109]')\n",
        "]\n",
        "\n",
        "columns = ['RUT', 'cuenta_corriente', 'tarjeta_credito', 'prestamo']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Eliminar los corchetes y dividir las cuentas\n",
        "df = df.withColumn(\"cuenta_corriente\", regexp_replace(col(\"cuenta_corriente\"), \"[\\\\[\\\\]]\", \"\")) \\\n",
        "       .withColumn(\"tarjeta_credito\", regexp_replace(col(\"tarjeta_credito\"), \"[\\\\[\\\\]]\", \"\")) \\\n",
        "       .withColumn(\"prestamo\", regexp_replace(col(\"prestamo\"), \"[\\\\[\\\\]]\", \"\"))\n",
        "\n",
        "# Dividir las cuentas y expandir las filas para cada producto\n",
        "df_expanded = df.withColumn(\"cuenta_corriente\", explode(split(col(\"cuenta_corriente\"), \", \"))) \\\n",
        "                .withColumn(\"tarjeta_credito\", explode(split(col(\"tarjeta_credito\"), \", \"))) \\\n",
        "                .withColumn(\"prestamo\", explode(split(col(\"prestamo\"), \", \")))\n",
        "\n",
        "df_expanded.show()"
      ],
      "metadata": {
        "id": "U1QvwdnVZps6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import when, col, explode, array, collect_list\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"PivotExample\").getOrCreate()\n",
        "\n",
        "# Ejemplo de DataFrame\n",
        "data = [\n",
        "    (\"12345678-9\", \"cuenta mx\", \"C001\"),\n",
        "    (\"12345678-9\", \"cuenta lucas\", \"C002\"),\n",
        "    (\"98765432-1\", \"tarjeta de debito\", \"C003\"),\n",
        "    (\"98765432-1\", \"cuenta corriente\", \"C004\"),\n",
        "    (\"12345678-9\", \"cuenta dorada\", \"C005\")\n",
        "]\n",
        "\n",
        "columns = [\"rut\", \"descripcion\", \"contrato\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Transformar las descripciones\n",
        "df = df.withColumn(\"descripcion\",\n",
        "                   when(df.descripcion.isin(\"cuenta mx\"), \"cuenta corriente\")\n",
        "                   .when(df.descripcion.isin(\"cuenta lucas\", \"cuenta dorada\"), \"cuenta vista\")\n",
        "                   .otherwise(df.descripcion))\n",
        "\n",
        "# Agrupar y explotar las filas\n",
        "df_grouped = df.groupBy(\"rut\").agg(\n",
        "    collect_list(when(col(\"descripcion\") == \"cuenta corriente\", col(\"contrato\"))).alias(\"cuenta_corriente\"),\n",
        "    collect_list(when(col(\"descripcion\") == \"cuenta vista\", col(\"contrato\"))).alias(\"cuenta_vista\"),\n",
        "    collect_list(when(col(\"descripcion\") == \"tarjeta de debito\", col(\"contrato\"))).alias(\"tarjeta_debito\")\n",
        ")\n",
        "\n",
        "# Explode las listas para que cada contrato aparezca en una fila separada\n",
        "df_exploded = df_grouped.select(\"rut\", explode(array(\"cuenta_corriente\", \"cuenta_vista\", \"tarjeta_debito\")).alias(\"contratos\"))\n",
        "\n",
        "# Crear un DataFrame final con las columnas separadas\n",
        "df_final = df_exploded.select(\"rut\",\n",
        "                              col(\"contratos\")[0].alias(\"cuenta_corriente\"),\n",
        "                              col(\"contratos\")[1].alias(\"cuenta_vista\"),\n",
        "                              col(\"contratos\")[2].alias(\"tarjeta_debito\"))\n",
        "\n",
        "# Mostrar el resultado\n",
        "df_final.show()"
      ],
      "metadata": {
        "id": "N5iWCk85Mwsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import when, col, explode, array, posexplode, lit\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"PivotExample\").getOrCreate()\n",
        "\n",
        "# Ejemplo de DataFrame\n",
        "data = [\n",
        "    (\"12345678-9\", \"cuenta mx\", \"C001\"),\n",
        "    (\"12345678-9\", \"cuenta lucas\", \"C002\"),\n",
        "    (\"98765432-1\", \"tarjeta de debito\", \"C003\"),\n",
        "    (\"98765432-1\", \"cuenta corriente\", \"C004\"),\n",
        "    (\"12345678-9\", \"cuenta dorada\", \"C005\")\n",
        "]\n",
        "\n",
        "columns = [\"rut\", \"descripcion\", \"contrato\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Transformar las descripciones\n",
        "df = df.withColumn(\"descripcion\",\n",
        "                   when(df.descripcion.isin(\"cuenta mx\"), \"cuenta corriente\")\n",
        "                   .when(df.descripcion.isin(\"cuenta lucas\", \"cuenta dorada\"), \"cuenta vista\")\n",
        "                   .otherwise(df.descripcion))\n",
        "\n",
        "# Agrupar y explotar las filas\n",
        "df_grouped = df.groupBy(\"rut\").agg(\n",
        "    collect_list(when(col(\"descripcion\") == \"cuenta corriente\", col(\"contrato\"))).alias(\"cuenta_corriente\"),\n",
        "    collect_list(when(col(\"descripcion\") == \"cuenta vista\", col(\"contrato\"))).alias(\"cuenta_vista\"),\n",
        "    collect_list(when(col(\"descripcion\") == \"tarjeta de debito\", col(\"contrato\"))).alias(\"tarjeta_debito\")\n",
        ")\n",
        "\n",
        "# Explode las listas para que cada contrato aparezca en una fila separada\n",
        "df_exploded_corriente = df_grouped.select(\"rut\", explode(\"cuenta_corriente\").alias(\"contrato\")).withColumn(\"descripcion\", lit(\"cuenta corriente\"))\n",
        "df_exploded_vista = df_grouped.select(\"rut\", explode(\"cuenta_vista\").alias(\"contrato\")).withColumn(\"descripcion\", lit(\"cuenta vista\"))\n",
        "df_exploded_debito = df_grouped.select(\"rut\", explode(\"tarjeta_debito\").alias(\"contrato\")).withColumn(\"descripcion\", lit(\"tarjeta de debito\"))\n",
        "\n",
        "# Unir todos los DataFrames explotados\n",
        "df_exploded = df_exploded_corriente.union(df_exploded_vista).union(df_exploded_debito)\n",
        "\n",
        "# Pivotar el DataFrame final\n",
        "pivot_df = df_exploded.groupBy(\"rut\").pivot(\"descripcion\").agg(first(\"contrato\"))\n",
        "\n",
        "# Mostrar el resultado\n",
        "pivot_df.show()"
      ],
      "metadata": {
        "id": "m30iQXqhMzVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, explode, col\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"ExpandRows\").getOrCreate()\n",
        "\n",
        "# Supongamos que tienes un DataFrame con tus datos\n",
        "data = [\n",
        "    (7101287883, '1010225434452, 510035767005, 72278398', '1234, 5678', '9876, 5432'),\n",
        "    (5102225434452, '510035767005, 72278398', '4321, 8765', '6543, 2109')\n",
        "]\n",
        "\n",
        "columns = ['RUT', 'cuenta_corriente', 'tarjeta_credito', 'prestamo']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Dividir las cuentas y expandir las filas para cada producto\n",
        "df_expanded = df.withColumn(\"cuenta_corriente\", explode(split(col(\"cuenta_corriente\"), \", \"))) \\\n",
        "                .withColumn(\"tarjeta_credito\", explode(split(col(\"tarjeta_credito\"), \", \"))) \\\n",
        "                .withColumn(\"prestamo\", explode(split(col(\"prestamo\"), \", \")))\n",
        "\n",
        "df_expanded.show()"
      ],
      "metadata": {
        "id": "axiU8Ir4M36v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sección nueva"
      ],
      "metadata": {
        "id": "vF0vNE3UM4_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lsvIkUlHM8zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Supongamos que tus DataFrames se llaman df_diciembre, df_campana (enero a abril) y df_respuestas\n",
        "\n",
        "# Primero, une df_diciembre con df_campana para encontrar los clientes que estuvieron en campaña en enero a abril\n",
        "df_unido = df_diciembre.join(df_campana, on='rut', how='inner')\n",
        "\n",
        "# Luego, une df_unido con df_respuestas para encontrar los clientes que respondieron\n",
        "df_unido = df_unido.join(df_respuestas, on=['rut', 'mes'], how='left_anti')\n",
        "\n",
        "# En este punto, df_unido contiene solo las filas de los clientes que no respondieron\n",
        "\n",
        "# Ordena el DataFrame por rut y mes, y luego guarda solo la última fila para cada rut\n",
        "df_ultima_campana = df_unido.orderBy('rut', 'mes', ascending=False).dropDuplicates(['rut'])\n"
      ],
      "metadata": {
        "id": "4Re9Cxoj6Bbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, first\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Inicializar SparkSession\n",
        "spark = SparkSession.builder.appName(\"AnalisisClientes\").getOrCreate()\n",
        "\n",
        "# Cargar los archivos\n",
        "df_clientes = spark.read.csv('clientes.csv', header=True, inferSchema=True)\n",
        "df_campanas = spark.read.csv('campanas.csv', header=True, inferSchema=True)\n",
        "df_respuestas = spark.read.csv('respuestas.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Asumiendo que 'df_respuestas' tiene las columnas 'id_cliente', 'mes_respuesta' y 'respondio'\n",
        "# Crear una ventana particionada por 'id_cliente' y ordenada por 'mes_respuesta' en orden descendente\n",
        "windowSpec = Window.partitionBy(\"id_cliente\").orderBy(col(\"mes_respuesta\").desc())\n",
        "\n",
        "# Usar la función 'first' para obtener la última respuesta de cada cliente\n",
        "df_ultima_respuesta = df_respuestas.withColumn(\"ultima_respuesta\", first(\"respondio\").over(windowSpec))\n",
        "\n",
        "# Eliminar duplicados para obtener una fila por cliente\n",
        "df_ultima_respuesta = df_ultima_respuesta.dropDuplicates([\"id_cliente\"])\n",
        "\n",
        "# Unir con el dataframe de clientes para obtener la información completa\n",
        "df_resultado = df_clientes.join(df_ultima_respuesta, \"id_cliente\")\n",
        "\n",
        "# Seleccionar las columnas relevantes\n",
        "df_resultado = df_resultado.select(\"id_cliente\", \"ultima_respuesta\")\n",
        "\n",
        "# Guardar o procesar los resultados según sea necesario\n",
        "df_resultado.show()"
      ],
      "metadata": {
        "id": "aT_DFaux3x91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Definimos el número de filas deseadas y el máximo de sucursales distintas\n",
        "num_filas = 262\n",
        "max_sucursales = 25\n",
        "\n",
        "# Suponiendo que 'df' es tu DataFrame, 'sucursales' y 'rut' son las columnas de tu interés\n",
        "# Inicializamos la muestra completa vacía y la semilla\n",
        "muestra_completa = None\n",
        "semilla = 42  # Puedes iniciar con cualquier número\n",
        "\n",
        "# Función para verificar las condiciones de la muestra\n",
        "def verificar_muestra(df, num_filas, max_sucursales):\n",
        "    cuenta_sucursales = df.select('sucursales').distinct().count()\n",
        "    cuenta_filas = df.count()\n",
        "    return cuenta_filas == num_filas and cuenta_sucursales <= max_sucursales\n",
        "\n",
        "# Iteramos sobre diferentes semillas hasta cumplir con las condiciones\n",
        "while not verificar_muestra(muestra_completa, num_filas, max_sucursales):\n",
        "    # Seleccionamos hasta 25 sucursales distintas de manera aleatoria\n",
        "    sucursales_distintas = df.select('sucursales').distinct().orderBy(F.rand(seed=semilla)).limit(max_sucursales)\n",
        "\n",
        "    # Creamos una muestra estratificada con las sucursales seleccionadas\n",
        "    df_filtrado = df.join(sucursales_distintas, 'sucursales', 'inner')\n",
        "    muestra_estratificada = df_filtrado.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(num_filas)\n",
        "\n",
        "    # Si la muestra estratificada tiene menos de 262 filas, rellenamos con más filas\n",
        "    if muestra_estratificada.count() < num_filas:\n",
        "        # Obtenemos los 'rut' que ya están en la muestra\n",
        "        ruts_en_muestra = muestra_estratificada.select('rut').distinct()\n",
        "\n",
        "        # Filtramos el DataFrame filtrado para excluir los 'rut' ya presentes\n",
        "        df_restante = df_filtrado.join(ruts_en_muestra, 'rut', 'left_anti')\n",
        "\n",
        "        # Calculamos cuántas filas faltan para llegar a 262\n",
        "        filas_faltantes = num_filas - muestra_estratificada.count()\n",
        "\n",
        "        # Tomamos filas adicionales sin repetir 'rut'\n",
        "        filas_adicionales = df_restante.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(filas_faltantes)\n",
        "\n",
        "        # Unimos las filas adicionales a la muestra estratificada\n",
        "        muestra_completa = muestra_estratificada.union(filas_adicionales)\n",
        "    else:\n",
        "        muestra_completa = muestra_estratificada\n",
        "\n",
        "    # Incrementamos la semilla para la siguiente iteración\n",
        "    semilla += 1\n",
        "\n",
        "# Mostrar la muestra completa\n",
        "muestra_completa.show()"
      ],
      "metadata": {
        "id": "JuTKfX-N5QoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Establecemos una semilla para la reproducibilidad\n",
        "semilla = 42\n",
        "\n",
        "# Suponiendo que 'df' es tu DataFrame, 'sucursales' y 'rut' son las columnas de tu interés\n",
        "# Paso 1: Seleccionamos hasta 25 sucursales distintas de manera aleatoria\n",
        "sucursales_distintas = df.select('sucursales').distinct().orderBy(F.rand(seed=semilla)).limit(25)\n",
        "\n",
        "# Paso 2: Creamos una muestra estratificada con las sucursales seleccionadas\n",
        "df_filtrado = df.join(sucursales_distintas, 'sucursales', 'inner')\n",
        "muestra_estratificada = df_filtrado.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(262)\n",
        "\n",
        "# Paso 3: Verificamos si la muestra alcanza las 262 filas\n",
        "if muestra_estratificada.count() < 262:\n",
        "    # Obtenemos los 'rut' que ya están en la muestra\n",
        "    ruts_en_muestra = muestra_estratificada.select('rut').distinct()\n",
        "\n",
        "    # Filtramos el DataFrame original para excluir los 'rut' ya presentes y mantener solo las sucursales seleccionadas\n",
        "    df_restante = df.join(sucursales_distintas, 'sucursales', 'inner').join(ruts_en_muestra, 'rut', 'left_anti')\n",
        "\n",
        "    # Calculamos cuántas filas faltan para llegar a 262\n",
        "    filas_faltantes = 262 - muestra_estratificada.count()\n",
        "\n",
        "    # Tomamos filas adicionales sin repetir 'rut' y que pertenezcan a las sucursales ya seleccionadas\n",
        "    filas_adicionales = df_restante.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(filas_faltantes)\n",
        "\n",
        "    # Unimos las filas adicionales a la muestra estratificada\n",
        "    muestra_completa = muestra_estratificada.union(filas_adicionales)\n",
        "\n",
        "# Mostrar la muestra completa\n",
        "muestra_completa.show()"
      ],
      "metadata": {
        "id": "ih0IRr-X3zjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Establecemos una semilla para la reproducibilidad\n",
        "semilla = 42\n",
        "\n",
        "# Suponiendo que 'df' es tu DataFrame y 'sucursales' es el nombre de tu columna\n",
        "# Paso 1: Estratificar el DataFrame\n",
        "estratos = df.select('sucursales').distinct()\n",
        "\n",
        "# Paso 2: Seleccionar hasta 25 sucursales aleatorias\n",
        "sucursales_aleatorias = estratos.orderBy(F.rand(seed=semilla)).limit(25)\n",
        "\n",
        "# Paso 3: Muestrear dentro de cada estrato\n",
        "# Creamos un diccionario para la muestra estratificada\n",
        "fracciones = {row['sucursales']: 1.0 for row in sucursales_aleatorias.collect()}\n",
        "\n",
        "# Realizamos la muestra estratificada\n",
        "muestra_estratificada = df.stat.sampleBy('sucursales', fracciones, seed=semilla)\n",
        "\n",
        "# Paso 4: Combinar las muestras hasta alcanzar 262 filas\n",
        "muestra_final = muestra_estratificada.limit(262)\n",
        "\n",
        "# Mostrar las filas seleccionadas\n",
        "muestra_final.show()"
      ],
      "metadata": {
        "id": "m7UPjHPutKEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Establecemos una semilla para la reproducibilidad\n",
        "semilla = 42\n",
        "\n",
        "# Suponiendo que 'df' es tu DataFrame, 'sucursales' y 'rut' son las columnas de tu interés\n",
        "# Paso 1: Seleccionamos hasta 25 sucursales distintas de manera aleatoria\n",
        "sucursales_distintas = df.select('sucursales').distinct().orderBy(F.rand(seed=semilla)).limit(25)\n",
        "\n",
        "# Paso 2: Creamos una muestra estratificada con las sucursales seleccionadas\n",
        "df_filtrado = df.join(sucursales_distintas, 'sucursales', 'inner')\n",
        "muestra_estratificada = df_filtrado.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(262)\n",
        "\n",
        "# Paso 3: Verificamos si la muestra alcanza las 262 filas\n",
        "if muestra_estratificada.count() < 262:\n",
        "    # Obtenemos los 'rut' que ya están en la muestra\n",
        "    ruts_en_muestra = muestra_estratificada.select('rut').distinct()\n",
        "\n",
        "    # Filtramos el DataFrame original para excluir los 'rut' ya presentes\n",
        "    df_restante = df_filtrado.join(ruts_en_muestra, 'rut', 'left_anti')\n",
        "\n",
        "    # Calculamos cuántas filas faltan para llegar a 262\n",
        "    filas_faltantes = 262 - muestra_estratificada.count()\n",
        "\n",
        "    # Tomamos filas adicionales sin repetir 'rut'\n",
        "    filas_adicionales = df_restante.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(filas_faltantes)\n",
        "\n",
        "    # Unimos las filas adicionales a la muestra estratificada\n",
        "    muestra_completa = muestra_estratificada.union(filas_adicionales)\n",
        "\n",
        "# Mostrar la muestra completa\n",
        "muestra_completa.show()"
      ],
      "metadata": {
        "id": "AMwgEcs_xosI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Establecemos una semilla para la reproducibilidad\n",
        "semilla = 42\n",
        "\n",
        "# Suponiendo que 'df' es tu DataFrame, 'sucursales' y 'rut' son las columnas de tu interés\n",
        "# Paso 1: Seleccionamos hasta 25 sucursales distintas de manera aleatoria\n",
        "sucursales_distintas = df.select('sucursales').distinct().orderBy(F.rand(seed=semilla)).limit(25)\n",
        "\n",
        "# Paso 2: Creamos una muestra estratificada con las sucursales seleccionadas\n",
        "df_filtrado = df.join(sucursales_distintas, 'sucursales', 'inner')\n",
        "muestra_estratificada = df_filtrado.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(262)\n",
        "\n",
        "# Paso 3: Verificamos si la muestra alcanza las 262 filas\n",
        "if muestra_estratificada.count() < 262:\n",
        "    # Obtenemos los 'rut' que ya están en la muestra\n",
        "    ruts_en_muestra = muestra_estratificada.select('rut').distinct()\n",
        "\n",
        "    # Filtramos el DataFrame original para excluir los 'rut' ya presentes y mantener solo las sucursales seleccionadas\n",
        "    df_restante = df_filtrado.join(ruts_en_muestra, 'rut', 'left_anti')\n",
        "\n",
        "    # Calculamos cuántas filas faltan para llegar a 262\n",
        "    filas_faltantes = 262 - muestra_estratificada.count()\n",
        "\n",
        "    # Tomamos filas adicionales sin repetir 'rut' y que pertenezcan a las sucursales ya seleccionadas\n",
        "    filas_adicionales = df_restante.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(filas_faltantes)\n",
        "\n",
        "    # Unimos las filas adicionales a la muestra estratificada\n",
        "    muestra_completa = muestra_estratificada.union(filas_adicionales)\n",
        "\n",
        "# Mostrar la muestra completa\n",
        "muestra_completa.show()"
      ],
      "metadata": {
        "id": "hi62UmZY1O5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Establecemos una semilla para la reproducibilidad\n",
        "semilla = 42\n",
        "\n",
        "# Suponiendo que 'df' es tu DataFrame, 'sucursales' y 'rut' son las columnas de tu interés\n",
        "# Paso 1: Seleccionamos hasta 25 sucursales distintas de manera aleatoria\n",
        "sucursales_distintas = df.select('sucursales').distinct().orderBy(F.rand(seed=semilla)).limit(25)\n",
        "\n",
        "# Paso 2: Creamos una muestra estratificada con las sucursales seleccionadas\n",
        "df_filtrado = df.join(sucursales_distintas, 'sucursales', 'inner')\n",
        "muestra_estratificada = df_filtrado.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(262)\n",
        "\n",
        "# Paso 3: Verificamos si la muestra alcanza las 262 filas\n",
        "if muestra_estratificada.count() < 262:\n",
        "    # Obtenemos los 'rut' que ya están en la muestra\n",
        "    ruts_en_muestra = muestra_estratificada.select('rut').distinct()\n",
        "\n",
        "    # Filtramos el DataFrame original para excluir los 'rut' ya presentes y mantener solo las sucursales seleccionadas\n",
        "    df_restante = df.join(sucursales_distintas, 'sucursales', 'inner').join(ruts_en_muestra, 'rut', 'left_anti')\n",
        "\n",
        "    # Calculamos cuántas filas faltan para llegar a 262\n",
        "    filas_faltantes = 262 - muestra_estratificada.count()\n",
        "\n",
        "    # Tomamos filas adicionales sin repetir 'rut' y que pertenezcan a las sucursales ya seleccionadas\n",
        "    filas_adicionales = df_restante.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(filas_faltantes)\n",
        "\n",
        "    # Unimos las filas adicionales a la muestra estratificada\n",
        "    muestra_completa = muestra_estratificada.union(filas_adicionales)\n",
        "\n",
        "# Mostrar la muestra completa\n",
        "muestra_completa.show()"
      ],
      "metadata": {
        "id": "Ka4wIWfj3aEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "weZHrqBd3wen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Establecemos una semilla para la reproducibilidad\n",
        "semilla = 42\n",
        "\n",
        "# Suponiendo que 'df' es tu DataFrame y 'sucursales' es el nombre de tu columna\n",
        "# Paso 1: Seleccionamos un máximo de 25 sucursales distintas de manera aleatoria\n",
        "sucursales_distintas = df.select('sucursales').distinct().orderBy(F.rand(seed=semilla)).limit(25)\n",
        "\n",
        "# Mostrar las sucursales seleccionadas\n",
        "sucursales_distintas.show()"
      ],
      "metadata": {
        "id": "u8Vfnvq8Inao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Establecemos una semilla para la reproducibilidad\n",
        "semilla = 42\n",
        "\n",
        "# Suponiendo que 'df' es tu DataFrame y 'sucursales' es el nombre de tu columna\n",
        "# Paso 1: Seleccionamos una muestra aleatoria de filas del DataFrame\n",
        "muestra_aleatoria = df.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(262)\n",
        "\n",
        "# Paso 2: Creamos una ventana para particionar por sucursal y ordenar por aleatoriedad\n",
        "ventana = Window.partitionBy('sucursales').orderBy(F.rand(seed=semilla))\n",
        "\n",
        "# Paso 3: Agregamos una columna de índice basada en la ventana\n",
        "muestra_con_indice = muestra_aleatoria.withColumn('indice', F.row_number().over(ventana))\n",
        "\n",
        "# Paso 4: Obtenemos las sucursales distintas con un índice de 1 (la primera fila de cada sucursal)\n",
        "sucursales_distintas = muestra_con_indice.filter(col('indice') == 1).select('sucursales')\n",
        "\n",
        "# Paso 5: Si hay más de 25 sucursales distintas, seleccionamos 25 de ellas aleatoriamente\n",
        "if sucursales_distintas.count() > 25:\n",
        "    sucursales_distintas = sucursales_distintas.orderBy(F.rand(seed=semilla)).limit(25)\n",
        "\n",
        "# Paso 6: Filtramos la muestra original para incluir solo filas de las sucursales seleccionadas\n",
        "muestra_final = muestra_con_indice.join(sucursales_distintas, 'sucursales', 'inner').drop('indice')\n",
        "\n",
        "# Mostrar las filas seleccionadas\n",
        "muestra_final.show()"
      ],
      "metadata": {
        "id": "kidCTZbQsRvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = spark.read.format('csv').option('header','true').option('delimiter', ',').load('/content/sample_data/mnist_test.csv')"
      ],
      "metadata": {
        "id": "84xlzw40QQb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columnas = df.columns\n",
        "df2 = df.select(columnas[10:12])"
      ],
      "metadata": {
        "id": "AOceAfxHTQc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpihPVyWUZrj",
        "outputId": "50629028-c571-4395-97d8-20a64657376d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+\n",
            "|010|011|\n",
            "+---+---+\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "+---+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "# Tamaño de la población\n",
        "N = 2799\n",
        "\n",
        "# Valores de Z para diferentes niveles de confianza\n",
        "Z_values = [1.70, 1.75, 1.81, 1.88, 1.96, 2.05, 2.17, 2.33]  # 91%, 92%, 93%, 94%, 95%, 96%, 97%, 98% de confianza\n",
        "\n",
        "# Valores de E para diferentes márgenes de error\n",
        "E_values = [0.01, 0.02, 0.03, 0.04, 0.05]  # 1%, 2%, 3%, 4%, 5% de margen de error\n",
        "\n",
        "# Crear una figura y un conjunto de subgráficos\n",
        "fig, axs = plt.subplots(len(Z_values), len(E_values), figsize=(15, 15))\n",
        "\n",
        "# Calcular y graficar el tamaño de la muestra para cada combinación de Z y E\n",
        "for i, Z in enumerate(Z_values):\n",
        "    for j, E in enumerate(E_values):\n",
        "        X = Z**2 * 0.25  # Varianza máxima para una distribución binomial\n",
        "        n = N * X / ((N - 1) * E**2 + X)\n",
        "        axs[i, j].plot([0, N], [n, n], 'r-')\n",
        "        axs[i, j].set_title(f'IC={int(Z*10)}%, Error={int(E*100)}%, Muestra={int(n)}')\n",
        "        axs[i, j].set_xlabel('Tamaño de la población')\n",
        "        axs[i, j].set_ylabel('Tamaño de la muestra')\n",
        "\n",
        "# Ajustar el diseño\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "MsKnph8rmsrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "from scipy.stats import fisher_exact\n",
        "\n",
        "# Definimos una función para calcular el valor p de la prueba exacta de Fisher\n",
        "def fisher_test(df1_count, df2_count, df1_total, df2_total):\n",
        "    contingency_table = [[df1_count, df2_count], [df1_total - df1_count, df2_total - df2_count]]\n",
        "    _, p_value = fisher_exact(contingency_table)\n",
        "    return float(p_value)\n",
        "\n",
        "# Registramos la función como una udf\n",
        "fisher_test_udf = udf(fisher_test, FloatType())\n",
        "\n",
        "# Supongamos que 'var_cat' es tu variable categórica y que ya has agregado las categorías que faltan y las has llenado con ceros\n",
        "\n",
        "# Calculamos el total de cada categoría en cada dataframe\n",
        "df1_total = df1.count()\n",
        "df2_total = df2.count()\n",
        "\n",
        "# Aplicamos la prueba exacta de Fisher a cada categoría\n",
        "for category in all_categories:\n",
        "    df1_count = df1.filter(df1['var_cat'] == category).count()\n",
        "    df2_count = df2.filter(df2['var_cat'] == category).count()\n",
        "    p_value = fisher_test_udf(df1_count, df2_count, df1_total, df2_total)\n",
        "    print(\"Categoría:\", category)\n",
        "    print(\"P-value:\", p_value)\n"
      ],
      "metadata": {
        "id": "E6HfozV_xRfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Carga los datos en DataFrames de Spark\n",
        "df_transactions = spark.read.table(\"transactions\")\n",
        "df_pedt001 = spark.read.table(\"pedt001\")\n",
        "\n",
        "# Realiza la unión interna\n",
        "df = df_transactions.alias('a').join(df_pedt001.alias('b'), df_transactions['primary_id'] == df_pedt001['penumper'])\n",
        "\n",
        "# Filtra los datos\n",
        "df = df.filter(\n",
        "    (F.substring(df['value_date'], 1, 10).between('2024-04-01', '2024-04-30')) &\n",
        "    (df['product_source'] == '70') &\n",
        "    (df['sa_subproduct'] == '1910') &\n",
        "    (df['credit_debit_code'] == 'H') &\n",
        "    (df['a.data_date_part'].between('2024-04-01', '2024-04-30')) &\n",
        "    (~df['trx_source'].isin(['0391', '7001', '0956', '1872']))\n",
        ")\n",
        "\n",
        "# Selecciona las columnas deseadas y realiza las transformaciones necesarias\n",
        "df = df.select(\n",
        "    df['primary_id'],\n",
        "    df['run_date'],\n",
        "    df['txn_source'],\n",
        "    df['sa_subproduct'].alias('subprod'),\n",
        "    F.substring(df['account_id'], 15, 12).alias('cuenta'),\n",
        "    df['txn_amount'].cast('bigint').alias('monto')\n",
        ")"
      ],
      "metadata": {
        "id": "9ugaTxRzaPVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Establecemos una semilla para la reproducibilidad\n",
        "semilla = 42\n",
        "\n",
        "# Suponiendo que 'df' es tu DataFrame y 'sucursales' es el nombre de tu columna\n",
        "# Paso 1: Tomamos una muestra aleatoria de 262 filas del DataFrame\n",
        "muestra_aleatoria = df.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(262)\n",
        "\n",
        "# Paso 2: Creamos una ventana para particionar por sucursal y ordenar por aleatoriedad\n",
        "ventana = Window.partitionBy('sucursales').orderBy(F.rand(seed=semilla))\n",
        "\n",
        "# Paso 3: Agregamos una columna de índice basada en la ventana\n",
        "muestra_con_indice = muestra_aleatoria.withColumn('indice', F.row_number().over(ventana))\n",
        "\n",
        "# Paso 4: Filtramos las filas para tener un máximo de 25 sucursales distintas\n",
        "muestra_final = muestra_con_indice.filter(col('indice') <= 25)\n",
        "\n",
        "# Paso 5: Si la muestra final tiene menos de 262 filas, ajustamos la fracción y repetimos\n",
        "while muestra_final.count() < 262:\n",
        "    fraccion_ajustada = (262 / muestra_aleatoria.count()) * 2  # Ajustamos la fracción para obtener más filas\n",
        "    muestra_aleatoria = df.sample(withReplacement=False, fraction=fraccion_ajustada, seed=semilla).limit(262)\n",
        "    muestra_con_indice = muestra_aleatoria.withColumn('indice', F.row_number().over(ventana))\n",
        "    muestra_final = muestra_con_indice.filter(col('indice') <= 25)\n",
        "\n",
        "# Mostrar las filas seleccionadas\n",
        "muestra_final.show()"
      ],
      "metadata": {
        "id": "p0yus_SCndip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Establecemos una semilla para la reproducibilidad\n",
        "semilla = 42\n",
        "\n",
        "# Suponiendo que 'df' es tu DataFrame y 'sucursales' es el nombre de tu columna\n",
        "# Paso 1: Seleccionamos una muestra aleatoria de 262 filas del DataFrame\n",
        "muestra_aleatoria = df.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(262)\n",
        "\n",
        "# Paso 2: Obtenemos las sucursales distintas de la muestra aleatoria\n",
        "sucursales_distintas = muestra_aleatoria.select('sucursales').distinct()\n",
        "\n",
        "# Paso 3: Si hay más de 25 sucursales distintas, seleccionamos 25 de ellas aleatoriamente\n",
        "if sucursales_distintas.count() > 25:\n",
        "    sucursales_distintas = sucursales_distintas.orderBy(F.rand(seed=semilla)).limit(25)\n",
        "\n",
        "# Paso 4: Filtramos la muestra original para incluir solo filas de las sucursales seleccionadas\n",
        "df_filtrado = df.join(sucursales_distintas, 'sucursales', 'inner')\n",
        "\n",
        "# Paso 5: Si la muestra filtrada es menor a 262 filas, ajustamos la fracción y repetimos\n",
        "while df_filtrado.count() < 262:\n",
        "    fraccion_ajustada = (262 / df.count()) * 2  # Ajustamos la fracción para obtener más filas\n",
        "    muestra_aleatoria = df.sample(withReplacement=False, fraction=fraccion_ajustada, seed=semilla).limit(262)\n",
        "    sucursales_distintas = muestra_aleatoria.select('sucursales').distinct()\n",
        "    if sucursales_distintas.count() > 25:\n",
        "        sucursales_distintas = sucursales_distintas.orderBy(F.rand(seed=semilla)).limit(25)\n",
        "    df_filtrado = df.join(sucursales_distintas, 'sucursales', 'inner')\n",
        "\n",
        "# Mostrar las filas seleccionadas\n",
        "df_filtrado.show()"
      ],
      "metadata": {
        "id": "OSL9XpFio7kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "import math\n",
        "\n",
        "# Definimos una UDF para truncar el número flotante\n",
        "def trunc_float(num, precision):\n",
        "    if num is not None:\n",
        "        return math.trunc(num * 10**precision) / 10**precision\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Registramos la UDF\n",
        "trunc_float_udf = udf(trunc_float, FloatType())\n",
        "\n",
        "# Aplicamos la UDF a la columna después de reemplazar las comas con puntos\n",
        "df = df.withColumn('columna_con_puntos', regexp_replace(col('columna_con_comas'), ',', '.'))\n",
        "df = df.withColumn('columna_flotante', trunc_float_udf(col('columna_con_puntos').cast('float'), lit(3)))\n"
      ],
      "metadata": {
        "id": "W9BrbFxJX268"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, recall_score, f1_score, roc_curve, auc\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Establecer la semilla aleatoria\n",
        "np.random.seed(42)\n",
        "\n",
        "# Supongamos que tienes un DataFrame 'df' con tus datos\n",
        "# X son las características y y es la variable objetivo\n",
        "X = df.drop(columns=[\"variable_dependiente\"])\n",
        "y = df[\"variable_dependiente\"]\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# Definir la grilla de hiperparámetros\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 150, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5]\n",
        "}\n",
        "\n",
        "# Crear el modelo Gradient Boosting Classifier\n",
        "modelo_gbt = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Configurar GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=modelo_gbt, param_grid=param_grid, cv=5, scoring='roc_auc', n_jobs=-1, random_state=42)\n",
        "\n",
        "# Ajustar el modelo con la búsqueda de hiperparámetros\n",
        "grid_search.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Obtener los mejores parámetros\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Mejores parámetros:\", best_params)\n",
        "\n",
        "# Entrenar el modelo con los mejores parámetros\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predecir en el conjunto de prueba\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calcular las métricas\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "\n",
        "# Calcular el coeficiente Gini\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "gini = 2 * roc_auc - 1\n",
        "\n",
        "# Calcular el valor KS\n",
        "def calc_ks(y_true, y_prob):\n",
        "    data = pd.DataFrame({'label': y_true, 'probability': y_prob})\n",
        "    data['good'] = (data['label'] == 0).astype(int)\n",
        "    data['bad'] = (data['label'] == 1).astype(int)\n",
        "    data['bucket'] = (data['probability'].rank(pct=True) * 10).astype(int)\n",
        "    grouped = data.groupby('bucket', as_index=True)\n",
        "    kstable = grouped.min().probability.to_frame(name='min_prob')\n",
        "    kstable['max_prob'] = grouped.max().probability\n",
        "    kstable['bads'] = grouped.sum().bad\n",
        "    kstable['goods'] = grouped.sum().good\n",
        "    kstable = kstable.reset_index()\n",
        "    kstable['bad_rate'] = kstable.bads / (kstable.bads + kstable.goods)\n",
        "    kstable['ks'] = np.round(((kstable.bads / data.bad.sum()).cumsum() - (kstable.goods / data.good.sum()).cumsum()), 4) * 100\n",
        "    ks_value = kstable.ks.abs().max()\n",
        "    return ks_value\n",
        "\n",
        "ks = calc_ks(y_test, y_proba)\n",
        "\n",
        "# Imprimir las métricas\n",
        "print(\"Precisión (Accuracy):\", accuracy)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Gini:\", gini)\n",
        "print(\"KS:\", ks)\n",
        "print(\"ROC AUC:\", roc_auc)\n",
        "\n",
        "# Informe de clasificación y matriz de confusión\n",
        "print(\"Informe de clasificación:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Crear un explainer para el mejor modelo\n",
        "explainer = shap.Explainer(best_model, X_train_res)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# Gráfico resumen para el modelo\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
        "plt.title(\"Importancia de características - Modelo GBT\")\n",
        "plt.show()\n",
        "\n",
        "# Gráfico de dependencia para una característica específica\n",
        "shap.dependence_plot(\"nombre_de_la_característica\", shap_values, X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "kUDwxePQI1pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Crear una lista de las columnas que son de tipo string\n",
        "categoricalColumns = [item[0] for item in datos2.dtypes if item[1].startswith('string')]\n",
        "\n",
        "# Definir una lista de etapas en tu pipeline. El StringIndexer será una etapa\n",
        "stages = []\n",
        "\n",
        "# Iterar a través de todos los valores categóricos\n",
        "for categoricalCol in categoricalColumns:\n",
        "    # Crear un StringIndexer para esos valores categóricos y asignar un nuevo nombre incluyendo la palabra 'Index'\n",
        "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + '_NUEVAS', handleInvalid='keep')\n",
        "\n",
        "    # Añadir el StringIndexer a nuestra lista de etapas\n",
        "    stages += [stringIndexer]\n",
        "\n",
        "# Crear el pipeline. Asignar la lista de etapas a la palabra clave stages del pipeline\n",
        "pipeline = Pipeline(stages=stages)\n",
        "\n",
        "# Ajustar el pipeline a nuestro DataFrame\n",
        "pipelineModel = pipeline.fit(datos2)\n",
        "\n",
        "# Guardar el modelo del pipeline\n",
        "pipelineModel.write().overwrite().save(\"path/to/save/pipelineModel\")\n",
        "#Código generado por IA. Revisar y usar cuidadosamente. Más información sobre preguntas frecuentes.\n",
        "#2. Cargar el modelo StringIndexer y transformar nuevos datos\n",
        "#Python\n",
        "\n",
        "from pyspark.ml import PipelineModel\n",
        "\n",
        "# Cargar el modelo del pipeline guardado\n",
        "pipelineModel = PipelineModel.load(\"path/to/save/pipelineModel\")\n",
        "\n",
        "# Transformar el DataFrame con el modelo cargado\n",
        "df_nuevo_transformado = pipelineModel.transform(df_nuevo)\n",
        "\n",
        "# Eliminar las variables que no necesitamos\n",
        "df_nuevo_transformado = df_nuevo_transformado.drop(*categoricalColumns)\n",
        "\n",
        "# Mostrar el DataFrame transformado\n",
        "df_nuevo_transformado.show(10)"
      ],
      "metadata": {
        "id": "yBo-1bUHMjLQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
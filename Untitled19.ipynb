{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMe926553rPaCACmLBoVlVd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTYfA5cQIEpp",
        "outputId": "643d1198-470a-460f-f022-345624ea966b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Iniciar una sesión de Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Python Spark SQL basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Supongamos que tus DataFrames se llaman df_diciembre, df_campana (enero a abril) y df_respuestas\n",
        "\n",
        "# Primero, une df_diciembre con df_campana para encontrar los clientes que estuvieron en campaña en enero a abril\n",
        "df_unido = df_diciembre.join(df_campana, on='rut', how='inner')\n",
        "\n",
        "# Luego, une df_unido con df_respuestas para encontrar los clientes que respondieron\n",
        "df_unido = df_unido.join(df_respuestas, on=['rut', 'mes'], how='left_anti')\n",
        "\n",
        "# En este punto, df_unido contiene solo las filas de los clientes que no respondieron\n",
        "\n",
        "# Ordena el DataFrame por rut y mes, y luego guarda solo la última fila para cada rut\n",
        "df_ultima_campana = df_unido.orderBy('rut', 'mes', ascending=False).dropDuplicates(['rut'])\n"
      ],
      "metadata": {
        "id": "4Re9Cxoj6Bbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, first\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Inicializar SparkSession\n",
        "spark = SparkSession.builder.appName(\"AnalisisClientes\").getOrCreate()\n",
        "\n",
        "# Cargar los archivos\n",
        "df_clientes = spark.read.csv('clientes.csv', header=True, inferSchema=True)\n",
        "df_campanas = spark.read.csv('campanas.csv', header=True, inferSchema=True)\n",
        "df_respuestas = spark.read.csv('respuestas.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Asumiendo que 'df_respuestas' tiene las columnas 'id_cliente', 'mes_respuesta' y 'respondio'\n",
        "# Crear una ventana particionada por 'id_cliente' y ordenada por 'mes_respuesta' en orden descendente\n",
        "windowSpec = Window.partitionBy(\"id_cliente\").orderBy(col(\"mes_respuesta\").desc())\n",
        "\n",
        "# Usar la función 'first' para obtener la última respuesta de cada cliente\n",
        "df_ultima_respuesta = df_respuestas.withColumn(\"ultima_respuesta\", first(\"respondio\").over(windowSpec))\n",
        "\n",
        "# Eliminar duplicados para obtener una fila por cliente\n",
        "df_ultima_respuesta = df_ultima_respuesta.dropDuplicates([\"id_cliente\"])\n",
        "\n",
        "# Unir con el dataframe de clientes para obtener la información completa\n",
        "df_resultado = df_clientes.join(df_ultima_respuesta, \"id_cliente\")\n",
        "\n",
        "# Seleccionar las columnas relevantes\n",
        "df_resultado = df_resultado.select(\"id_cliente\", \"ultima_respuesta\")\n",
        "\n",
        "# Guardar o procesar los resultados según sea necesario\n",
        "df_resultado.show()"
      ],
      "metadata": {
        "id": "aT_DFaux3x91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Definimos el número de filas deseadas y el máximo de sucursales distintas\n",
        "num_filas = 262\n",
        "max_sucursales = 25\n",
        "\n",
        "# Suponiendo que 'df' es tu DataFrame, 'sucursales' y 'rut' son las columnas de tu interés\n",
        "# Inicializamos la muestra completa vacía y la semilla\n",
        "muestra_completa = None\n",
        "semilla = 42  # Puedes iniciar con cualquier número\n",
        "\n",
        "# Función para verificar las condiciones de la muestra\n",
        "def verificar_muestra(df, num_filas, max_sucursales):\n",
        "    cuenta_sucursales = df.select('sucursales').distinct().count()\n",
        "    cuenta_filas = df.count()\n",
        "    return cuenta_filas == num_filas and cuenta_sucursales <= max_sucursales\n",
        "\n",
        "# Iteramos sobre diferentes semillas hasta cumplir con las condiciones\n",
        "while not verificar_muestra(muestra_completa, num_filas, max_sucursales):\n",
        "    # Seleccionamos hasta 25 sucursales distintas de manera aleatoria\n",
        "    sucursales_distintas = df.select('sucursales').distinct().orderBy(F.rand(seed=semilla)).limit(max_sucursales)\n",
        "\n",
        "    # Creamos una muestra estratificada con las sucursales seleccionadas\n",
        "    df_filtrado = df.join(sucursales_distintas, 'sucursales', 'inner')\n",
        "    muestra_estratificada = df_filtrado.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(num_filas)\n",
        "\n",
        "    # Si la muestra estratificada tiene menos de 262 filas, rellenamos con más filas\n",
        "    if muestra_estratificada.count() < num_filas:\n",
        "        # Obtenemos los 'rut' que ya están en la muestra\n",
        "        ruts_en_muestra = muestra_estratificada.select('rut').distinct()\n",
        "\n",
        "        # Filtramos el DataFrame filtrado para excluir los 'rut' ya presentes\n",
        "        df_restante = df_filtrado.join(ruts_en_muestra, 'rut', 'left_anti')\n",
        "\n",
        "        # Calculamos cuántas filas faltan para llegar a 262\n",
        "        filas_faltantes = num_filas - muestra_estratificada.count()\n",
        "\n",
        "        # Tomamos filas adicionales sin repetir 'rut'\n",
        "        filas_adicionales = df_restante.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(filas_faltantes)\n",
        "\n",
        "        # Unimos las filas adicionales a la muestra estratificada\n",
        "        muestra_completa = muestra_estratificada.union(filas_adicionales)\n",
        "    else:\n",
        "        muestra_completa = muestra_estratificada\n",
        "\n",
        "    # Incrementamos la semilla para la siguiente iteración\n",
        "    semilla += 1\n",
        "\n",
        "# Mostrar la muestra completa\n",
        "muestra_completa.show()"
      ],
      "metadata": {
        "id": "JuTKfX-N5QoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Establecemos una semilla para la reproducibilidad\n",
        "semilla = 42\n",
        "\n",
        "# Suponiendo que 'df' es tu DataFrame, 'sucursales' y 'rut' son las columnas de tu interés\n",
        "# Paso 1: Seleccionamos hasta 25 sucursales distintas de manera aleatoria\n",
        "sucursales_distintas = df.select('sucursales').distinct().orderBy(F.rand(seed=semilla)).limit(25)\n",
        "\n",
        "# Paso 2: Creamos una muestra estratificada con las sucursales seleccionadas\n",
        "df_filtrado = df.join(sucursales_distintas, 'sucursales', 'inner')\n",
        "muestra_estratificada = df_filtrado.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(262)\n",
        "\n",
        "# Paso 3: Verificamos si la muestra alcanza las 262 filas\n",
        "if muestra_estratificada.count() < 262:\n",
        "    # Obtenemos los 'rut' que ya están en la muestra\n",
        "    ruts_en_muestra = muestra_estratificada.select('rut').distinct()\n",
        "\n",
        "    # Filtramos el DataFrame original para excluir los 'rut' ya presentes y mantener solo las sucursales seleccionadas\n",
        "    df_restante = df.join(sucursales_distintas, 'sucursales', 'inner').join(ruts_en_muestra, 'rut', 'left_anti')\n",
        "\n",
        "    # Calculamos cuántas filas faltan para llegar a 262\n",
        "    filas_faltantes = 262 - muestra_estratificada.count()\n",
        "\n",
        "    # Tomamos filas adicionales sin repetir 'rut' y que pertenezcan a las sucursales ya seleccionadas\n",
        "    filas_adicionales = df_restante.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(filas_faltantes)\n",
        "\n",
        "    # Unimos las filas adicionales a la muestra estratificada\n",
        "    muestra_completa = muestra_estratificada.union(filas_adicionales)\n",
        "\n",
        "# Mostrar la muestra completa\n",
        "muestra_completa.show()"
      ],
      "metadata": {
        "id": "ih0IRr-X3zjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Establecemos una semilla para la reproducibilidad\n",
        "semilla = 42\n",
        "\n",
        "# Suponiendo que 'df' es tu DataFrame y 'sucursales' es el nombre de tu columna\n",
        "# Paso 1: Estratificar el DataFrame\n",
        "estratos = df.select('sucursales').distinct()\n",
        "\n",
        "# Paso 2: Seleccionar hasta 25 sucursales aleatorias\n",
        "sucursales_aleatorias = estratos.orderBy(F.rand(seed=semilla)).limit(25)\n",
        "\n",
        "# Paso 3: Muestrear dentro de cada estrato\n",
        "# Creamos un diccionario para la muestra estratificada\n",
        "fracciones = {row['sucursales']: 1.0 for row in sucursales_aleatorias.collect()}\n",
        "\n",
        "# Realizamos la muestra estratificada\n",
        "muestra_estratificada = df.stat.sampleBy('sucursales', fracciones, seed=semilla)\n",
        "\n",
        "# Paso 4: Combinar las muestras hasta alcanzar 262 filas\n",
        "muestra_final = muestra_estratificada.limit(262)\n",
        "\n",
        "# Mostrar las filas seleccionadas\n",
        "muestra_final.show()"
      ],
      "metadata": {
        "id": "m7UPjHPutKEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Establecemos una semilla para la reproducibilidad\n",
        "semilla = 42\n",
        "\n",
        "# Suponiendo que 'df' es tu DataFrame, 'sucursales' y 'rut' son las columnas de tu interés\n",
        "# Paso 1: Seleccionamos hasta 25 sucursales distintas de manera aleatoria\n",
        "sucursales_distintas = df.select('sucursales').distinct().orderBy(F.rand(seed=semilla)).limit(25)\n",
        "\n",
        "# Paso 2: Creamos una muestra estratificada con las sucursales seleccionadas\n",
        "df_filtrado = df.join(sucursales_distintas, 'sucursales', 'inner')\n",
        "muestra_estratificada = df_filtrado.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(262)\n",
        "\n",
        "# Paso 3: Verificamos si la muestra alcanza las 262 filas\n",
        "if muestra_estratificada.count() < 262:\n",
        "    # Obtenemos los 'rut' que ya están en la muestra\n",
        "    ruts_en_muestra = muestra_estratificada.select('rut').distinct()\n",
        "\n",
        "    # Filtramos el DataFrame original para excluir los 'rut' ya presentes\n",
        "    df_restante = df_filtrado.join(ruts_en_muestra, 'rut', 'left_anti')\n",
        "\n",
        "    # Calculamos cuántas filas faltan para llegar a 262\n",
        "    filas_faltantes = 262 - muestra_estratificada.count()\n",
        "\n",
        "    # Tomamos filas adicionales sin repetir 'rut'\n",
        "    filas_adicionales = df_restante.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(filas_faltantes)\n",
        "\n",
        "    # Unimos las filas adicionales a la muestra estratificada\n",
        "    muestra_completa = muestra_estratificada.union(filas_adicionales)\n",
        "\n",
        "# Mostrar la muestra completa\n",
        "muestra_completa.show()"
      ],
      "metadata": {
        "id": "AMwgEcs_xosI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Establecemos una semilla para la reproducibilidad\n",
        "semilla = 42\n",
        "\n",
        "# Suponiendo que 'df' es tu DataFrame, 'sucursales' y 'rut' son las columnas de tu interés\n",
        "# Paso 1: Seleccionamos hasta 25 sucursales distintas de manera aleatoria\n",
        "sucursales_distintas = df.select('sucursales').distinct().orderBy(F.rand(seed=semilla)).limit(25)\n",
        "\n",
        "# Paso 2: Creamos una muestra estratificada con las sucursales seleccionadas\n",
        "df_filtrado = df.join(sucursales_distintas, 'sucursales', 'inner')\n",
        "muestra_estratificada = df_filtrado.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(262)\n",
        "\n",
        "# Paso 3: Verificamos si la muestra alcanza las 262 filas\n",
        "if muestra_estratificada.count() < 262:\n",
        "    # Obtenemos los 'rut' que ya están en la muestra\n",
        "    ruts_en_muestra = muestra_estratificada.select('rut').distinct()\n",
        "\n",
        "    # Filtramos el DataFrame original para excluir los 'rut' ya presentes y mantener solo las sucursales seleccionadas\n",
        "    df_restante = df_filtrado.join(ruts_en_muestra, 'rut', 'left_anti')\n",
        "\n",
        "    # Calculamos cuántas filas faltan para llegar a 262\n",
        "    filas_faltantes = 262 - muestra_estratificada.count()\n",
        "\n",
        "    # Tomamos filas adicionales sin repetir 'rut' y que pertenezcan a las sucursales ya seleccionadas\n",
        "    filas_adicionales = df_restante.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(filas_faltantes)\n",
        "\n",
        "    # Unimos las filas adicionales a la muestra estratificada\n",
        "    muestra_completa = muestra_estratificada.union(filas_adicionales)\n",
        "\n",
        "# Mostrar la muestra completa\n",
        "muestra_completa.show()"
      ],
      "metadata": {
        "id": "hi62UmZY1O5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Establecemos una semilla para la reproducibilidad\n",
        "semilla = 42\n",
        "\n",
        "# Suponiendo que 'df' es tu DataFrame, 'sucursales' y 'rut' son las columnas de tu interés\n",
        "# Paso 1: Seleccionamos hasta 25 sucursales distintas de manera aleatoria\n",
        "sucursales_distintas = df.select('sucursales').distinct().orderBy(F.rand(seed=semilla)).limit(25)\n",
        "\n",
        "# Paso 2: Creamos una muestra estratificada con las sucursales seleccionadas\n",
        "df_filtrado = df.join(sucursales_distintas, 'sucursales', 'inner')\n",
        "muestra_estratificada = df_filtrado.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(262)\n",
        "\n",
        "# Paso 3: Verificamos si la muestra alcanza las 262 filas\n",
        "if muestra_estratificada.count() < 262:\n",
        "    # Obtenemos los 'rut' que ya están en la muestra\n",
        "    ruts_en_muestra = muestra_estratificada.select('rut').distinct()\n",
        "\n",
        "    # Filtramos el DataFrame original para excluir los 'rut' ya presentes y mantener solo las sucursales seleccionadas\n",
        "    df_restante = df.join(sucursales_distintas, 'sucursales', 'inner').join(ruts_en_muestra, 'rut', 'left_anti')\n",
        "\n",
        "    # Calculamos cuántas filas faltan para llegar a 262\n",
        "    filas_faltantes = 262 - muestra_estratificada.count()\n",
        "\n",
        "    # Tomamos filas adicionales sin repetir 'rut' y que pertenezcan a las sucursales ya seleccionadas\n",
        "    filas_adicionales = df_restante.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(filas_faltantes)\n",
        "\n",
        "    # Unimos las filas adicionales a la muestra estratificada\n",
        "    muestra_completa = muestra_estratificada.union(filas_adicionales)\n",
        "\n",
        "# Mostrar la muestra completa\n",
        "muestra_completa.show()"
      ],
      "metadata": {
        "id": "Ka4wIWfj3aEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "weZHrqBd3wen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Establecemos una semilla para la reproducibilidad\n",
        "semilla = 42\n",
        "\n",
        "# Suponiendo que 'df' es tu DataFrame y 'sucursales' es el nombre de tu columna\n",
        "# Paso 1: Seleccionamos un máximo de 25 sucursales distintas de manera aleatoria\n",
        "sucursales_distintas = df.select('sucursales').distinct().orderBy(F.rand(seed=semilla)).limit(25)\n",
        "\n",
        "# Mostrar las sucursales seleccionadas\n",
        "sucursales_distintas.show()"
      ],
      "metadata": {
        "id": "u8Vfnvq8Inao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Establecemos una semilla para la reproducibilidad\n",
        "semilla = 42\n",
        "\n",
        "# Suponiendo que 'df' es tu DataFrame y 'sucursales' es el nombre de tu columna\n",
        "# Paso 1: Seleccionamos una muestra aleatoria de filas del DataFrame\n",
        "muestra_aleatoria = df.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(262)\n",
        "\n",
        "# Paso 2: Creamos una ventana para particionar por sucursal y ordenar por aleatoriedad\n",
        "ventana = Window.partitionBy('sucursales').orderBy(F.rand(seed=semilla))\n",
        "\n",
        "# Paso 3: Agregamos una columna de índice basada en la ventana\n",
        "muestra_con_indice = muestra_aleatoria.withColumn('indice', F.row_number().over(ventana))\n",
        "\n",
        "# Paso 4: Obtenemos las sucursales distintas con un índice de 1 (la primera fila de cada sucursal)\n",
        "sucursales_distintas = muestra_con_indice.filter(col('indice') == 1).select('sucursales')\n",
        "\n",
        "# Paso 5: Si hay más de 25 sucursales distintas, seleccionamos 25 de ellas aleatoriamente\n",
        "if sucursales_distintas.count() > 25:\n",
        "    sucursales_distintas = sucursales_distintas.orderBy(F.rand(seed=semilla)).limit(25)\n",
        "\n",
        "# Paso 6: Filtramos la muestra original para incluir solo filas de las sucursales seleccionadas\n",
        "muestra_final = muestra_con_indice.join(sucursales_distintas, 'sucursales', 'inner').drop('indice')\n",
        "\n",
        "# Mostrar las filas seleccionadas\n",
        "muestra_final.show()"
      ],
      "metadata": {
        "id": "kidCTZbQsRvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = spark.read.format('csv').option('header','true').option('delimiter', ',').load('/content/sample_data/mnist_test.csv')"
      ],
      "metadata": {
        "id": "84xlzw40QQb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columnas = df.columns\n",
        "df2 = df.select(columnas[10:12])"
      ],
      "metadata": {
        "id": "AOceAfxHTQc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpihPVyWUZrj",
        "outputId": "50629028-c571-4395-97d8-20a64657376d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+\n",
            "|010|011|\n",
            "+---+---+\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "+---+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "# Tamaño de la población\n",
        "N = 2799\n",
        "\n",
        "# Valores de Z para diferentes niveles de confianza\n",
        "Z_values = [1.70, 1.75, 1.81, 1.88, 1.96, 2.05, 2.17, 2.33]  # 91%, 92%, 93%, 94%, 95%, 96%, 97%, 98% de confianza\n",
        "\n",
        "# Valores de E para diferentes márgenes de error\n",
        "E_values = [0.01, 0.02, 0.03, 0.04, 0.05]  # 1%, 2%, 3%, 4%, 5% de margen de error\n",
        "\n",
        "# Crear una figura y un conjunto de subgráficos\n",
        "fig, axs = plt.subplots(len(Z_values), len(E_values), figsize=(15, 15))\n",
        "\n",
        "# Calcular y graficar el tamaño de la muestra para cada combinación de Z y E\n",
        "for i, Z in enumerate(Z_values):\n",
        "    for j, E in enumerate(E_values):\n",
        "        X = Z**2 * 0.25  # Varianza máxima para una distribución binomial\n",
        "        n = N * X / ((N - 1) * E**2 + X)\n",
        "        axs[i, j].plot([0, N], [n, n], 'r-')\n",
        "        axs[i, j].set_title(f'IC={int(Z*10)}%, Error={int(E*100)}%, Muestra={int(n)}')\n",
        "        axs[i, j].set_xlabel('Tamaño de la población')\n",
        "        axs[i, j].set_ylabel('Tamaño de la muestra')\n",
        "\n",
        "# Ajustar el diseño\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "MsKnph8rmsrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "from scipy.stats import fisher_exact\n",
        "\n",
        "# Definimos una función para calcular el valor p de la prueba exacta de Fisher\n",
        "def fisher_test(df1_count, df2_count, df1_total, df2_total):\n",
        "    contingency_table = [[df1_count, df2_count], [df1_total - df1_count, df2_total - df2_count]]\n",
        "    _, p_value = fisher_exact(contingency_table)\n",
        "    return float(p_value)\n",
        "\n",
        "# Registramos la función como una udf\n",
        "fisher_test_udf = udf(fisher_test, FloatType())\n",
        "\n",
        "# Supongamos que 'var_cat' es tu variable categórica y que ya has agregado las categorías que faltan y las has llenado con ceros\n",
        "\n",
        "# Calculamos el total de cada categoría en cada dataframe\n",
        "df1_total = df1.count()\n",
        "df2_total = df2.count()\n",
        "\n",
        "# Aplicamos la prueba exacta de Fisher a cada categoría\n",
        "for category in all_categories:\n",
        "    df1_count = df1.filter(df1['var_cat'] == category).count()\n",
        "    df2_count = df2.filter(df2['var_cat'] == category).count()\n",
        "    p_value = fisher_test_udf(df1_count, df2_count, df1_total, df2_total)\n",
        "    print(\"Categoría:\", category)\n",
        "    print(\"P-value:\", p_value)\n"
      ],
      "metadata": {
        "id": "E6HfozV_xRfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Carga los datos en DataFrames de Spark\n",
        "df_transactions = spark.read.table(\"transactions\")\n",
        "df_pedt001 = spark.read.table(\"pedt001\")\n",
        "\n",
        "# Realiza la unión interna\n",
        "df = df_transactions.alias('a').join(df_pedt001.alias('b'), df_transactions['primary_id'] == df_pedt001['penumper'])\n",
        "\n",
        "# Filtra los datos\n",
        "df = df.filter(\n",
        "    (F.substring(df['value_date'], 1, 10).between('2024-04-01', '2024-04-30')) &\n",
        "    (df['product_source'] == '70') &\n",
        "    (df['sa_subproduct'] == '1910') &\n",
        "    (df['credit_debit_code'] == 'H') &\n",
        "    (df['a.data_date_part'].between('2024-04-01', '2024-04-30')) &\n",
        "    (~df['trx_source'].isin(['0391', '7001', '0956', '1872']))\n",
        ")\n",
        "\n",
        "# Selecciona las columnas deseadas y realiza las transformaciones necesarias\n",
        "df = df.select(\n",
        "    df['primary_id'],\n",
        "    df['run_date'],\n",
        "    df['txn_source'],\n",
        "    df['sa_subproduct'].alias('subprod'),\n",
        "    F.substring(df['account_id'], 15, 12).alias('cuenta'),\n",
        "    df['txn_amount'].cast('bigint').alias('monto')\n",
        ")"
      ],
      "metadata": {
        "id": "9ugaTxRzaPVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Establecemos una semilla para la reproducibilidad\n",
        "semilla = 42\n",
        "\n",
        "# Suponiendo que 'df' es tu DataFrame y 'sucursales' es el nombre de tu columna\n",
        "# Paso 1: Tomamos una muestra aleatoria de 262 filas del DataFrame\n",
        "muestra_aleatoria = df.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(262)\n",
        "\n",
        "# Paso 2: Creamos una ventana para particionar por sucursal y ordenar por aleatoriedad\n",
        "ventana = Window.partitionBy('sucursales').orderBy(F.rand(seed=semilla))\n",
        "\n",
        "# Paso 3: Agregamos una columna de índice basada en la ventana\n",
        "muestra_con_indice = muestra_aleatoria.withColumn('indice', F.row_number().over(ventana))\n",
        "\n",
        "# Paso 4: Filtramos las filas para tener un máximo de 25 sucursales distintas\n",
        "muestra_final = muestra_con_indice.filter(col('indice') <= 25)\n",
        "\n",
        "# Paso 5: Si la muestra final tiene menos de 262 filas, ajustamos la fracción y repetimos\n",
        "while muestra_final.count() < 262:\n",
        "    fraccion_ajustada = (262 / muestra_aleatoria.count()) * 2  # Ajustamos la fracción para obtener más filas\n",
        "    muestra_aleatoria = df.sample(withReplacement=False, fraction=fraccion_ajustada, seed=semilla).limit(262)\n",
        "    muestra_con_indice = muestra_aleatoria.withColumn('indice', F.row_number().over(ventana))\n",
        "    muestra_final = muestra_con_indice.filter(col('indice') <= 25)\n",
        "\n",
        "# Mostrar las filas seleccionadas\n",
        "muestra_final.show()"
      ],
      "metadata": {
        "id": "p0yus_SCndip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Establecemos una semilla para la reproducibilidad\n",
        "semilla = 42\n",
        "\n",
        "# Suponiendo que 'df' es tu DataFrame y 'sucursales' es el nombre de tu columna\n",
        "# Paso 1: Seleccionamos una muestra aleatoria de 262 filas del DataFrame\n",
        "muestra_aleatoria = df.sample(withReplacement=False, fraction=1.0, seed=semilla).limit(262)\n",
        "\n",
        "# Paso 2: Obtenemos las sucursales distintas de la muestra aleatoria\n",
        "sucursales_distintas = muestra_aleatoria.select('sucursales').distinct()\n",
        "\n",
        "# Paso 3: Si hay más de 25 sucursales distintas, seleccionamos 25 de ellas aleatoriamente\n",
        "if sucursales_distintas.count() > 25:\n",
        "    sucursales_distintas = sucursales_distintas.orderBy(F.rand(seed=semilla)).limit(25)\n",
        "\n",
        "# Paso 4: Filtramos la muestra original para incluir solo filas de las sucursales seleccionadas\n",
        "df_filtrado = df.join(sucursales_distintas, 'sucursales', 'inner')\n",
        "\n",
        "# Paso 5: Si la muestra filtrada es menor a 262 filas, ajustamos la fracción y repetimos\n",
        "while df_filtrado.count() < 262:\n",
        "    fraccion_ajustada = (262 / df.count()) * 2  # Ajustamos la fracción para obtener más filas\n",
        "    muestra_aleatoria = df.sample(withReplacement=False, fraction=fraccion_ajustada, seed=semilla).limit(262)\n",
        "    sucursales_distintas = muestra_aleatoria.select('sucursales').distinct()\n",
        "    if sucursales_distintas.count() > 25:\n",
        "        sucursales_distintas = sucursales_distintas.orderBy(F.rand(seed=semilla)).limit(25)\n",
        "    df_filtrado = df.join(sucursales_distintas, 'sucursales', 'inner')\n",
        "\n",
        "# Mostrar las filas seleccionadas\n",
        "df_filtrado.show()"
      ],
      "metadata": {
        "id": "OSL9XpFio7kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "import math\n",
        "\n",
        "# Definimos una UDF para truncar el número flotante\n",
        "def trunc_float(num, precision):\n",
        "    if num is not None:\n",
        "        return math.trunc(num * 10**precision) / 10**precision\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Registramos la UDF\n",
        "trunc_float_udf = udf(trunc_float, FloatType())\n",
        "\n",
        "# Aplicamos la UDF a la columna después de reemplazar las comas con puntos\n",
        "df = df.withColumn('columna_con_puntos', regexp_replace(col('columna_con_comas'), ',', '.'))\n",
        "df = df.withColumn('columna_flotante', trunc_float_udf(col('columna_con_puntos').cast('float'), lit(3)))\n"
      ],
      "metadata": {
        "id": "W9BrbFxJX268"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
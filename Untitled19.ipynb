{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaeR3A2Soyj0r4OvuAZv7X"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTYfA5cQIEpp",
        "outputId": "643d1198-470a-460f-f022-345624ea966b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Iniciar una sesión de Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Python Spark SQL basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u8Vfnvq8Inao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = spark.read.format('csv').option('header','true').option('delimiter', ',').load('/content/sample_data/mnist_test.csv')"
      ],
      "metadata": {
        "id": "84xlzw40QQb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columnas = df.columns\n",
        "df2 = df.select(columnas[10:12])"
      ],
      "metadata": {
        "id": "AOceAfxHTQc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpihPVyWUZrj",
        "outputId": "50629028-c571-4395-97d8-20a64657376d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+\n",
            "|010|011|\n",
            "+---+---+\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "|  0|  0|\n",
            "+---+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "# Tamaño de la población\n",
        "N = 2799\n",
        "\n",
        "# Valores de Z para diferentes niveles de confianza\n",
        "Z_values = [1.70, 1.75, 1.81, 1.88, 1.96, 2.05, 2.17, 2.33]  # 91%, 92%, 93%, 94%, 95%, 96%, 97%, 98% de confianza\n",
        "\n",
        "# Valores de E para diferentes márgenes de error\n",
        "E_values = [0.01, 0.02, 0.03, 0.04, 0.05]  # 1%, 2%, 3%, 4%, 5% de margen de error\n",
        "\n",
        "# Crear una figura y un conjunto de subgráficos\n",
        "fig, axs = plt.subplots(len(Z_values), len(E_values), figsize=(15, 15))\n",
        "\n",
        "# Calcular y graficar el tamaño de la muestra para cada combinación de Z y E\n",
        "for i, Z in enumerate(Z_values):\n",
        "    for j, E in enumerate(E_values):\n",
        "        X = Z**2 * 0.25  # Varianza máxima para una distribución binomial\n",
        "        n = N * X / ((N - 1) * E**2 + X)\n",
        "        axs[i, j].plot([0, N], [n, n], 'r-')\n",
        "        axs[i, j].set_title(f'IC={int(Z*10)}%, Error={int(E*100)}%, Muestra={int(n)}')\n",
        "        axs[i, j].set_xlabel('Tamaño de la población')\n",
        "        axs[i, j].set_ylabel('Tamaño de la muestra')\n",
        "\n",
        "# Ajustar el diseño\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "MsKnph8rmsrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "from scipy.stats import fisher_exact\n",
        "\n",
        "# Definimos una función para calcular el valor p de la prueba exacta de Fisher\n",
        "def fisher_test(df1_count, df2_count, df1_total, df2_total):\n",
        "    contingency_table = [[df1_count, df2_count], [df1_total - df1_count, df2_total - df2_count]]\n",
        "    _, p_value = fisher_exact(contingency_table)\n",
        "    return float(p_value)\n",
        "\n",
        "# Registramos la función como una udf\n",
        "fisher_test_udf = udf(fisher_test, FloatType())\n",
        "\n",
        "# Supongamos que 'var_cat' es tu variable categórica y que ya has agregado las categorías que faltan y las has llenado con ceros\n",
        "\n",
        "# Calculamos el total de cada categoría en cada dataframe\n",
        "df1_total = df1.count()\n",
        "df2_total = df2.count()\n",
        "\n",
        "# Aplicamos la prueba exacta de Fisher a cada categoría\n",
        "for category in all_categories:\n",
        "    df1_count = df1.filter(df1['var_cat'] == category).count()\n",
        "    df2_count = df2.filter(df2['var_cat'] == category).count()\n",
        "    p_value = fisher_test_udf(df1_count, df2_count, df1_total, df2_total)\n",
        "    print(\"Categoría:\", category)\n",
        "    print(\"P-value:\", p_value)\n"
      ],
      "metadata": {
        "id": "E6HfozV_xRfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Carga los datos en DataFrames de Spark\n",
        "df_transactions = spark.read.table(\"transactions\")\n",
        "df_pedt001 = spark.read.table(\"pedt001\")\n",
        "\n",
        "# Realiza la unión interna\n",
        "df = df_transactions.alias('a').join(df_pedt001.alias('b'), df_transactions['primary_id'] == df_pedt001['penumper'])\n",
        "\n",
        "# Filtra los datos\n",
        "df = df.filter(\n",
        "    (F.substring(df['value_date'], 1, 10).between('2024-04-01', '2024-04-30')) &\n",
        "    (df['product_source'] == '70') &\n",
        "    (df['sa_subproduct'] == '1910') &\n",
        "    (df['credit_debit_code'] == 'H') &\n",
        "    (df['a.data_date_part'].between('2024-04-01', '2024-04-30')) &\n",
        "    (~df['trx_source'].isin(['0391', '7001', '0956', '1872']))\n",
        ")\n",
        "\n",
        "# Selecciona las columnas deseadas y realiza las transformaciones necesarias\n",
        "df = df.select(\n",
        "    df['primary_id'],\n",
        "    df['run_date'],\n",
        "    df['txn_source'],\n",
        "    df['sa_subproduct'].alias('subprod'),\n",
        "    F.substring(df['account_id'], 15, 12).alias('cuenta'),\n",
        "    df['txn_amount'].cast('bigint').alias('monto')\n",
        ")\n"
      ],
      "metadata": {
        "id": "9ugaTxRzaPVR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}